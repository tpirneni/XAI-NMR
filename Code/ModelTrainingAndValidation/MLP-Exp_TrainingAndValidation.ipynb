{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nmrglue as ng\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 100000\n",
    "\n",
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"MLP-Exp_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/ModelPerformanceMetrics') #Save random seed used\n",
    "seed = 4\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra = np.load('ExperimentalLikeDataset_Spec.npy')\n",
    "conc1 = np.load('ExperimentalLikeDataset_Conc.npy')\n",
    "\n",
    "# Load validation dataset\n",
    "spectraVal = np.load('ExperimentalLikeDataset_ValSpec.npy')\n",
    "concVal = np.load('ExperimentalLikeDataset_ValConc.npy')\n",
    "\n",
    "# Load representative validation spectra\n",
    "ValSpectra = np.load(\"ExperimentalLikeDatasetRepresentativeExamples_Spectra.npy\")\n",
    "ValConc = np.load(\"ExperimentalLikeDatasetRepresentativeExamples_Concentrations.npy\")\n",
    "ValSpecNames = np.load(\"ExperimentalLikeDatasetRepresentativeExamples_VariableNames.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(spectra, conc1, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train1).float()\n",
    "y_train = torch.tensor(y_train1).float()\n",
    "X_test = torch.tensor(X_test1).float()\n",
    "y_test = torch.tensor(y_test1).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)   # Confusing names, these spectra are the 5000 spectra generated like the training dataset\n",
    "ValSpectra = torch.tensor(ValSpectra).float().to(device)   # Confusing names, these spectra are the 10 representative example spectra\n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ValConc = torch.tensor(ValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 128, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "size_hidden1 = 200\n",
    "size_hidden2 = 8\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(39500, size_hidden1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "    def forward(self, input):\n",
    "        return (self.lin2(self.relu1(self.lin1(input))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "size_hidden1 = 200\n",
    "size_hidden2 = 200\n",
    "size_hidden3 = 8\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(39500, size_hidden1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "        self.relu2 = nn.ReLU()  \n",
    "        self.lin3 = nn.Linear(size_hidden2, size_hidden3)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.relu1(self.lin1(input))\n",
    "        x = self.relu2(self.lin2(x))\n",
    "        return self.lin3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "size_hidden1 = 200\n",
    "size_hidden2 = 200\n",
    "size_hidden3 = 200\n",
    "size_hidden4 = 8\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(39500, size_hidden1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "        self.relu2 = nn.ReLU()  \n",
    "        self.lin3 = nn.Linear(size_hidden2, size_hidden3)\n",
    "        self.relu3 = nn.ReLU()  \n",
    "        self.lin4 = nn.Linear(size_hidden3, size_hidden4)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.relu1(self.lin1(input))\n",
    "        x = self.relu2(self.lin2(x))\n",
    "        x = self.relu3(self.lin3(x))\n",
    "        return self.lin4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define model with batch normalization\n",
    "class NMR_Model_Aq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(39500, size_hidden1)\n",
    "        self.bn1 = nn.BatchNorm1d(size_hidden1)  # BatchNorm1d layer after first linear layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "        self.bn2 = nn.BatchNorm1d(size_hidden2)  # BatchNorm1d layer after second linear layer\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.lin1(input)\n",
    "        x = self.bn1(x)  # Applying batch normalization\n",
    "        x = self.relu1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.bn2(x)  # Applying batch normalization\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "        # Save model at specific epochs\n",
    "        if epoch + 1 in [50000]:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'{save_path}_epoch_{epoch+1}.pt')\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # Save model when test loss improves\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [50/100000], Train Loss: 15633.1004, Test Loss: 11770.3058\n",
      "Epoch [100/100000], Train Loss: 5004.5578, Test Loss: 6927.8727\n",
      "Epoch [150/100000], Train Loss: 2443.5778, Test Loss: 4681.2082\n",
      "Epoch [200/100000], Train Loss: 1334.0507, Test Loss: 3560.0529\n",
      "Epoch [250/100000], Train Loss: 987.4384, Test Loss: 2988.5085\n",
      "Epoch [300/100000], Train Loss: 860.2739, Test Loss: 2728.8967\n",
      "Epoch [350/100000], Train Loss: 745.3993, Test Loss: 2579.9976\n",
      "Epoch [400/100000], Train Loss: 665.4160, Test Loss: 2433.7716\n",
      "Epoch [450/100000], Train Loss: 581.0540, Test Loss: 2327.4148\n",
      "Epoch [500/100000], Train Loss: 509.2173, Test Loss: 2270.5074\n",
      "Epoch [550/100000], Train Loss: 455.1942, Test Loss: 2203.9250\n",
      "Epoch [600/100000], Train Loss: 400.4290, Test Loss: 2124.6104\n",
      "Epoch [650/100000], Train Loss: 379.8206, Test Loss: 2072.8421\n",
      "Epoch [700/100000], Train Loss: 395.0922, Test Loss: 2038.6833\n",
      "Epoch [750/100000], Train Loss: 291.4330, Test Loss: 1988.7259\n",
      "Epoch [800/100000], Train Loss: 271.3303, Test Loss: 1920.4115\n",
      "Epoch [850/100000], Train Loss: 262.3087, Test Loss: 1876.5554\n",
      "Epoch [900/100000], Train Loss: 266.8705, Test Loss: 1834.3025\n",
      "Epoch [950/100000], Train Loss: 229.6664, Test Loss: 1790.2958\n",
      "Epoch [1000/100000], Train Loss: 223.7416, Test Loss: 1739.9474\n",
      "Epoch [1050/100000], Train Loss: 209.5299, Test Loss: 1699.7762\n",
      "Epoch [1100/100000], Train Loss: 226.8964, Test Loss: 1656.4711\n",
      "Epoch [1150/100000], Train Loss: 192.7985, Test Loss: 1637.5601\n",
      "Epoch [1200/100000], Train Loss: 180.6207, Test Loss: 1578.3925\n",
      "Epoch [1250/100000], Train Loss: 164.5824, Test Loss: 1580.9291\n",
      "Epoch [1300/100000], Train Loss: 183.9254, Test Loss: 1518.7763\n",
      "Epoch [1350/100000], Train Loss: 158.5396, Test Loss: 1488.2090\n",
      "Epoch [1400/100000], Train Loss: 165.5742, Test Loss: 1471.0466\n",
      "Epoch [1450/100000], Train Loss: 141.7239, Test Loss: 1450.1744\n",
      "Epoch [1500/100000], Train Loss: 166.7093, Test Loss: 1418.6403\n",
      "Epoch [1550/100000], Train Loss: 136.9423, Test Loss: 1388.8797\n",
      "Epoch [1600/100000], Train Loss: 140.8595, Test Loss: 1368.0134\n",
      "Epoch [1650/100000], Train Loss: 130.4675, Test Loss: 1352.7608\n",
      "Epoch [1700/100000], Train Loss: 121.1606, Test Loss: 1336.5157\n",
      "Epoch [1750/100000], Train Loss: 126.7269, Test Loss: 1302.6018\n",
      "Epoch [1800/100000], Train Loss: 137.3142, Test Loss: 1296.7779\n",
      "Epoch [1850/100000], Train Loss: 113.5014, Test Loss: 1278.6187\n",
      "Epoch [1900/100000], Train Loss: 120.5688, Test Loss: 1265.1573\n",
      "Epoch [1950/100000], Train Loss: 126.6639, Test Loss: 1255.6298\n",
      "Epoch [2000/100000], Train Loss: 122.6564, Test Loss: 1244.8870\n",
      "Epoch [2050/100000], Train Loss: 125.3044, Test Loss: 1231.0811\n",
      "Epoch [2100/100000], Train Loss: 121.7684, Test Loss: 1222.7995\n",
      "Epoch [2150/100000], Train Loss: 122.4538, Test Loss: 1213.5188\n",
      "Epoch [2200/100000], Train Loss: 103.0111, Test Loss: 1184.4370\n",
      "Epoch [2250/100000], Train Loss: 100.5608, Test Loss: 1167.5075\n",
      "Epoch [2300/100000], Train Loss: 106.5788, Test Loss: 1155.9313\n",
      "Epoch [2350/100000], Train Loss: 100.2320, Test Loss: 1145.6936\n",
      "Epoch [2400/100000], Train Loss: 92.5907, Test Loss: 1131.6369\n",
      "Epoch [2450/100000], Train Loss: 104.4723, Test Loss: 1115.4736\n",
      "Epoch [2500/100000], Train Loss: 97.8195, Test Loss: 1120.9704\n",
      "Epoch [2550/100000], Train Loss: 87.0357, Test Loss: 1110.1884\n",
      "Epoch [2600/100000], Train Loss: 90.3093, Test Loss: 1095.4597\n",
      "Epoch [2650/100000], Train Loss: 91.5899, Test Loss: 1090.4172\n",
      "Epoch [2700/100000], Train Loss: 101.0426, Test Loss: 1074.3187\n",
      "Epoch [2750/100000], Train Loss: 87.8304, Test Loss: 1068.7408\n",
      "Epoch [2800/100000], Train Loss: 82.4492, Test Loss: 1067.5309\n",
      "Epoch [2850/100000], Train Loss: 97.7463, Test Loss: 1053.5505\n",
      "Epoch [2900/100000], Train Loss: 76.5918, Test Loss: 1040.8869\n",
      "Epoch [2950/100000], Train Loss: 83.7974, Test Loss: 1031.4832\n",
      "Epoch [3000/100000], Train Loss: 88.5974, Test Loss: 1028.9140\n",
      "Epoch [3050/100000], Train Loss: 83.8746, Test Loss: 1017.6537\n",
      "Epoch [3100/100000], Train Loss: 79.4658, Test Loss: 1029.5366\n",
      "Epoch [3150/100000], Train Loss: 75.5518, Test Loss: 1003.2602\n",
      "Epoch [3200/100000], Train Loss: 93.2763, Test Loss: 1023.6427\n",
      "Epoch [3250/100000], Train Loss: 84.4134, Test Loss: 1010.6268\n",
      "Epoch [3300/100000], Train Loss: 78.2554, Test Loss: 994.2480\n",
      "Epoch [3350/100000], Train Loss: 77.5133, Test Loss: 986.2403\n",
      "Epoch [3400/100000], Train Loss: 76.0694, Test Loss: 986.9092\n",
      "Epoch [3450/100000], Train Loss: 91.7576, Test Loss: 975.1971\n",
      "Epoch [3500/100000], Train Loss: 80.8507, Test Loss: 982.7702\n",
      "Epoch [3550/100000], Train Loss: 74.9761, Test Loss: 970.2597\n",
      "Epoch [3600/100000], Train Loss: 74.8130, Test Loss: 960.6237\n",
      "Epoch [3650/100000], Train Loss: 64.7283, Test Loss: 965.7329\n",
      "Epoch [3700/100000], Train Loss: 66.0883, Test Loss: 951.8262\n",
      "Epoch [3750/100000], Train Loss: 71.1702, Test Loss: 953.8583\n",
      "Epoch [3800/100000], Train Loss: 73.5791, Test Loss: 949.1908\n",
      "Epoch [3850/100000], Train Loss: 64.3049, Test Loss: 941.2733\n",
      "Epoch [3900/100000], Train Loss: 66.6224, Test Loss: 939.6776\n",
      "Epoch [3950/100000], Train Loss: 60.4571, Test Loss: 947.6366\n",
      "Epoch [4000/100000], Train Loss: 60.1353, Test Loss: 930.1745\n",
      "Epoch [4050/100000], Train Loss: 66.7939, Test Loss: 932.7778\n",
      "Epoch [4100/100000], Train Loss: 59.0926, Test Loss: 925.9700\n",
      "Epoch [4150/100000], Train Loss: 60.3566, Test Loss: 923.0624\n",
      "Epoch [4200/100000], Train Loss: 55.9720, Test Loss: 929.7896\n",
      "Epoch [4250/100000], Train Loss: 65.9450, Test Loss: 918.7317\n",
      "Epoch [4300/100000], Train Loss: 79.1340, Test Loss: 915.7036\n",
      "Epoch [4350/100000], Train Loss: 60.4875, Test Loss: 914.7741\n",
      "Epoch [4400/100000], Train Loss: 53.9880, Test Loss: 902.5325\n",
      "Epoch [4450/100000], Train Loss: 54.0149, Test Loss: 902.7368\n",
      "Epoch [4500/100000], Train Loss: 58.1224, Test Loss: 909.8858\n",
      "Epoch [4550/100000], Train Loss: 51.5708, Test Loss: 902.4468\n",
      "Epoch [4600/100000], Train Loss: 59.1492, Test Loss: 896.8108\n",
      "Epoch [4650/100000], Train Loss: 54.3701, Test Loss: 895.5497\n",
      "Epoch [4700/100000], Train Loss: 47.9084, Test Loss: 889.8809\n",
      "Epoch [4750/100000], Train Loss: 48.4121, Test Loss: 883.1991\n",
      "Epoch [4800/100000], Train Loss: 49.7858, Test Loss: 886.2560\n",
      "Epoch [4850/100000], Train Loss: 48.2379, Test Loss: 889.2520\n",
      "Epoch [4900/100000], Train Loss: 50.7770, Test Loss: 879.4505\n",
      "Epoch [4950/100000], Train Loss: 56.1367, Test Loss: 883.6156\n",
      "Epoch [5000/100000], Train Loss: 52.3546, Test Loss: 882.6513\n",
      "Epoch [5050/100000], Train Loss: 60.1360, Test Loss: 879.0868\n",
      "Epoch [5100/100000], Train Loss: 55.4934, Test Loss: 875.2030\n",
      "Epoch [5150/100000], Train Loss: 53.5638, Test Loss: 868.1726\n",
      "Epoch [5200/100000], Train Loss: 55.1955, Test Loss: 867.4767\n",
      "Epoch [5250/100000], Train Loss: 55.5843, Test Loss: 872.6470\n",
      "Epoch [5300/100000], Train Loss: 48.1149, Test Loss: 877.4771\n",
      "Epoch [5350/100000], Train Loss: 50.7857, Test Loss: 868.8511\n",
      "Epoch [5400/100000], Train Loss: 43.7741, Test Loss: 860.8908\n",
      "Epoch [5450/100000], Train Loss: 64.2207, Test Loss: 866.6781\n",
      "Epoch [5500/100000], Train Loss: 42.4694, Test Loss: 854.2815\n",
      "Epoch [5550/100000], Train Loss: 57.1426, Test Loss: 864.0290\n",
      "Epoch [5600/100000], Train Loss: 49.2728, Test Loss: 850.8888\n",
      "Epoch [5650/100000], Train Loss: 47.1025, Test Loss: 855.7663\n",
      "Epoch [5700/100000], Train Loss: 47.5481, Test Loss: 852.1200\n",
      "Epoch [5750/100000], Train Loss: 47.0748, Test Loss: 847.0931\n",
      "Epoch [5800/100000], Train Loss: 45.2012, Test Loss: 848.1002\n",
      "Epoch [5850/100000], Train Loss: 46.7064, Test Loss: 843.8562\n",
      "Epoch [5900/100000], Train Loss: 45.3156, Test Loss: 849.7630\n",
      "Epoch [5950/100000], Train Loss: 43.9519, Test Loss: 843.8556\n",
      "Epoch [6000/100000], Train Loss: 45.1921, Test Loss: 838.9130\n",
      "Epoch [6050/100000], Train Loss: 50.3950, Test Loss: 839.1248\n",
      "Epoch [6100/100000], Train Loss: 50.2971, Test Loss: 843.9612\n",
      "Epoch [6150/100000], Train Loss: 54.2260, Test Loss: 839.2486\n",
      "Epoch [6200/100000], Train Loss: 44.3513, Test Loss: 841.2347\n",
      "Epoch [6250/100000], Train Loss: 46.4649, Test Loss: 836.9541\n",
      "Epoch [6300/100000], Train Loss: 45.8825, Test Loss: 837.2699\n",
      "Epoch [6350/100000], Train Loss: 45.8717, Test Loss: 843.9750\n",
      "Epoch [6400/100000], Train Loss: 47.3338, Test Loss: 835.7127\n",
      "Epoch [6450/100000], Train Loss: 46.2402, Test Loss: 836.2734\n",
      "Epoch [6500/100000], Train Loss: 45.1940, Test Loss: 830.8811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6550/100000], Train Loss: 54.5501, Test Loss: 833.4955\n",
      "Epoch [6600/100000], Train Loss: 40.1201, Test Loss: 834.4090\n",
      "Epoch [6650/100000], Train Loss: 51.2851, Test Loss: 832.2157\n",
      "Epoch [6700/100000], Train Loss: 56.3405, Test Loss: 821.8095\n",
      "Epoch [6750/100000], Train Loss: 50.1451, Test Loss: 832.9910\n",
      "Epoch [6800/100000], Train Loss: 48.0318, Test Loss: 824.3734\n",
      "Epoch [6850/100000], Train Loss: 43.5451, Test Loss: 820.9125\n",
      "Epoch [6900/100000], Train Loss: 42.2106, Test Loss: 819.7991\n",
      "Epoch [6950/100000], Train Loss: 45.3863, Test Loss: 821.3607\n",
      "Epoch [7000/100000], Train Loss: 41.5247, Test Loss: 816.1755\n",
      "Epoch [7050/100000], Train Loss: 39.8405, Test Loss: 810.7776\n",
      "Epoch [7100/100000], Train Loss: 44.5478, Test Loss: 815.4946\n",
      "Epoch [7150/100000], Train Loss: 42.5578, Test Loss: 816.3529\n",
      "Epoch [7200/100000], Train Loss: 39.1879, Test Loss: 812.7519\n",
      "Epoch [7250/100000], Train Loss: 43.8532, Test Loss: 817.8085\n",
      "Epoch [7300/100000], Train Loss: 34.2624, Test Loss: 810.0755\n",
      "Epoch [7350/100000], Train Loss: 38.5765, Test Loss: 808.2446\n",
      "Epoch [7400/100000], Train Loss: 34.0868, Test Loss: 808.1837\n",
      "Epoch [7450/100000], Train Loss: 36.4622, Test Loss: 806.1399\n",
      "Epoch [7500/100000], Train Loss: 43.7498, Test Loss: 808.2706\n",
      "Epoch [7550/100000], Train Loss: 43.1767, Test Loss: 800.6024\n",
      "Epoch [7600/100000], Train Loss: 44.0296, Test Loss: 809.5997\n",
      "Epoch [7650/100000], Train Loss: 38.6885, Test Loss: 802.2527\n",
      "Epoch [7700/100000], Train Loss: 36.8331, Test Loss: 801.8991\n",
      "Epoch [7750/100000], Train Loss: 43.0484, Test Loss: 805.3198\n",
      "Epoch [7800/100000], Train Loss: 35.8058, Test Loss: 808.8280\n",
      "Epoch [7850/100000], Train Loss: 45.2892, Test Loss: 810.6640\n",
      "Epoch [7900/100000], Train Loss: 36.8215, Test Loss: 804.0126\n",
      "Epoch [7950/100000], Train Loss: 41.0911, Test Loss: 804.5146\n",
      "Epoch [8000/100000], Train Loss: 39.2594, Test Loss: 801.8250\n",
      "Epoch [8050/100000], Train Loss: 43.3406, Test Loss: 795.4297\n",
      "Epoch [8100/100000], Train Loss: 43.5332, Test Loss: 800.4041\n",
      "Epoch [8150/100000], Train Loss: 36.1487, Test Loss: 794.9571\n",
      "Epoch [8200/100000], Train Loss: 40.8172, Test Loss: 790.7970\n",
      "Epoch [8250/100000], Train Loss: 41.3651, Test Loss: 805.9061\n",
      "Epoch [8300/100000], Train Loss: 42.4807, Test Loss: 798.3673\n",
      "Epoch [8350/100000], Train Loss: 38.7997, Test Loss: 791.6058\n",
      "Epoch [8400/100000], Train Loss: 35.6702, Test Loss: 791.3235\n",
      "Epoch [8450/100000], Train Loss: 34.1505, Test Loss: 790.7151\n",
      "Epoch [8500/100000], Train Loss: 38.0481, Test Loss: 793.1914\n",
      "Epoch [8550/100000], Train Loss: 36.6380, Test Loss: 793.7195\n",
      "Epoch [8600/100000], Train Loss: 38.1400, Test Loss: 795.1646\n",
      "Epoch [8650/100000], Train Loss: 43.3074, Test Loss: 795.7667\n",
      "Epoch [8700/100000], Train Loss: 42.1387, Test Loss: 790.1123\n",
      "Epoch [8750/100000], Train Loss: 32.4642, Test Loss: 792.1006\n",
      "Epoch [8800/100000], Train Loss: 35.9341, Test Loss: 786.2378\n",
      "Epoch [8850/100000], Train Loss: 38.2130, Test Loss: 795.7578\n",
      "Epoch [8900/100000], Train Loss: 37.5308, Test Loss: 795.0628\n",
      "Epoch [8950/100000], Train Loss: 38.7037, Test Loss: 791.3004\n",
      "Epoch [9000/100000], Train Loss: 38.4092, Test Loss: 784.8896\n",
      "Epoch [9050/100000], Train Loss: 30.7378, Test Loss: 785.8385\n",
      "Epoch [9100/100000], Train Loss: 33.8277, Test Loss: 784.5497\n",
      "Epoch [9150/100000], Train Loss: 35.3725, Test Loss: 793.3802\n",
      "Epoch [9200/100000], Train Loss: 33.8596, Test Loss: 792.0351\n",
      "Epoch [9250/100000], Train Loss: 37.8774, Test Loss: 795.3885\n",
      "Epoch [9300/100000], Train Loss: 41.5158, Test Loss: 790.5356\n",
      "Epoch [9350/100000], Train Loss: 33.3006, Test Loss: 788.4157\n",
      "Epoch [9400/100000], Train Loss: 34.4850, Test Loss: 783.8278\n",
      "Epoch [9450/100000], Train Loss: 30.0908, Test Loss: 781.9785\n",
      "Epoch [9500/100000], Train Loss: 38.2779, Test Loss: 789.9265\n",
      "Epoch [9550/100000], Train Loss: 32.2144, Test Loss: 781.3996\n",
      "Epoch [9600/100000], Train Loss: 34.2224, Test Loss: 783.3557\n",
      "Epoch [9650/100000], Train Loss: 33.3080, Test Loss: 782.7507\n",
      "Epoch [9700/100000], Train Loss: 31.2777, Test Loss: 784.2064\n",
      "Epoch [9750/100000], Train Loss: 33.2813, Test Loss: 780.9898\n",
      "Epoch [9800/100000], Train Loss: 29.9166, Test Loss: 787.3428\n",
      "Epoch [9850/100000], Train Loss: 42.8965, Test Loss: 781.9907\n",
      "Epoch [9900/100000], Train Loss: 39.6980, Test Loss: 788.5868\n",
      "Epoch [9950/100000], Train Loss: 47.3155, Test Loss: 783.2774\n",
      "Epoch [10000/100000], Train Loss: 27.0373, Test Loss: 778.2199\n",
      "Epoch [10050/100000], Train Loss: 29.4592, Test Loss: 780.3255\n",
      "Epoch [10100/100000], Train Loss: 36.0319, Test Loss: 780.2165\n",
      "Epoch [10150/100000], Train Loss: 36.5802, Test Loss: 782.4686\n",
      "Epoch [10200/100000], Train Loss: 30.7430, Test Loss: 785.0506\n",
      "Epoch [10250/100000], Train Loss: 37.0984, Test Loss: 778.6362\n",
      "Epoch [10300/100000], Train Loss: 40.4924, Test Loss: 782.2521\n",
      "Epoch [10350/100000], Train Loss: 34.8338, Test Loss: 778.0736\n",
      "Epoch [10400/100000], Train Loss: 31.0220, Test Loss: 782.0877\n",
      "Epoch [10450/100000], Train Loss: 32.4199, Test Loss: 777.9328\n",
      "Epoch [10500/100000], Train Loss: 32.0242, Test Loss: 773.6592\n",
      "Epoch [10550/100000], Train Loss: 31.8504, Test Loss: 777.2970\n",
      "Epoch [10600/100000], Train Loss: 31.6534, Test Loss: 776.9180\n",
      "Epoch [10650/100000], Train Loss: 29.8057, Test Loss: 774.3952\n",
      "Epoch [10700/100000], Train Loss: 33.0851, Test Loss: 777.0916\n",
      "Epoch [10750/100000], Train Loss: 33.3910, Test Loss: 777.4999\n",
      "Epoch [10800/100000], Train Loss: 30.9440, Test Loss: 779.3330\n",
      "Epoch [10850/100000], Train Loss: 31.7601, Test Loss: 770.6645\n",
      "Epoch [10900/100000], Train Loss: 33.2486, Test Loss: 773.7177\n",
      "Epoch [10950/100000], Train Loss: 32.4026, Test Loss: 774.8583\n",
      "Epoch [11000/100000], Train Loss: 32.2050, Test Loss: 783.0840\n",
      "Epoch [11050/100000], Train Loss: 30.0931, Test Loss: 773.8612\n",
      "Epoch [11100/100000], Train Loss: 32.2334, Test Loss: 778.4597\n",
      "Epoch [11150/100000], Train Loss: 27.0603, Test Loss: 774.0846\n",
      "Epoch [11200/100000], Train Loss: 26.6906, Test Loss: 771.6538\n",
      "Epoch [11250/100000], Train Loss: 27.3948, Test Loss: 770.0339\n",
      "Epoch [11300/100000], Train Loss: 31.0245, Test Loss: 769.8597\n",
      "Epoch [11350/100000], Train Loss: 34.8815, Test Loss: 776.2114\n",
      "Epoch [11400/100000], Train Loss: 35.1611, Test Loss: 774.3879\n",
      "Epoch [11450/100000], Train Loss: 32.6873, Test Loss: 772.7007\n",
      "Epoch [11500/100000], Train Loss: 24.1394, Test Loss: 775.4161\n",
      "Epoch [11550/100000], Train Loss: 33.8746, Test Loss: 774.3853\n",
      "Epoch [11600/100000], Train Loss: 30.8587, Test Loss: 770.0267\n",
      "Epoch [11650/100000], Train Loss: 32.6532, Test Loss: 768.5495\n",
      "Epoch [11700/100000], Train Loss: 31.4133, Test Loss: 769.4464\n",
      "Epoch [11750/100000], Train Loss: 33.1774, Test Loss: 770.6483\n",
      "Epoch [11800/100000], Train Loss: 31.2636, Test Loss: 768.5359\n",
      "Epoch [11850/100000], Train Loss: 33.0342, Test Loss: 778.0488\n",
      "Epoch [11900/100000], Train Loss: 32.5487, Test Loss: 773.4064\n",
      "Epoch [11950/100000], Train Loss: 25.6701, Test Loss: 773.8522\n",
      "Epoch [12000/100000], Train Loss: 37.3973, Test Loss: 776.6090\n",
      "Epoch [12050/100000], Train Loss: 33.4124, Test Loss: 777.2920\n",
      "Epoch [12100/100000], Train Loss: 31.3593, Test Loss: 768.5024\n",
      "Epoch [12150/100000], Train Loss: 28.5054, Test Loss: 771.4826\n",
      "Epoch [12200/100000], Train Loss: 31.4135, Test Loss: 772.4187\n",
      "Epoch [12250/100000], Train Loss: 24.3321, Test Loss: 776.5746\n",
      "Epoch [12300/100000], Train Loss: 29.3583, Test Loss: 769.6311\n",
      "Epoch [12350/100000], Train Loss: 29.8477, Test Loss: 771.0588\n",
      "Epoch [12400/100000], Train Loss: 26.4509, Test Loss: 774.8039\n",
      "Epoch [12450/100000], Train Loss: 27.8834, Test Loss: 769.5449\n",
      "Epoch [12500/100000], Train Loss: 29.9381, Test Loss: 776.5923\n",
      "Epoch [12550/100000], Train Loss: 30.4549, Test Loss: 776.1831\n",
      "Epoch [12600/100000], Train Loss: 26.7145, Test Loss: 770.8922\n",
      "Epoch [12650/100000], Train Loss: 30.4192, Test Loss: 769.9110\n",
      "Epoch [12700/100000], Train Loss: 30.6568, Test Loss: 773.2423\n",
      "Epoch [12750/100000], Train Loss: 31.2036, Test Loss: 771.4804\n",
      "Epoch [12800/100000], Train Loss: 24.8291, Test Loss: 770.7005\n",
      "Epoch [12850/100000], Train Loss: 27.8101, Test Loss: 767.4503\n",
      "Epoch [12900/100000], Train Loss: 30.6510, Test Loss: 770.5999\n",
      "Epoch [12950/100000], Train Loss: 34.8494, Test Loss: 774.7509\n",
      "Epoch [13000/100000], Train Loss: 28.3442, Test Loss: 767.0545\n",
      "Epoch [13050/100000], Train Loss: 32.6593, Test Loss: 769.2949\n",
      "Epoch [13100/100000], Train Loss: 29.3162, Test Loss: 767.0505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13150/100000], Train Loss: 26.6372, Test Loss: 772.0738\n",
      "Epoch [13200/100000], Train Loss: 29.3342, Test Loss: 766.3257\n",
      "Epoch [13250/100000], Train Loss: 25.7734, Test Loss: 771.4461\n",
      "Epoch [13300/100000], Train Loss: 25.5095, Test Loss: 767.6365\n",
      "Epoch [13350/100000], Train Loss: 26.2840, Test Loss: 769.1626\n",
      "Epoch [13400/100000], Train Loss: 32.8201, Test Loss: 768.2110\n",
      "Epoch [13450/100000], Train Loss: 33.4460, Test Loss: 766.8654\n",
      "Epoch [13500/100000], Train Loss: 25.2293, Test Loss: 765.5765\n",
      "Epoch [13550/100000], Train Loss: 34.5391, Test Loss: 762.6886\n",
      "Epoch [13600/100000], Train Loss: 27.7275, Test Loss: 767.2003\n",
      "Epoch [13650/100000], Train Loss: 33.3580, Test Loss: 767.4467\n",
      "Epoch [13700/100000], Train Loss: 29.1724, Test Loss: 770.8869\n",
      "Epoch [13750/100000], Train Loss: 33.4146, Test Loss: 762.8448\n",
      "Epoch [13800/100000], Train Loss: 25.8306, Test Loss: 765.9990\n",
      "Epoch [13850/100000], Train Loss: 29.9001, Test Loss: 766.8504\n",
      "Epoch [13900/100000], Train Loss: 25.9968, Test Loss: 768.8599\n",
      "Epoch [13950/100000], Train Loss: 23.9259, Test Loss: 767.4863\n",
      "Epoch [14000/100000], Train Loss: 28.6767, Test Loss: 764.6651\n",
      "Epoch [14050/100000], Train Loss: 25.8358, Test Loss: 764.7017\n",
      "Epoch [14100/100000], Train Loss: 26.2855, Test Loss: 768.7677\n",
      "Epoch [14150/100000], Train Loss: 31.0376, Test Loss: 771.4512\n",
      "Epoch [14200/100000], Train Loss: 28.8430, Test Loss: 763.3828\n",
      "Epoch [14250/100000], Train Loss: 32.8652, Test Loss: 763.4687\n",
      "Epoch [14300/100000], Train Loss: 27.1783, Test Loss: 769.1584\n",
      "Epoch [14350/100000], Train Loss: 25.2631, Test Loss: 769.5865\n",
      "Epoch [14400/100000], Train Loss: 24.0698, Test Loss: 768.0157\n",
      "Epoch [14450/100000], Train Loss: 31.6902, Test Loss: 771.8675\n",
      "Epoch [14500/100000], Train Loss: 28.9873, Test Loss: 767.6723\n",
      "Epoch [14550/100000], Train Loss: 28.9202, Test Loss: 769.1974\n",
      "Epoch [14600/100000], Train Loss: 32.4946, Test Loss: 767.6270\n",
      "Epoch [14650/100000], Train Loss: 23.4588, Test Loss: 769.9597\n",
      "Epoch [14700/100000], Train Loss: 26.8335, Test Loss: 766.6153\n",
      "Epoch [14750/100000], Train Loss: 25.2094, Test Loss: 765.1020\n",
      "Epoch [14800/100000], Train Loss: 23.8912, Test Loss: 762.2851\n",
      "Epoch [14850/100000], Train Loss: 23.3235, Test Loss: 759.1141\n",
      "Epoch [14900/100000], Train Loss: 27.4905, Test Loss: 760.9780\n",
      "Epoch [14950/100000], Train Loss: 35.3894, Test Loss: 763.4280\n",
      "Epoch [15000/100000], Train Loss: 29.2073, Test Loss: 764.2837\n",
      "Epoch [15050/100000], Train Loss: 25.2520, Test Loss: 761.7086\n",
      "Epoch [15100/100000], Train Loss: 23.6072, Test Loss: 763.2351\n",
      "Epoch [15150/100000], Train Loss: 32.0833, Test Loss: 762.9522\n",
      "Epoch [15200/100000], Train Loss: 28.8205, Test Loss: 760.3367\n",
      "Epoch [15250/100000], Train Loss: 25.0920, Test Loss: 760.4425\n",
      "Epoch [15300/100000], Train Loss: 23.4982, Test Loss: 762.2437\n",
      "Epoch [15350/100000], Train Loss: 25.9471, Test Loss: 764.8860\n",
      "Epoch [15400/100000], Train Loss: 26.9505, Test Loss: 759.4125\n",
      "Epoch [15450/100000], Train Loss: 22.4859, Test Loss: 758.2640\n",
      "Epoch [15500/100000], Train Loss: 29.7380, Test Loss: 764.4169\n",
      "Epoch [15550/100000], Train Loss: 27.0827, Test Loss: 761.7869\n",
      "Epoch [15600/100000], Train Loss: 22.4574, Test Loss: 761.9446\n",
      "Epoch [15650/100000], Train Loss: 24.0452, Test Loss: 765.5253\n",
      "Epoch [15700/100000], Train Loss: 29.4975, Test Loss: 764.2630\n",
      "Epoch [15750/100000], Train Loss: 26.2878, Test Loss: 761.9472\n",
      "Epoch [15800/100000], Train Loss: 24.5004, Test Loss: 763.1349\n",
      "Epoch [15850/100000], Train Loss: 36.6895, Test Loss: 767.7215\n",
      "Epoch [15900/100000], Train Loss: 25.8448, Test Loss: 765.7376\n",
      "Epoch [15950/100000], Train Loss: 28.8317, Test Loss: 761.8090\n",
      "Epoch [16000/100000], Train Loss: 27.7984, Test Loss: 760.9138\n",
      "Epoch [16050/100000], Train Loss: 22.9709, Test Loss: 763.0704\n",
      "Epoch [16100/100000], Train Loss: 22.0443, Test Loss: 762.3227\n",
      "Epoch [16150/100000], Train Loss: 27.3693, Test Loss: 763.5764\n",
      "Epoch [16200/100000], Train Loss: 24.7619, Test Loss: 762.6487\n",
      "Epoch [16250/100000], Train Loss: 23.4956, Test Loss: 757.9266\n",
      "Epoch [16300/100000], Train Loss: 32.8767, Test Loss: 764.8265\n",
      "Epoch [16350/100000], Train Loss: 29.0108, Test Loss: 763.9572\n",
      "Epoch [16400/100000], Train Loss: 21.4637, Test Loss: 764.7188\n",
      "Epoch [16450/100000], Train Loss: 24.3826, Test Loss: 764.8820\n",
      "Epoch [16500/100000], Train Loss: 26.0817, Test Loss: 763.5645\n",
      "Epoch [16550/100000], Train Loss: 28.5635, Test Loss: 765.8794\n",
      "Epoch [16600/100000], Train Loss: 24.1158, Test Loss: 762.0716\n",
      "Epoch [16650/100000], Train Loss: 22.2701, Test Loss: 767.3947\n",
      "Epoch [16700/100000], Train Loss: 28.2691, Test Loss: 761.1577\n",
      "Epoch [16750/100000], Train Loss: 25.4580, Test Loss: 765.2548\n",
      "Epoch [16800/100000], Train Loss: 24.7020, Test Loss: 760.6079\n",
      "Epoch [16850/100000], Train Loss: 27.8778, Test Loss: 766.6331\n",
      "Epoch [16900/100000], Train Loss: 26.7777, Test Loss: 765.5750\n",
      "Epoch [16950/100000], Train Loss: 31.1169, Test Loss: 758.8165\n",
      "Epoch [17000/100000], Train Loss: 21.8215, Test Loss: 760.9626\n",
      "Epoch [17050/100000], Train Loss: 21.6179, Test Loss: 763.1153\n",
      "Epoch [17100/100000], Train Loss: 23.8921, Test Loss: 758.1615\n",
      "Epoch [17150/100000], Train Loss: 35.3141, Test Loss: 765.5097\n",
      "Epoch [17200/100000], Train Loss: 25.9109, Test Loss: 763.6754\n",
      "Epoch [17250/100000], Train Loss: 18.8846, Test Loss: 758.7087\n",
      "Epoch [17300/100000], Train Loss: 25.0183, Test Loss: 758.5932\n",
      "Epoch [17350/100000], Train Loss: 27.4193, Test Loss: 766.0064\n",
      "Epoch [17400/100000], Train Loss: 18.0685, Test Loss: 761.4007\n",
      "Epoch [17450/100000], Train Loss: 22.4161, Test Loss: 763.2699\n",
      "Epoch [17500/100000], Train Loss: 20.8778, Test Loss: 764.5877\n",
      "Epoch [17550/100000], Train Loss: 26.3434, Test Loss: 758.4903\n",
      "Epoch [17600/100000], Train Loss: 19.0131, Test Loss: 758.5974\n",
      "Epoch [17650/100000], Train Loss: 24.6112, Test Loss: 765.1713\n",
      "Epoch [17700/100000], Train Loss: 21.0120, Test Loss: 755.2477\n",
      "Epoch [17750/100000], Train Loss: 20.4816, Test Loss: 760.7105\n",
      "Epoch [17800/100000], Train Loss: 24.9106, Test Loss: 762.6584\n",
      "Epoch [17850/100000], Train Loss: 23.5418, Test Loss: 762.5818\n",
      "Epoch [17900/100000], Train Loss: 20.3044, Test Loss: 762.0231\n",
      "Epoch [17950/100000], Train Loss: 23.1738, Test Loss: 759.5731\n",
      "Epoch [18000/100000], Train Loss: 25.6781, Test Loss: 761.4680\n",
      "Epoch [18050/100000], Train Loss: 21.3875, Test Loss: 764.5745\n",
      "Epoch [18100/100000], Train Loss: 23.8043, Test Loss: 765.9408\n",
      "Epoch [18150/100000], Train Loss: 20.8994, Test Loss: 760.6732\n",
      "Epoch [18200/100000], Train Loss: 23.8676, Test Loss: 766.2304\n",
      "Epoch [18250/100000], Train Loss: 22.5092, Test Loss: 760.5183\n",
      "Epoch [18300/100000], Train Loss: 20.6801, Test Loss: 762.0152\n",
      "Epoch [18350/100000], Train Loss: 25.1470, Test Loss: 760.7809\n",
      "Epoch [18400/100000], Train Loss: 24.4501, Test Loss: 764.7893\n",
      "Epoch [18450/100000], Train Loss: 17.0921, Test Loss: 767.1190\n",
      "Epoch [18500/100000], Train Loss: 23.2100, Test Loss: 768.0563\n",
      "Epoch [18550/100000], Train Loss: 31.1052, Test Loss: 763.9326\n",
      "Epoch [18600/100000], Train Loss: 21.2372, Test Loss: 762.4545\n",
      "Epoch [18650/100000], Train Loss: 25.4332, Test Loss: 765.2021\n",
      "Epoch [18700/100000], Train Loss: 27.6901, Test Loss: 763.8574\n",
      "Epoch [18750/100000], Train Loss: 19.4685, Test Loss: 760.7992\n",
      "Epoch [18800/100000], Train Loss: 21.3939, Test Loss: 760.3343\n",
      "Epoch [18850/100000], Train Loss: 21.1832, Test Loss: 764.0898\n",
      "Epoch [18900/100000], Train Loss: 28.3773, Test Loss: 763.0904\n",
      "Epoch [18950/100000], Train Loss: 22.2275, Test Loss: 763.1473\n",
      "Epoch [19000/100000], Train Loss: 22.3436, Test Loss: 758.7245\n",
      "Epoch [19050/100000], Train Loss: 23.5031, Test Loss: 764.8835\n",
      "Epoch [19100/100000], Train Loss: 20.3525, Test Loss: 762.8432\n",
      "Epoch [19150/100000], Train Loss: 23.8158, Test Loss: 765.6447\n",
      "Epoch [19200/100000], Train Loss: 22.9791, Test Loss: 765.6046\n",
      "Epoch [19250/100000], Train Loss: 21.2008, Test Loss: 762.7527\n",
      "Epoch [19300/100000], Train Loss: 22.7428, Test Loss: 762.5477\n",
      "Epoch [19350/100000], Train Loss: 18.5681, Test Loss: 759.7324\n",
      "Epoch [19400/100000], Train Loss: 21.1829, Test Loss: 759.4887\n",
      "Epoch [19450/100000], Train Loss: 23.3550, Test Loss: 767.5591\n",
      "Epoch [19500/100000], Train Loss: 21.3648, Test Loss: 764.7816\n",
      "Epoch [19550/100000], Train Loss: 21.2292, Test Loss: 769.5800\n",
      "Epoch [19600/100000], Train Loss: 28.0893, Test Loss: 760.8890\n",
      "Epoch [19650/100000], Train Loss: 24.6103, Test Loss: 760.8995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19700/100000], Train Loss: 21.4263, Test Loss: 765.8669\n",
      "Epoch [19750/100000], Train Loss: 24.9390, Test Loss: 763.4834\n",
      "Epoch [19800/100000], Train Loss: 26.6026, Test Loss: 764.9747\n",
      "Epoch [19850/100000], Train Loss: 17.4297, Test Loss: 762.9738\n",
      "Epoch [19900/100000], Train Loss: 17.3680, Test Loss: 761.1176\n",
      "Epoch [19950/100000], Train Loss: 23.9449, Test Loss: 766.4032\n",
      "Epoch [20000/100000], Train Loss: 21.8478, Test Loss: 768.3546\n",
      "Epoch [20050/100000], Train Loss: 20.9568, Test Loss: 769.3977\n",
      "Epoch [20100/100000], Train Loss: 23.0300, Test Loss: 764.8815\n",
      "Epoch [20150/100000], Train Loss: 23.5079, Test Loss: 768.1562\n",
      "Epoch [20200/100000], Train Loss: 19.7456, Test Loss: 759.9497\n",
      "Epoch [20250/100000], Train Loss: 23.2896, Test Loss: 767.6894\n",
      "Epoch [20300/100000], Train Loss: 19.6495, Test Loss: 771.5946\n",
      "Epoch [20350/100000], Train Loss: 27.4369, Test Loss: 765.1812\n",
      "Epoch [20400/100000], Train Loss: 17.6549, Test Loss: 763.5180\n",
      "Epoch [20450/100000], Train Loss: 20.5142, Test Loss: 761.7275\n",
      "Epoch [20500/100000], Train Loss: 21.6923, Test Loss: 766.9067\n",
      "Epoch [20550/100000], Train Loss: 27.7830, Test Loss: 762.2008\n",
      "Epoch [20600/100000], Train Loss: 22.5730, Test Loss: 767.8719\n",
      "Epoch [20650/100000], Train Loss: 24.8535, Test Loss: 763.1896\n",
      "Epoch [20700/100000], Train Loss: 21.4562, Test Loss: 766.0836\n",
      "Epoch [20750/100000], Train Loss: 20.8701, Test Loss: 766.4883\n",
      "Epoch [20800/100000], Train Loss: 23.9614, Test Loss: 766.2624\n",
      "Epoch [20850/100000], Train Loss: 21.7640, Test Loss: 764.8188\n",
      "Epoch [20900/100000], Train Loss: 24.5015, Test Loss: 765.8218\n",
      "Epoch [20950/100000], Train Loss: 23.6709, Test Loss: 764.5764\n",
      "Epoch [21000/100000], Train Loss: 20.6670, Test Loss: 763.1998\n",
      "Epoch [21050/100000], Train Loss: 19.0622, Test Loss: 765.9427\n",
      "Epoch [21100/100000], Train Loss: 23.1085, Test Loss: 764.0561\n",
      "Epoch [21150/100000], Train Loss: 15.8223, Test Loss: 763.7354\n",
      "Epoch [21200/100000], Train Loss: 23.3595, Test Loss: 769.9312\n",
      "Epoch [21250/100000], Train Loss: 24.5235, Test Loss: 761.0264\n",
      "Epoch [21300/100000], Train Loss: 20.0060, Test Loss: 763.0783\n",
      "Epoch [21350/100000], Train Loss: 20.0793, Test Loss: 762.0240\n",
      "Epoch [21400/100000], Train Loss: 21.8389, Test Loss: 760.5651\n",
      "Epoch [21450/100000], Train Loss: 24.4297, Test Loss: 765.2598\n",
      "Epoch [21500/100000], Train Loss: 22.7307, Test Loss: 766.8711\n",
      "Epoch [21550/100000], Train Loss: 29.7415, Test Loss: 767.2854\n",
      "Epoch [21600/100000], Train Loss: 20.5715, Test Loss: 766.5862\n",
      "Epoch [21650/100000], Train Loss: 24.2197, Test Loss: 769.0598\n",
      "Epoch [21700/100000], Train Loss: 18.1814, Test Loss: 764.3420\n",
      "Epoch [21750/100000], Train Loss: 17.8836, Test Loss: 767.7600\n",
      "Epoch [21800/100000], Train Loss: 24.0424, Test Loss: 764.5785\n",
      "Epoch [21850/100000], Train Loss: 21.1832, Test Loss: 768.2553\n",
      "Epoch [21900/100000], Train Loss: 20.8902, Test Loss: 767.5516\n",
      "Epoch [21950/100000], Train Loss: 18.3269, Test Loss: 763.6284\n",
      "Epoch [22000/100000], Train Loss: 26.2499, Test Loss: 769.6554\n",
      "Epoch [22050/100000], Train Loss: 19.3424, Test Loss: 767.6334\n",
      "Epoch [22100/100000], Train Loss: 18.9975, Test Loss: 766.3939\n",
      "Epoch [22150/100000], Train Loss: 18.2740, Test Loss: 769.4898\n",
      "Epoch [22200/100000], Train Loss: 20.3946, Test Loss: 766.8561\n",
      "Epoch [22250/100000], Train Loss: 17.5052, Test Loss: 766.7295\n",
      "Epoch [22300/100000], Train Loss: 20.6792, Test Loss: 770.0980\n",
      "Epoch [22350/100000], Train Loss: 21.8622, Test Loss: 767.1891\n",
      "Epoch [22400/100000], Train Loss: 22.6652, Test Loss: 768.5761\n",
      "Epoch [22450/100000], Train Loss: 21.8986, Test Loss: 776.0874\n",
      "Epoch [22500/100000], Train Loss: 17.3666, Test Loss: 768.3336\n",
      "Epoch [22550/100000], Train Loss: 18.3165, Test Loss: 769.6811\n",
      "Epoch [22600/100000], Train Loss: 16.4727, Test Loss: 768.7129\n",
      "Epoch [22650/100000], Train Loss: 17.6340, Test Loss: 770.9715\n",
      "Epoch [22700/100000], Train Loss: 18.4373, Test Loss: 767.1256\n",
      "Epoch [22750/100000], Train Loss: 23.0020, Test Loss: 769.2328\n",
      "Epoch [22800/100000], Train Loss: 20.2694, Test Loss: 770.7925\n",
      "Epoch [22850/100000], Train Loss: 21.4340, Test Loss: 770.6033\n",
      "Epoch [22900/100000], Train Loss: 21.3194, Test Loss: 763.3343\n",
      "Epoch [22950/100000], Train Loss: 23.5321, Test Loss: 767.6457\n",
      "Epoch [23000/100000], Train Loss: 15.6436, Test Loss: 768.5616\n",
      "Epoch [23050/100000], Train Loss: 19.1379, Test Loss: 765.5240\n",
      "Epoch [23100/100000], Train Loss: 18.0100, Test Loss: 766.7031\n",
      "Epoch [23150/100000], Train Loss: 16.4626, Test Loss: 765.6855\n",
      "Epoch [23200/100000], Train Loss: 26.2229, Test Loss: 771.1220\n",
      "Epoch [23250/100000], Train Loss: 15.7921, Test Loss: 771.7462\n",
      "Epoch [23300/100000], Train Loss: 15.2675, Test Loss: 769.3685\n",
      "Epoch [23350/100000], Train Loss: 18.7531, Test Loss: 769.5390\n",
      "Epoch [23400/100000], Train Loss: 17.3582, Test Loss: 769.6851\n",
      "Epoch [23450/100000], Train Loss: 22.2713, Test Loss: 770.4499\n",
      "Epoch [23500/100000], Train Loss: 19.3829, Test Loss: 770.0041\n",
      "Epoch [23550/100000], Train Loss: 18.1756, Test Loss: 772.5250\n",
      "Epoch [23600/100000], Train Loss: 17.1346, Test Loss: 767.1541\n",
      "Epoch [23650/100000], Train Loss: 26.9754, Test Loss: 772.1452\n",
      "Epoch [23700/100000], Train Loss: 17.0909, Test Loss: 768.0475\n",
      "Epoch [23750/100000], Train Loss: 27.5307, Test Loss: 766.5378\n",
      "Epoch [23800/100000], Train Loss: 19.7863, Test Loss: 771.5920\n",
      "Epoch [23850/100000], Train Loss: 17.6604, Test Loss: 767.9008\n",
      "Epoch [23900/100000], Train Loss: 20.6190, Test Loss: 769.5875\n",
      "Epoch [23950/100000], Train Loss: 20.2638, Test Loss: 773.4273\n",
      "Epoch [24000/100000], Train Loss: 21.0654, Test Loss: 770.3041\n",
      "Epoch [24050/100000], Train Loss: 21.2633, Test Loss: 768.4401\n",
      "Epoch [24100/100000], Train Loss: 14.8115, Test Loss: 766.8325\n",
      "Epoch [24150/100000], Train Loss: 20.0705, Test Loss: 767.6691\n",
      "Epoch [24200/100000], Train Loss: 24.9376, Test Loss: 772.3565\n",
      "Epoch [24250/100000], Train Loss: 16.9917, Test Loss: 768.3931\n",
      "Epoch [24300/100000], Train Loss: 11.5457, Test Loss: 769.4792\n",
      "Epoch [24350/100000], Train Loss: 20.5456, Test Loss: 768.7553\n",
      "Epoch [24400/100000], Train Loss: 20.6991, Test Loss: 766.2225\n",
      "Epoch [24450/100000], Train Loss: 18.2243, Test Loss: 771.7884\n",
      "Epoch [24500/100000], Train Loss: 14.4620, Test Loss: 771.4809\n",
      "Epoch [24550/100000], Train Loss: 21.7884, Test Loss: 767.7859\n",
      "Epoch [24600/100000], Train Loss: 18.3696, Test Loss: 777.3965\n",
      "Epoch [24650/100000], Train Loss: 18.5324, Test Loss: 771.3648\n",
      "Epoch [24700/100000], Train Loss: 13.9494, Test Loss: 772.3970\n",
      "Epoch [24750/100000], Train Loss: 18.9225, Test Loss: 772.4229\n",
      "Epoch [24800/100000], Train Loss: 19.7233, Test Loss: 772.2169\n",
      "Epoch [24850/100000], Train Loss: 19.3501, Test Loss: 768.0510\n",
      "Epoch [24900/100000], Train Loss: 13.4449, Test Loss: 774.0197\n",
      "Epoch [24950/100000], Train Loss: 18.1034, Test Loss: 770.4783\n",
      "Epoch [25000/100000], Train Loss: 18.5673, Test Loss: 772.9512\n",
      "Epoch [25050/100000], Train Loss: 21.4423, Test Loss: 770.9744\n",
      "Epoch [25100/100000], Train Loss: 19.9365, Test Loss: 776.7841\n",
      "Epoch [25150/100000], Train Loss: 17.6244, Test Loss: 772.5296\n",
      "Epoch [25200/100000], Train Loss: 15.2651, Test Loss: 767.9478\n",
      "Epoch [25250/100000], Train Loss: 16.3850, Test Loss: 770.9770\n",
      "Epoch [25300/100000], Train Loss: 21.3188, Test Loss: 770.4454\n",
      "Epoch [25350/100000], Train Loss: 14.2676, Test Loss: 772.6950\n",
      "Epoch [25400/100000], Train Loss: 20.2900, Test Loss: 774.8871\n",
      "Epoch [25450/100000], Train Loss: 17.3897, Test Loss: 769.6210\n",
      "Epoch [25500/100000], Train Loss: 20.1657, Test Loss: 775.5983\n",
      "Epoch [25550/100000], Train Loss: 20.8212, Test Loss: 769.5184\n",
      "Epoch [25600/100000], Train Loss: 17.4453, Test Loss: 771.8990\n",
      "Epoch [25650/100000], Train Loss: 22.2800, Test Loss: 773.2128\n",
      "Epoch [25700/100000], Train Loss: 17.6418, Test Loss: 770.9214\n",
      "Epoch [25750/100000], Train Loss: 17.0762, Test Loss: 773.0718\n",
      "Epoch [25800/100000], Train Loss: 19.1639, Test Loss: 771.3398\n",
      "Epoch [25850/100000], Train Loss: 23.5308, Test Loss: 776.1077\n",
      "Epoch [25900/100000], Train Loss: 20.9165, Test Loss: 777.9148\n",
      "Epoch [25950/100000], Train Loss: 16.0587, Test Loss: 771.1359\n",
      "Epoch [26000/100000], Train Loss: 21.0321, Test Loss: 775.5307\n",
      "Epoch [26050/100000], Train Loss: 17.2641, Test Loss: 776.0374\n",
      "Epoch [26100/100000], Train Loss: 25.3425, Test Loss: 777.3894\n",
      "Epoch [26150/100000], Train Loss: 20.1626, Test Loss: 775.6693\n",
      "Epoch [26200/100000], Train Loss: 15.5113, Test Loss: 774.1762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26250/100000], Train Loss: 21.2611, Test Loss: 772.6537\n",
      "Epoch [26300/100000], Train Loss: 19.2011, Test Loss: 770.6215\n",
      "Epoch [26350/100000], Train Loss: 15.3356, Test Loss: 775.4240\n",
      "Epoch [26400/100000], Train Loss: 18.7432, Test Loss: 776.6464\n",
      "Epoch [26450/100000], Train Loss: 13.3623, Test Loss: 773.7194\n",
      "Epoch [26500/100000], Train Loss: 24.7013, Test Loss: 782.0962\n",
      "Epoch [26550/100000], Train Loss: 19.3298, Test Loss: 773.7332\n",
      "Epoch [26600/100000], Train Loss: 21.4440, Test Loss: 775.2859\n",
      "Epoch [26650/100000], Train Loss: 20.0087, Test Loss: 778.3942\n",
      "Epoch [26700/100000], Train Loss: 19.2040, Test Loss: 774.6308\n",
      "Epoch [26750/100000], Train Loss: 16.0475, Test Loss: 776.4893\n",
      "Epoch [26800/100000], Train Loss: 20.5580, Test Loss: 775.3796\n",
      "Epoch [26850/100000], Train Loss: 19.2620, Test Loss: 775.1958\n",
      "Epoch [26900/100000], Train Loss: 18.4360, Test Loss: 776.0643\n",
      "Epoch [26950/100000], Train Loss: 18.0814, Test Loss: 776.2309\n",
      "Epoch [27000/100000], Train Loss: 17.9270, Test Loss: 775.8129\n",
      "Epoch [27050/100000], Train Loss: 20.6063, Test Loss: 780.2444\n",
      "Epoch [27100/100000], Train Loss: 15.3393, Test Loss: 773.6114\n",
      "Epoch [27150/100000], Train Loss: 16.7713, Test Loss: 774.6286\n",
      "Epoch [27200/100000], Train Loss: 18.9340, Test Loss: 775.1779\n",
      "Epoch [27250/100000], Train Loss: 20.1237, Test Loss: 779.5367\n",
      "Epoch [27300/100000], Train Loss: 16.4795, Test Loss: 773.7652\n",
      "Epoch [27350/100000], Train Loss: 13.9319, Test Loss: 776.9465\n",
      "Epoch [27400/100000], Train Loss: 21.4163, Test Loss: 782.5105\n",
      "Epoch [27450/100000], Train Loss: 16.6360, Test Loss: 775.7752\n",
      "Epoch [27500/100000], Train Loss: 16.2747, Test Loss: 780.8790\n",
      "Epoch [27550/100000], Train Loss: 26.2345, Test Loss: 781.0636\n",
      "Epoch [27600/100000], Train Loss: 16.6601, Test Loss: 780.7935\n",
      "Epoch [27650/100000], Train Loss: 16.1732, Test Loss: 780.9728\n",
      "Epoch [27700/100000], Train Loss: 16.6620, Test Loss: 783.2995\n",
      "Epoch [27750/100000], Train Loss: 16.1915, Test Loss: 780.8127\n",
      "Epoch [27800/100000], Train Loss: 17.9278, Test Loss: 777.1868\n",
      "Epoch [27850/100000], Train Loss: 17.2316, Test Loss: 783.1887\n",
      "Epoch [27900/100000], Train Loss: 19.4832, Test Loss: 782.5938\n",
      "Epoch [27950/100000], Train Loss: 17.4251, Test Loss: 782.2519\n",
      "Epoch [28000/100000], Train Loss: 14.7237, Test Loss: 782.4742\n",
      "Epoch [28050/100000], Train Loss: 18.6546, Test Loss: 777.7171\n",
      "Epoch [28100/100000], Train Loss: 19.8456, Test Loss: 786.9853\n",
      "Epoch [28150/100000], Train Loss: 19.6702, Test Loss: 779.9295\n",
      "Epoch [28200/100000], Train Loss: 17.7464, Test Loss: 779.6857\n",
      "Epoch [28250/100000], Train Loss: 13.9478, Test Loss: 779.1458\n",
      "Epoch [28300/100000], Train Loss: 19.6149, Test Loss: 780.8390\n",
      "Epoch [28350/100000], Train Loss: 21.9436, Test Loss: 779.9968\n",
      "Epoch [28400/100000], Train Loss: 19.1001, Test Loss: 784.4937\n",
      "Epoch [28450/100000], Train Loss: 21.5287, Test Loss: 784.5617\n",
      "Epoch [28500/100000], Train Loss: 20.6905, Test Loss: 783.5716\n",
      "Epoch [28550/100000], Train Loss: 16.1141, Test Loss: 784.2281\n",
      "Epoch [28600/100000], Train Loss: 19.0721, Test Loss: 782.7806\n",
      "Epoch [28650/100000], Train Loss: 15.5170, Test Loss: 780.9009\n",
      "Epoch [28700/100000], Train Loss: 16.9410, Test Loss: 781.8273\n",
      "Epoch [28750/100000], Train Loss: 17.7861, Test Loss: 783.3623\n",
      "Epoch [28800/100000], Train Loss: 17.5038, Test Loss: 780.8211\n",
      "Epoch [28850/100000], Train Loss: 17.7117, Test Loss: 783.2619\n",
      "Epoch [28900/100000], Train Loss: 20.8560, Test Loss: 783.4682\n",
      "Epoch [28950/100000], Train Loss: 16.6069, Test Loss: 782.0876\n",
      "Epoch [29000/100000], Train Loss: 15.7247, Test Loss: 782.4673\n",
      "Epoch [29050/100000], Train Loss: 14.3416, Test Loss: 782.8922\n",
      "Epoch [29100/100000], Train Loss: 15.4469, Test Loss: 786.5235\n",
      "Epoch [29150/100000], Train Loss: 15.6785, Test Loss: 781.1563\n",
      "Epoch [29200/100000], Train Loss: 15.8740, Test Loss: 783.9736\n",
      "Epoch [29250/100000], Train Loss: 20.4835, Test Loss: 787.0961\n",
      "Epoch [29300/100000], Train Loss: 19.5367, Test Loss: 783.6325\n",
      "Epoch [29350/100000], Train Loss: 21.6172, Test Loss: 783.0945\n",
      "Epoch [29400/100000], Train Loss: 17.7822, Test Loss: 786.2173\n",
      "Epoch [29450/100000], Train Loss: 16.1667, Test Loss: 785.1767\n",
      "Epoch [29500/100000], Train Loss: 14.1095, Test Loss: 785.3941\n",
      "Epoch [29550/100000], Train Loss: 12.1054, Test Loss: 784.4311\n",
      "Epoch [29600/100000], Train Loss: 20.9496, Test Loss: 782.4375\n",
      "Epoch [29650/100000], Train Loss: 13.3777, Test Loss: 782.4392\n",
      "Epoch [29700/100000], Train Loss: 15.1268, Test Loss: 793.3546\n",
      "Epoch [29750/100000], Train Loss: 21.0342, Test Loss: 789.0804\n",
      "Epoch [29800/100000], Train Loss: 17.1359, Test Loss: 787.8669\n",
      "Epoch [29850/100000], Train Loss: 16.9447, Test Loss: 789.2948\n",
      "Epoch [29900/100000], Train Loss: 16.7213, Test Loss: 789.7379\n",
      "Epoch [29950/100000], Train Loss: 16.7139, Test Loss: 787.7829\n",
      "Epoch [30000/100000], Train Loss: 20.4902, Test Loss: 792.6427\n",
      "Epoch [30050/100000], Train Loss: 13.1546, Test Loss: 785.0328\n",
      "Epoch [30100/100000], Train Loss: 18.5639, Test Loss: 787.9594\n",
      "Epoch [30150/100000], Train Loss: 16.3863, Test Loss: 785.2554\n",
      "Epoch [30200/100000], Train Loss: 16.9106, Test Loss: 787.9616\n",
      "Epoch [30250/100000], Train Loss: 19.0373, Test Loss: 789.1379\n",
      "Epoch [30300/100000], Train Loss: 15.7336, Test Loss: 790.2005\n",
      "Epoch [30350/100000], Train Loss: 11.7090, Test Loss: 789.8387\n",
      "Epoch [30400/100000], Train Loss: 16.0085, Test Loss: 786.2511\n",
      "Epoch [30450/100000], Train Loss: 17.1564, Test Loss: 789.0300\n",
      "Epoch [30500/100000], Train Loss: 17.5946, Test Loss: 786.3611\n",
      "Epoch [30550/100000], Train Loss: 16.7292, Test Loss: 791.5247\n",
      "Epoch [30600/100000], Train Loss: 13.9657, Test Loss: 788.3526\n",
      "Epoch [30650/100000], Train Loss: 13.2216, Test Loss: 792.5451\n",
      "Epoch [30700/100000], Train Loss: 16.8938, Test Loss: 788.4231\n",
      "Epoch [30750/100000], Train Loss: 20.5718, Test Loss: 790.3658\n",
      "Epoch [30800/100000], Train Loss: 16.5881, Test Loss: 792.6243\n",
      "Epoch [30850/100000], Train Loss: 15.7345, Test Loss: 791.1391\n",
      "Epoch [30900/100000], Train Loss: 16.8405, Test Loss: 792.2737\n",
      "Epoch [30950/100000], Train Loss: 16.5504, Test Loss: 792.2018\n",
      "Epoch [31000/100000], Train Loss: 13.9032, Test Loss: 790.9384\n",
      "Epoch [31050/100000], Train Loss: 11.7143, Test Loss: 793.7735\n",
      "Epoch [31100/100000], Train Loss: 18.6922, Test Loss: 793.5769\n",
      "Epoch [31150/100000], Train Loss: 21.2891, Test Loss: 792.4188\n",
      "Epoch [31200/100000], Train Loss: 17.4441, Test Loss: 790.9982\n",
      "Epoch [31250/100000], Train Loss: 15.3624, Test Loss: 794.8878\n",
      "Epoch [31300/100000], Train Loss: 17.0712, Test Loss: 791.0590\n",
      "Epoch [31350/100000], Train Loss: 17.2378, Test Loss: 789.3167\n",
      "Epoch [31400/100000], Train Loss: 15.1412, Test Loss: 787.8986\n",
      "Epoch [31450/100000], Train Loss: 15.6240, Test Loss: 791.2602\n",
      "Epoch [31500/100000], Train Loss: 13.7475, Test Loss: 791.0964\n",
      "Epoch [31550/100000], Train Loss: 26.1625, Test Loss: 794.7350\n",
      "Epoch [31600/100000], Train Loss: 12.1738, Test Loss: 788.5719\n",
      "Epoch [31650/100000], Train Loss: 14.1793, Test Loss: 792.5936\n",
      "Epoch [31700/100000], Train Loss: 22.0725, Test Loss: 793.9205\n",
      "Epoch [31750/100000], Train Loss: 17.6316, Test Loss: 795.7092\n",
      "Epoch [31800/100000], Train Loss: 16.3198, Test Loss: 790.3817\n",
      "Epoch [31850/100000], Train Loss: 16.5179, Test Loss: 790.5499\n",
      "Epoch [31900/100000], Train Loss: 15.3791, Test Loss: 790.1668\n",
      "Epoch [31950/100000], Train Loss: 19.0126, Test Loss: 790.8159\n",
      "Epoch [32000/100000], Train Loss: 14.9617, Test Loss: 792.0792\n",
      "Epoch [32050/100000], Train Loss: 18.3352, Test Loss: 794.1426\n",
      "Epoch [32100/100000], Train Loss: 14.0214, Test Loss: 793.0212\n",
      "Epoch [32150/100000], Train Loss: 13.1595, Test Loss: 793.5147\n",
      "Epoch [32200/100000], Train Loss: 15.5183, Test Loss: 794.7354\n",
      "Epoch [32250/100000], Train Loss: 15.8794, Test Loss: 796.3418\n",
      "Epoch [32300/100000], Train Loss: 17.9130, Test Loss: 792.7835\n",
      "Epoch [32350/100000], Train Loss: 17.5123, Test Loss: 795.1279\n",
      "Epoch [32400/100000], Train Loss: 19.5880, Test Loss: 799.3541\n",
      "Epoch [32450/100000], Train Loss: 20.2879, Test Loss: 793.5004\n",
      "Epoch [32500/100000], Train Loss: 16.1547, Test Loss: 794.0808\n",
      "Epoch [32550/100000], Train Loss: 15.3708, Test Loss: 795.0446\n",
      "Epoch [32600/100000], Train Loss: 16.6776, Test Loss: 798.8417\n",
      "Epoch [32650/100000], Train Loss: 18.7860, Test Loss: 793.9013\n",
      "Epoch [32700/100000], Train Loss: 17.8126, Test Loss: 792.9647\n",
      "Epoch [32750/100000], Train Loss: 20.1211, Test Loss: 794.2861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32800/100000], Train Loss: 15.6348, Test Loss: 795.9634\n",
      "Epoch [32850/100000], Train Loss: 13.0707, Test Loss: 793.2409\n",
      "Epoch [32900/100000], Train Loss: 13.6484, Test Loss: 801.4042\n",
      "Epoch [32950/100000], Train Loss: 17.5721, Test Loss: 798.5374\n",
      "Epoch [33000/100000], Train Loss: 16.7631, Test Loss: 802.3509\n",
      "Epoch [33050/100000], Train Loss: 16.2063, Test Loss: 795.0887\n",
      "Epoch [33100/100000], Train Loss: 19.8593, Test Loss: 797.4921\n",
      "Epoch [33150/100000], Train Loss: 12.5737, Test Loss: 795.8950\n",
      "Epoch [33200/100000], Train Loss: 15.2236, Test Loss: 796.4408\n",
      "Epoch [33250/100000], Train Loss: 14.4051, Test Loss: 799.5036\n",
      "Epoch [33300/100000], Train Loss: 17.0406, Test Loss: 803.5928\n",
      "Epoch [33350/100000], Train Loss: 16.9305, Test Loss: 801.0862\n",
      "Epoch [33400/100000], Train Loss: 17.1652, Test Loss: 799.5258\n",
      "Epoch [33450/100000], Train Loss: 20.4284, Test Loss: 805.2910\n",
      "Epoch [33500/100000], Train Loss: 13.4631, Test Loss: 802.5940\n",
      "Epoch [33550/100000], Train Loss: 16.6641, Test Loss: 796.5765\n",
      "Epoch [33600/100000], Train Loss: 13.6226, Test Loss: 793.7974\n",
      "Epoch [33650/100000], Train Loss: 14.0273, Test Loss: 797.4621\n",
      "Epoch [33700/100000], Train Loss: 19.4823, Test Loss: 798.7090\n",
      "Epoch [33750/100000], Train Loss: 15.0166, Test Loss: 795.7921\n",
      "Epoch [33800/100000], Train Loss: 16.4048, Test Loss: 799.8013\n",
      "Epoch [33850/100000], Train Loss: 17.5818, Test Loss: 795.8764\n",
      "Epoch [33900/100000], Train Loss: 17.8737, Test Loss: 801.8769\n",
      "Epoch [33950/100000], Train Loss: 17.2205, Test Loss: 800.5926\n",
      "Epoch [34000/100000], Train Loss: 14.5696, Test Loss: 800.3557\n",
      "Epoch [34050/100000], Train Loss: 16.8933, Test Loss: 795.9079\n",
      "Epoch [34100/100000], Train Loss: 17.9109, Test Loss: 800.5459\n",
      "Epoch [34150/100000], Train Loss: 16.1265, Test Loss: 800.6533\n",
      "Epoch [34200/100000], Train Loss: 18.2505, Test Loss: 800.2805\n",
      "Epoch [34250/100000], Train Loss: 15.0250, Test Loss: 797.3001\n",
      "Epoch [34300/100000], Train Loss: 13.0459, Test Loss: 798.8341\n",
      "Epoch [34350/100000], Train Loss: 17.1558, Test Loss: 798.3328\n",
      "Epoch [34400/100000], Train Loss: 19.5564, Test Loss: 801.3373\n",
      "Epoch [34450/100000], Train Loss: 14.4639, Test Loss: 798.7155\n",
      "Epoch [34500/100000], Train Loss: 16.7052, Test Loss: 800.4192\n",
      "Epoch [34550/100000], Train Loss: 15.5621, Test Loss: 799.7081\n",
      "Epoch [34600/100000], Train Loss: 24.5469, Test Loss: 804.4199\n",
      "Epoch [34650/100000], Train Loss: 14.9372, Test Loss: 801.7572\n",
      "Epoch [34700/100000], Train Loss: 17.9394, Test Loss: 803.8116\n",
      "Epoch [34750/100000], Train Loss: 15.6392, Test Loss: 804.1619\n",
      "Epoch [34800/100000], Train Loss: 15.8334, Test Loss: 803.2743\n",
      "Epoch [34850/100000], Train Loss: 14.2430, Test Loss: 804.3112\n",
      "Epoch [34900/100000], Train Loss: 14.8801, Test Loss: 800.3266\n",
      "Epoch [34950/100000], Train Loss: 13.6790, Test Loss: 801.4561\n",
      "Epoch [35000/100000], Train Loss: 10.9234, Test Loss: 803.4175\n",
      "Epoch [35050/100000], Train Loss: 16.5352, Test Loss: 803.3086\n",
      "Epoch [35100/100000], Train Loss: 16.8478, Test Loss: 806.9213\n",
      "Epoch [35150/100000], Train Loss: 21.2227, Test Loss: 804.0445\n",
      "Epoch [35200/100000], Train Loss: 14.5789, Test Loss: 799.6775\n",
      "Epoch [35250/100000], Train Loss: 13.0199, Test Loss: 800.5491\n",
      "Epoch [35300/100000], Train Loss: 12.2428, Test Loss: 799.8202\n",
      "Epoch [35350/100000], Train Loss: 13.3723, Test Loss: 801.9812\n",
      "Epoch [35400/100000], Train Loss: 18.5470, Test Loss: 804.1935\n",
      "Epoch [35450/100000], Train Loss: 15.7734, Test Loss: 803.6802\n",
      "Epoch [35500/100000], Train Loss: 17.9032, Test Loss: 803.6006\n",
      "Epoch [35550/100000], Train Loss: 13.4165, Test Loss: 799.9091\n",
      "Epoch [35600/100000], Train Loss: 17.1070, Test Loss: 802.5938\n",
      "Epoch [35650/100000], Train Loss: 13.6228, Test Loss: 803.7147\n",
      "Epoch [35700/100000], Train Loss: 18.7387, Test Loss: 807.1103\n",
      "Epoch [35750/100000], Train Loss: 16.6658, Test Loss: 804.0085\n",
      "Epoch [35800/100000], Train Loss: 14.9683, Test Loss: 802.0446\n",
      "Epoch [35850/100000], Train Loss: 14.4376, Test Loss: 797.9705\n",
      "Epoch [35900/100000], Train Loss: 15.6273, Test Loss: 802.2149\n",
      "Epoch [35950/100000], Train Loss: 17.5237, Test Loss: 800.7991\n",
      "Epoch [36000/100000], Train Loss: 14.0406, Test Loss: 801.3126\n",
      "Epoch [36050/100000], Train Loss: 16.3564, Test Loss: 798.9863\n",
      "Epoch [36100/100000], Train Loss: 16.6905, Test Loss: 808.8125\n",
      "Epoch [36150/100000], Train Loss: 15.7231, Test Loss: 801.8427\n",
      "Epoch [36200/100000], Train Loss: 15.5842, Test Loss: 799.0769\n",
      "Epoch [36250/100000], Train Loss: 11.9273, Test Loss: 803.8011\n",
      "Epoch [36300/100000], Train Loss: 15.9019, Test Loss: 805.7021\n",
      "Epoch [36350/100000], Train Loss: 12.3472, Test Loss: 799.7669\n",
      "Epoch [36400/100000], Train Loss: 15.3531, Test Loss: 802.1305\n",
      "Epoch [36450/100000], Train Loss: 16.9077, Test Loss: 804.8627\n",
      "Epoch [36500/100000], Train Loss: 17.5911, Test Loss: 802.5521\n",
      "Epoch [36550/100000], Train Loss: 15.7953, Test Loss: 802.0960\n",
      "Epoch [36600/100000], Train Loss: 17.6386, Test Loss: 802.1274\n",
      "Epoch [36650/100000], Train Loss: 14.2821, Test Loss: 803.6680\n",
      "Epoch [36700/100000], Train Loss: 12.9135, Test Loss: 803.1018\n",
      "Epoch [36750/100000], Train Loss: 12.8987, Test Loss: 804.7769\n",
      "Epoch [36800/100000], Train Loss: 13.2581, Test Loss: 800.8194\n",
      "Epoch [36850/100000], Train Loss: 13.0550, Test Loss: 805.6670\n",
      "Epoch [36900/100000], Train Loss: 14.9559, Test Loss: 801.2493\n",
      "Epoch [36950/100000], Train Loss: 13.6431, Test Loss: 802.9926\n",
      "Epoch [37000/100000], Train Loss: 14.4021, Test Loss: 800.3799\n",
      "Epoch [37050/100000], Train Loss: 13.0403, Test Loss: 803.3045\n",
      "Epoch [37100/100000], Train Loss: 15.4903, Test Loss: 803.1461\n",
      "Epoch [37150/100000], Train Loss: 15.0064, Test Loss: 804.1988\n",
      "Epoch [37200/100000], Train Loss: 13.8435, Test Loss: 806.6320\n",
      "Epoch [37250/100000], Train Loss: 12.6292, Test Loss: 803.9478\n",
      "Epoch [37300/100000], Train Loss: 15.5595, Test Loss: 803.1900\n",
      "Epoch [37350/100000], Train Loss: 16.9454, Test Loss: 802.6154\n",
      "Epoch [37400/100000], Train Loss: 12.8645, Test Loss: 802.4760\n",
      "Epoch [37450/100000], Train Loss: 13.1490, Test Loss: 802.8771\n",
      "Epoch [37500/100000], Train Loss: 17.0085, Test Loss: 804.6862\n",
      "Epoch [37550/100000], Train Loss: 11.7261, Test Loss: 808.3805\n",
      "Epoch [37600/100000], Train Loss: 12.1039, Test Loss: 803.6921\n",
      "Epoch [37650/100000], Train Loss: 14.5353, Test Loss: 803.5436\n",
      "Epoch [37700/100000], Train Loss: 16.1983, Test Loss: 800.4072\n",
      "Epoch [37750/100000], Train Loss: 13.1660, Test Loss: 802.0342\n",
      "Epoch [37800/100000], Train Loss: 14.6215, Test Loss: 803.8233\n",
      "Epoch [37850/100000], Train Loss: 21.1059, Test Loss: 802.9745\n",
      "Epoch [37900/100000], Train Loss: 20.4816, Test Loss: 803.1312\n",
      "Epoch [37950/100000], Train Loss: 10.2290, Test Loss: 801.6467\n",
      "Epoch [38000/100000], Train Loss: 15.2315, Test Loss: 806.7251\n",
      "Epoch [38050/100000], Train Loss: 16.0201, Test Loss: 802.9861\n",
      "Epoch [38100/100000], Train Loss: 12.5867, Test Loss: 804.3265\n",
      "Epoch [38150/100000], Train Loss: 16.8141, Test Loss: 804.4536\n",
      "Epoch [38200/100000], Train Loss: 14.0264, Test Loss: 803.1846\n",
      "Epoch [38250/100000], Train Loss: 16.6386, Test Loss: 805.9176\n",
      "Epoch [38300/100000], Train Loss: 11.8046, Test Loss: 804.1307\n",
      "Epoch [38350/100000], Train Loss: 12.3088, Test Loss: 804.9783\n",
      "Epoch [38400/100000], Train Loss: 13.7514, Test Loss: 803.2816\n",
      "Epoch [38450/100000], Train Loss: 11.5924, Test Loss: 802.8589\n",
      "Epoch [38500/100000], Train Loss: 16.1018, Test Loss: 803.7242\n",
      "Epoch [38550/100000], Train Loss: 16.0181, Test Loss: 805.7209\n",
      "Epoch [38600/100000], Train Loss: 11.7732, Test Loss: 804.4340\n",
      "Epoch [38650/100000], Train Loss: 11.9409, Test Loss: 801.5850\n",
      "Epoch [38700/100000], Train Loss: 13.5404, Test Loss: 804.1597\n",
      "Epoch [38750/100000], Train Loss: 13.6602, Test Loss: 807.2548\n",
      "Epoch [38800/100000], Train Loss: 12.1712, Test Loss: 808.6361\n",
      "Epoch [38850/100000], Train Loss: 13.6678, Test Loss: 803.8179\n",
      "Epoch [38900/100000], Train Loss: 12.9847, Test Loss: 802.7354\n",
      "Epoch [38950/100000], Train Loss: 12.9752, Test Loss: 808.0368\n",
      "Epoch [39000/100000], Train Loss: 14.4050, Test Loss: 809.7704\n",
      "Epoch [39050/100000], Train Loss: 19.7364, Test Loss: 808.5317\n",
      "Epoch [39100/100000], Train Loss: 14.3953, Test Loss: 808.0352\n",
      "Epoch [39150/100000], Train Loss: 12.6897, Test Loss: 804.4063\n",
      "Epoch [39200/100000], Train Loss: 14.7845, Test Loss: 803.2205\n",
      "Epoch [39250/100000], Train Loss: 15.5531, Test Loss: 807.9155\n",
      "Epoch [39300/100000], Train Loss: 14.8966, Test Loss: 803.5607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39350/100000], Train Loss: 13.3343, Test Loss: 806.6511\n",
      "Epoch [39400/100000], Train Loss: 10.9314, Test Loss: 803.3206\n",
      "Epoch [39450/100000], Train Loss: 10.3870, Test Loss: 802.5782\n",
      "Epoch [39500/100000], Train Loss: 13.7323, Test Loss: 806.5920\n",
      "Epoch [39550/100000], Train Loss: 13.5767, Test Loss: 804.1651\n",
      "Epoch [39600/100000], Train Loss: 11.6027, Test Loss: 806.4788\n",
      "Epoch [39650/100000], Train Loss: 14.6559, Test Loss: 809.2175\n",
      "Epoch [39700/100000], Train Loss: 13.6152, Test Loss: 807.4276\n",
      "Epoch [39750/100000], Train Loss: 16.3522, Test Loss: 807.0123\n",
      "Epoch [39800/100000], Train Loss: 13.9610, Test Loss: 806.6569\n",
      "Epoch [39850/100000], Train Loss: 16.1056, Test Loss: 806.4211\n",
      "Epoch [39900/100000], Train Loss: 12.0596, Test Loss: 806.0380\n",
      "Epoch [39950/100000], Train Loss: 13.9099, Test Loss: 805.8289\n",
      "Epoch [40000/100000], Train Loss: 15.3290, Test Loss: 807.2734\n",
      "Epoch [40050/100000], Train Loss: 13.0633, Test Loss: 807.3410\n",
      "Epoch [40100/100000], Train Loss: 13.3135, Test Loss: 806.0681\n",
      "Epoch [40150/100000], Train Loss: 14.0724, Test Loss: 805.6650\n",
      "Epoch [40200/100000], Train Loss: 18.2934, Test Loss: 810.0106\n",
      "Epoch [40250/100000], Train Loss: 16.4038, Test Loss: 806.6971\n",
      "Epoch [40300/100000], Train Loss: 17.6235, Test Loss: 810.3678\n",
      "Epoch [40350/100000], Train Loss: 13.9225, Test Loss: 806.6237\n",
      "Epoch [40400/100000], Train Loss: 11.7067, Test Loss: 812.2668\n",
      "Epoch [40450/100000], Train Loss: 14.5427, Test Loss: 809.4201\n",
      "Epoch [40500/100000], Train Loss: 16.3271, Test Loss: 810.3754\n",
      "Epoch [40550/100000], Train Loss: 17.1558, Test Loss: 808.8384\n",
      "Epoch [40600/100000], Train Loss: 10.7927, Test Loss: 808.6257\n",
      "Epoch [40650/100000], Train Loss: 12.3463, Test Loss: 808.8167\n",
      "Epoch [40700/100000], Train Loss: 16.0241, Test Loss: 811.3693\n",
      "Epoch [40750/100000], Train Loss: 13.9749, Test Loss: 809.7200\n",
      "Epoch [40800/100000], Train Loss: 15.9330, Test Loss: 806.6011\n",
      "Epoch [40850/100000], Train Loss: 12.8316, Test Loss: 808.2713\n",
      "Epoch [40900/100000], Train Loss: 14.2912, Test Loss: 810.1599\n",
      "Epoch [40950/100000], Train Loss: 16.8818, Test Loss: 811.9549\n",
      "Epoch [41000/100000], Train Loss: 14.1128, Test Loss: 810.4777\n",
      "Epoch [41050/100000], Train Loss: 13.2577, Test Loss: 809.1487\n",
      "Epoch [41100/100000], Train Loss: 15.4578, Test Loss: 811.7797\n",
      "Epoch [41150/100000], Train Loss: 14.0773, Test Loss: 817.7717\n",
      "Epoch [41200/100000], Train Loss: 16.1723, Test Loss: 810.3353\n",
      "Epoch [41250/100000], Train Loss: 17.3950, Test Loss: 811.2161\n",
      "Epoch [41300/100000], Train Loss: 15.5595, Test Loss: 812.1760\n",
      "Epoch [41350/100000], Train Loss: 12.6665, Test Loss: 809.0136\n",
      "Epoch [41400/100000], Train Loss: 14.3468, Test Loss: 808.1616\n",
      "Epoch [41450/100000], Train Loss: 15.5314, Test Loss: 811.3802\n",
      "Epoch [41500/100000], Train Loss: 13.9122, Test Loss: 813.9218\n",
      "Epoch [41550/100000], Train Loss: 16.4328, Test Loss: 813.4961\n",
      "Epoch [41600/100000], Train Loss: 11.1486, Test Loss: 814.1320\n",
      "Epoch [41650/100000], Train Loss: 16.4972, Test Loss: 814.0372\n",
      "Epoch [41700/100000], Train Loss: 14.8909, Test Loss: 812.4848\n",
      "Epoch [41750/100000], Train Loss: 14.2885, Test Loss: 810.3461\n",
      "Epoch [41800/100000], Train Loss: 15.8463, Test Loss: 810.3452\n",
      "Epoch [41850/100000], Train Loss: 10.9128, Test Loss: 812.2598\n",
      "Epoch [41900/100000], Train Loss: 9.7820, Test Loss: 810.5027\n",
      "Epoch [41950/100000], Train Loss: 15.6040, Test Loss: 812.3176\n",
      "Epoch [42000/100000], Train Loss: 14.6451, Test Loss: 811.0912\n",
      "Epoch [42050/100000], Train Loss: 14.7128, Test Loss: 810.6428\n",
      "Epoch [42100/100000], Train Loss: 12.5810, Test Loss: 810.0338\n",
      "Epoch [42150/100000], Train Loss: 11.6101, Test Loss: 809.6380\n",
      "Epoch [42200/100000], Train Loss: 15.8331, Test Loss: 812.8608\n",
      "Epoch [42250/100000], Train Loss: 13.9960, Test Loss: 814.7609\n",
      "Epoch [42300/100000], Train Loss: 14.4880, Test Loss: 809.3002\n",
      "Epoch [42350/100000], Train Loss: 14.8401, Test Loss: 809.7553\n",
      "Epoch [42400/100000], Train Loss: 17.0576, Test Loss: 808.7419\n",
      "Epoch [42450/100000], Train Loss: 12.0938, Test Loss: 809.7402\n",
      "Epoch [42500/100000], Train Loss: 13.5097, Test Loss: 813.7079\n",
      "Epoch [42550/100000], Train Loss: 14.3390, Test Loss: 811.5431\n",
      "Epoch [42600/100000], Train Loss: 11.5170, Test Loss: 812.3504\n",
      "Epoch [42650/100000], Train Loss: 12.1431, Test Loss: 812.5197\n",
      "Epoch [42700/100000], Train Loss: 10.7436, Test Loss: 814.2220\n",
      "Epoch [42750/100000], Train Loss: 14.1092, Test Loss: 815.9890\n",
      "Epoch [42800/100000], Train Loss: 13.1101, Test Loss: 811.1181\n",
      "Epoch [42850/100000], Train Loss: 12.7727, Test Loss: 817.1363\n",
      "Epoch [42900/100000], Train Loss: 11.3069, Test Loss: 810.3723\n",
      "Epoch [42950/100000], Train Loss: 10.1783, Test Loss: 811.7059\n",
      "Epoch [43000/100000], Train Loss: 11.0858, Test Loss: 812.0802\n",
      "Epoch [43050/100000], Train Loss: 12.5415, Test Loss: 811.2440\n",
      "Epoch [43100/100000], Train Loss: 9.3167, Test Loss: 811.2748\n",
      "Epoch [43150/100000], Train Loss: 12.4208, Test Loss: 813.6317\n",
      "Epoch [43200/100000], Train Loss: 13.2126, Test Loss: 820.9477\n",
      "Epoch [43250/100000], Train Loss: 13.0633, Test Loss: 812.6823\n",
      "Epoch [43300/100000], Train Loss: 11.7313, Test Loss: 810.7921\n",
      "Epoch [43350/100000], Train Loss: 12.4351, Test Loss: 807.8270\n",
      "Epoch [43400/100000], Train Loss: 11.9261, Test Loss: 810.7818\n",
      "Epoch [43450/100000], Train Loss: 10.0179, Test Loss: 814.0438\n",
      "Epoch [43500/100000], Train Loss: 12.1537, Test Loss: 813.6980\n",
      "Epoch [43550/100000], Train Loss: 14.8400, Test Loss: 815.4005\n",
      "Epoch [43600/100000], Train Loss: 12.7978, Test Loss: 810.8517\n",
      "Epoch [43650/100000], Train Loss: 12.1823, Test Loss: 813.2484\n",
      "Epoch [43700/100000], Train Loss: 12.2250, Test Loss: 809.8549\n",
      "Epoch [43750/100000], Train Loss: 15.1033, Test Loss: 814.8590\n",
      "Epoch [43800/100000], Train Loss: 8.3451, Test Loss: 810.9562\n",
      "Epoch [43850/100000], Train Loss: 12.5117, Test Loss: 814.5630\n",
      "Epoch [43900/100000], Train Loss: 11.7059, Test Loss: 810.7730\n",
      "Epoch [43950/100000], Train Loss: 11.3732, Test Loss: 814.4564\n",
      "Epoch [44000/100000], Train Loss: 12.3684, Test Loss: 814.1710\n",
      "Epoch [44050/100000], Train Loss: 11.9554, Test Loss: 813.3189\n",
      "Epoch [44100/100000], Train Loss: 12.4281, Test Loss: 811.7306\n",
      "Epoch [44150/100000], Train Loss: 11.4705, Test Loss: 815.3597\n",
      "Epoch [44200/100000], Train Loss: 11.3694, Test Loss: 816.2608\n",
      "Epoch [44250/100000], Train Loss: 10.9600, Test Loss: 814.1634\n",
      "Epoch [44300/100000], Train Loss: 13.0274, Test Loss: 817.1968\n",
      "Epoch [44350/100000], Train Loss: 14.4296, Test Loss: 812.1715\n",
      "Epoch [44400/100000], Train Loss: 9.1775, Test Loss: 816.7854\n",
      "Epoch [44450/100000], Train Loss: 14.9983, Test Loss: 818.3613\n",
      "Epoch [44500/100000], Train Loss: 12.1116, Test Loss: 813.8756\n",
      "Epoch [44550/100000], Train Loss: 12.9655, Test Loss: 816.8963\n",
      "Epoch [44600/100000], Train Loss: 9.2986, Test Loss: 813.3547\n",
      "Epoch [44650/100000], Train Loss: 14.8851, Test Loss: 815.7477\n",
      "Epoch [44700/100000], Train Loss: 13.3099, Test Loss: 815.9140\n",
      "Epoch [44750/100000], Train Loss: 13.5562, Test Loss: 817.4288\n",
      "Epoch [44800/100000], Train Loss: 12.7345, Test Loss: 815.5942\n",
      "Epoch [44850/100000], Train Loss: 20.4422, Test Loss: 819.9953\n",
      "Epoch [44900/100000], Train Loss: 12.8013, Test Loss: 817.5425\n",
      "Epoch [44950/100000], Train Loss: 10.4736, Test Loss: 817.3210\n",
      "Epoch [45000/100000], Train Loss: 14.6888, Test Loss: 815.0572\n",
      "Epoch [45050/100000], Train Loss: 12.6350, Test Loss: 815.0499\n",
      "Epoch [45100/100000], Train Loss: 8.6438, Test Loss: 818.2328\n",
      "Epoch [45150/100000], Train Loss: 11.5393, Test Loss: 817.8871\n",
      "Epoch [45200/100000], Train Loss: 9.8922, Test Loss: 815.1720\n",
      "Epoch [45250/100000], Train Loss: 12.8792, Test Loss: 815.9354\n",
      "Epoch [45300/100000], Train Loss: 11.3408, Test Loss: 820.2639\n",
      "Epoch [45350/100000], Train Loss: 11.4442, Test Loss: 815.8935\n",
      "Epoch [45400/100000], Train Loss: 13.6138, Test Loss: 817.1530\n",
      "Epoch [45450/100000], Train Loss: 15.1684, Test Loss: 814.6213\n",
      "Epoch [45500/100000], Train Loss: 11.7619, Test Loss: 822.1384\n",
      "Epoch [45550/100000], Train Loss: 14.8433, Test Loss: 817.0931\n",
      "Epoch [45600/100000], Train Loss: 15.1891, Test Loss: 818.3625\n",
      "Epoch [45650/100000], Train Loss: 13.3625, Test Loss: 817.4330\n",
      "Epoch [45700/100000], Train Loss: 15.3502, Test Loss: 819.1043\n",
      "Epoch [45750/100000], Train Loss: 13.5273, Test Loss: 819.1638\n",
      "Epoch [45800/100000], Train Loss: 10.8891, Test Loss: 817.8436\n",
      "Epoch [45850/100000], Train Loss: 12.4491, Test Loss: 816.3096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45900/100000], Train Loss: 12.3643, Test Loss: 819.8493\n",
      "Epoch [45950/100000], Train Loss: 11.0729, Test Loss: 819.6127\n",
      "Epoch [46000/100000], Train Loss: 13.7660, Test Loss: 819.0187\n",
      "Epoch [46050/100000], Train Loss: 14.2793, Test Loss: 819.8258\n",
      "Epoch [46100/100000], Train Loss: 16.4138, Test Loss: 817.2827\n",
      "Epoch [46150/100000], Train Loss: 10.3067, Test Loss: 822.3916\n",
      "Epoch [46200/100000], Train Loss: 14.0350, Test Loss: 816.0141\n",
      "Epoch [46250/100000], Train Loss: 13.5212, Test Loss: 815.5628\n",
      "Epoch [46300/100000], Train Loss: 13.3229, Test Loss: 818.3044\n",
      "Epoch [46350/100000], Train Loss: 11.6721, Test Loss: 818.4419\n",
      "Epoch [46400/100000], Train Loss: 14.6098, Test Loss: 818.0814\n",
      "Epoch [46450/100000], Train Loss: 9.0600, Test Loss: 817.4619\n",
      "Epoch [46500/100000], Train Loss: 9.5509, Test Loss: 818.2414\n",
      "Epoch [46550/100000], Train Loss: 12.6075, Test Loss: 815.7181\n",
      "Epoch [46600/100000], Train Loss: 11.3929, Test Loss: 819.6703\n",
      "Epoch [46650/100000], Train Loss: 11.8001, Test Loss: 816.6772\n",
      "Epoch [46700/100000], Train Loss: 10.6654, Test Loss: 818.6051\n",
      "Epoch [46750/100000], Train Loss: 11.0014, Test Loss: 817.8782\n",
      "Epoch [46800/100000], Train Loss: 11.9468, Test Loss: 820.6855\n",
      "Epoch [46850/100000], Train Loss: 14.7982, Test Loss: 820.8329\n",
      "Epoch [46900/100000], Train Loss: 16.5743, Test Loss: 821.4899\n",
      "Epoch [46950/100000], Train Loss: 13.0577, Test Loss: 820.2857\n",
      "Epoch [47000/100000], Train Loss: 11.8922, Test Loss: 821.1714\n",
      "Epoch [47050/100000], Train Loss: 12.5278, Test Loss: 816.3967\n",
      "Epoch [47100/100000], Train Loss: 14.6942, Test Loss: 820.7386\n",
      "Epoch [47150/100000], Train Loss: 11.4481, Test Loss: 820.5167\n",
      "Epoch [47200/100000], Train Loss: 13.2286, Test Loss: 822.0927\n",
      "Epoch [47250/100000], Train Loss: 11.5217, Test Loss: 818.9903\n",
      "Epoch [47300/100000], Train Loss: 12.9320, Test Loss: 816.6092\n",
      "Epoch [47350/100000], Train Loss: 12.0808, Test Loss: 817.8122\n",
      "Epoch [47400/100000], Train Loss: 14.0702, Test Loss: 818.6354\n",
      "Epoch [47450/100000], Train Loss: 13.2513, Test Loss: 820.7754\n",
      "Epoch [47500/100000], Train Loss: 14.4799, Test Loss: 818.6972\n",
      "Epoch [47550/100000], Train Loss: 10.2042, Test Loss: 821.8364\n",
      "Epoch [47600/100000], Train Loss: 10.9398, Test Loss: 818.1631\n",
      "Epoch [47650/100000], Train Loss: 12.5088, Test Loss: 818.0317\n",
      "Epoch [47700/100000], Train Loss: 12.7898, Test Loss: 824.0421\n",
      "Epoch [47750/100000], Train Loss: 8.5846, Test Loss: 819.1326\n",
      "Epoch [47800/100000], Train Loss: 14.6551, Test Loss: 817.1764\n",
      "Epoch [47850/100000], Train Loss: 10.2608, Test Loss: 818.0269\n",
      "Epoch [47900/100000], Train Loss: 11.9392, Test Loss: 817.2481\n",
      "Epoch [47950/100000], Train Loss: 11.5937, Test Loss: 819.6145\n",
      "Epoch [48000/100000], Train Loss: 12.1102, Test Loss: 824.6658\n",
      "Epoch [48050/100000], Train Loss: 11.1805, Test Loss: 817.2538\n",
      "Epoch [48100/100000], Train Loss: 10.8932, Test Loss: 819.1745\n",
      "Epoch [48150/100000], Train Loss: 13.5011, Test Loss: 821.3400\n",
      "Epoch [48200/100000], Train Loss: 11.0464, Test Loss: 817.9650\n",
      "Epoch [48250/100000], Train Loss: 9.9967, Test Loss: 818.5486\n",
      "Epoch [48300/100000], Train Loss: 11.5317, Test Loss: 821.2874\n",
      "Epoch [48350/100000], Train Loss: 11.7738, Test Loss: 816.9035\n",
      "Epoch [48400/100000], Train Loss: 12.6395, Test Loss: 819.3954\n",
      "Epoch [48450/100000], Train Loss: 12.3779, Test Loss: 817.9976\n",
      "Epoch [48500/100000], Train Loss: 10.3796, Test Loss: 820.6136\n",
      "Epoch [48550/100000], Train Loss: 12.1018, Test Loss: 819.1568\n",
      "Epoch [48600/100000], Train Loss: 15.6233, Test Loss: 823.8998\n",
      "Epoch [48650/100000], Train Loss: 11.5747, Test Loss: 820.6878\n",
      "Epoch [48700/100000], Train Loss: 11.5639, Test Loss: 819.8797\n",
      "Epoch [48750/100000], Train Loss: 11.1313, Test Loss: 819.9511\n",
      "Epoch [48800/100000], Train Loss: 14.4180, Test Loss: 818.3288\n",
      "Epoch [48850/100000], Train Loss: 12.5374, Test Loss: 822.1189\n",
      "Epoch [48900/100000], Train Loss: 12.3621, Test Loss: 822.3518\n",
      "Epoch [48950/100000], Train Loss: 11.8873, Test Loss: 825.8456\n",
      "Epoch [49000/100000], Train Loss: 8.9938, Test Loss: 816.7231\n",
      "Epoch [49050/100000], Train Loss: 12.5639, Test Loss: 819.7491\n",
      "Epoch [49100/100000], Train Loss: 12.2514, Test Loss: 816.8433\n",
      "Epoch [49150/100000], Train Loss: 13.0178, Test Loss: 820.3346\n",
      "Epoch [49200/100000], Train Loss: 13.8748, Test Loss: 818.4135\n",
      "Epoch [49250/100000], Train Loss: 13.3002, Test Loss: 819.1991\n",
      "Epoch [49300/100000], Train Loss: 9.7748, Test Loss: 819.2619\n",
      "Epoch [49350/100000], Train Loss: 10.8836, Test Loss: 817.0941\n",
      "Epoch [49400/100000], Train Loss: 11.6202, Test Loss: 818.9492\n",
      "Epoch [49450/100000], Train Loss: 8.2096, Test Loss: 817.3556\n",
      "Epoch [49500/100000], Train Loss: 9.9253, Test Loss: 821.4534\n",
      "Epoch [49550/100000], Train Loss: 16.7542, Test Loss: 820.4822\n",
      "Epoch [49600/100000], Train Loss: 12.0324, Test Loss: 818.2278\n",
      "Epoch [49650/100000], Train Loss: 10.7956, Test Loss: 821.1001\n",
      "Epoch [49700/100000], Train Loss: 12.8635, Test Loss: 821.0866\n",
      "Epoch [49750/100000], Train Loss: 10.0737, Test Loss: 819.6545\n",
      "Epoch [49800/100000], Train Loss: 9.7705, Test Loss: 818.7512\n",
      "Epoch [49850/100000], Train Loss: 10.1354, Test Loss: 819.9995\n",
      "Epoch [49900/100000], Train Loss: 9.8242, Test Loss: 819.7576\n",
      "Epoch [49950/100000], Train Loss: 10.1414, Test Loss: 821.2942\n",
      "Epoch [50000/100000], Train Loss: 12.8114, Test Loss: 821.9618\n",
      "Epoch [50050/100000], Train Loss: 12.1211, Test Loss: 820.6613\n",
      "Epoch [50100/100000], Train Loss: 15.9007, Test Loss: 823.1126\n",
      "Epoch [50150/100000], Train Loss: 10.6788, Test Loss: 816.0931\n",
      "Epoch [50200/100000], Train Loss: 12.1774, Test Loss: 821.8249\n",
      "Epoch [50250/100000], Train Loss: 10.5294, Test Loss: 818.3629\n",
      "Epoch [50300/100000], Train Loss: 12.1629, Test Loss: 819.2049\n",
      "Epoch [50350/100000], Train Loss: 11.6852, Test Loss: 823.6654\n",
      "Epoch [50400/100000], Train Loss: 9.6905, Test Loss: 822.1133\n",
      "Epoch [50450/100000], Train Loss: 9.6773, Test Loss: 822.1494\n",
      "Epoch [50500/100000], Train Loss: 9.3122, Test Loss: 818.7114\n",
      "Epoch [50550/100000], Train Loss: 7.9673, Test Loss: 820.5737\n",
      "Epoch [50600/100000], Train Loss: 11.3614, Test Loss: 820.0317\n",
      "Epoch [50650/100000], Train Loss: 13.3201, Test Loss: 822.1703\n",
      "Epoch [50700/100000], Train Loss: 10.7843, Test Loss: 820.8393\n",
      "Epoch [50750/100000], Train Loss: 12.0227, Test Loss: 822.9490\n",
      "Epoch [50800/100000], Train Loss: 13.7289, Test Loss: 825.8570\n",
      "Epoch [50850/100000], Train Loss: 13.2847, Test Loss: 822.3338\n",
      "Epoch [50900/100000], Train Loss: 11.2522, Test Loss: 818.4891\n",
      "Epoch [50950/100000], Train Loss: 10.5934, Test Loss: 820.1601\n",
      "Epoch [51000/100000], Train Loss: 10.6625, Test Loss: 824.2002\n",
      "Epoch [51050/100000], Train Loss: 8.4259, Test Loss: 822.0479\n",
      "Epoch [51100/100000], Train Loss: 11.7860, Test Loss: 820.9951\n",
      "Epoch [51150/100000], Train Loss: 10.9866, Test Loss: 824.5128\n",
      "Epoch [51200/100000], Train Loss: 13.1798, Test Loss: 823.1603\n",
      "Epoch [51250/100000], Train Loss: 10.7289, Test Loss: 824.9120\n",
      "Epoch [51300/100000], Train Loss: 13.1548, Test Loss: 823.4355\n",
      "Epoch [51350/100000], Train Loss: 13.5965, Test Loss: 824.0759\n",
      "Epoch [51400/100000], Train Loss: 11.0854, Test Loss: 827.8909\n",
      "Epoch [51450/100000], Train Loss: 10.8970, Test Loss: 823.0671\n",
      "Epoch [51500/100000], Train Loss: 12.4225, Test Loss: 825.2691\n",
      "Epoch [51550/100000], Train Loss: 13.0373, Test Loss: 823.7795\n",
      "Epoch [51600/100000], Train Loss: 11.0685, Test Loss: 824.0522\n",
      "Epoch [51650/100000], Train Loss: 10.5382, Test Loss: 824.3589\n",
      "Epoch [51700/100000], Train Loss: 11.2987, Test Loss: 822.0281\n",
      "Epoch [51750/100000], Train Loss: 9.0201, Test Loss: 827.4658\n",
      "Epoch [51800/100000], Train Loss: 16.5800, Test Loss: 826.9923\n",
      "Epoch [51850/100000], Train Loss: 12.4894, Test Loss: 823.8642\n",
      "Epoch [51900/100000], Train Loss: 12.2167, Test Loss: 826.3587\n",
      "Epoch [51950/100000], Train Loss: 10.8836, Test Loss: 825.3139\n",
      "Epoch [52000/100000], Train Loss: 10.8203, Test Loss: 825.5748\n",
      "Epoch [52050/100000], Train Loss: 11.3772, Test Loss: 826.5913\n",
      "Epoch [52100/100000], Train Loss: 9.5330, Test Loss: 822.9535\n",
      "Epoch [52150/100000], Train Loss: 6.8369, Test Loss: 821.8416\n",
      "Epoch [52200/100000], Train Loss: 12.3463, Test Loss: 825.4525\n",
      "Epoch [52250/100000], Train Loss: 11.0101, Test Loss: 823.9089\n",
      "Epoch [52300/100000], Train Loss: 11.9400, Test Loss: 824.7659\n",
      "Epoch [52350/100000], Train Loss: 12.7552, Test Loss: 826.3511\n",
      "Epoch [52400/100000], Train Loss: 10.1789, Test Loss: 827.0114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52450/100000], Train Loss: 13.4017, Test Loss: 825.5653\n",
      "Epoch [52500/100000], Train Loss: 9.3477, Test Loss: 823.6351\n",
      "Epoch [52550/100000], Train Loss: 12.2715, Test Loss: 830.0305\n",
      "Epoch [52600/100000], Train Loss: 12.7055, Test Loss: 828.7468\n",
      "Epoch [52650/100000], Train Loss: 12.6807, Test Loss: 826.5359\n",
      "Epoch [52700/100000], Train Loss: 9.5105, Test Loss: 826.8719\n",
      "Epoch [52750/100000], Train Loss: 13.2329, Test Loss: 832.1312\n",
      "Epoch [52800/100000], Train Loss: 14.3627, Test Loss: 825.8898\n",
      "Epoch [52850/100000], Train Loss: 11.3194, Test Loss: 824.9960\n",
      "Epoch [52900/100000], Train Loss: 9.6879, Test Loss: 826.5678\n",
      "Epoch [52950/100000], Train Loss: 10.0573, Test Loss: 826.3657\n",
      "Epoch [53000/100000], Train Loss: 10.2131, Test Loss: 825.8357\n",
      "Epoch [53050/100000], Train Loss: 10.7484, Test Loss: 823.3496\n",
      "Epoch [53100/100000], Train Loss: 11.4004, Test Loss: 824.0975\n",
      "Epoch [53150/100000], Train Loss: 9.8921, Test Loss: 823.5543\n",
      "Epoch [53200/100000], Train Loss: 10.1054, Test Loss: 825.8394\n",
      "Epoch [53250/100000], Train Loss: 10.9399, Test Loss: 829.0621\n",
      "Epoch [53300/100000], Train Loss: 10.9958, Test Loss: 823.6547\n",
      "Epoch [53350/100000], Train Loss: 11.5946, Test Loss: 825.8925\n",
      "Epoch [53400/100000], Train Loss: 12.1339, Test Loss: 825.6695\n",
      "Epoch [53450/100000], Train Loss: 12.5086, Test Loss: 829.9667\n",
      "Epoch [53500/100000], Train Loss: 9.1782, Test Loss: 827.7101\n",
      "Epoch [53550/100000], Train Loss: 12.5518, Test Loss: 830.4701\n",
      "Epoch [53600/100000], Train Loss: 11.4780, Test Loss: 830.1281\n",
      "Epoch [53650/100000], Train Loss: 9.2841, Test Loss: 830.6796\n",
      "Epoch [53700/100000], Train Loss: 10.0554, Test Loss: 826.3713\n",
      "Epoch [53750/100000], Train Loss: 10.3639, Test Loss: 825.6721\n",
      "Epoch [53800/100000], Train Loss: 13.3581, Test Loss: 826.8038\n",
      "Epoch [53850/100000], Train Loss: 15.3897, Test Loss: 828.2939\n",
      "Epoch [53900/100000], Train Loss: 9.2173, Test Loss: 826.2825\n",
      "Epoch [53950/100000], Train Loss: 12.6625, Test Loss: 827.7420\n",
      "Epoch [54000/100000], Train Loss: 11.3241, Test Loss: 826.3168\n",
      "Epoch [54050/100000], Train Loss: 7.5978, Test Loss: 825.1493\n",
      "Epoch [54100/100000], Train Loss: 8.6621, Test Loss: 825.9897\n",
      "Epoch [54150/100000], Train Loss: 12.2453, Test Loss: 828.3087\n",
      "Epoch [54200/100000], Train Loss: 9.0998, Test Loss: 826.5366\n",
      "Epoch [54250/100000], Train Loss: 9.9725, Test Loss: 827.3720\n",
      "Epoch [54300/100000], Train Loss: 8.8607, Test Loss: 828.6534\n",
      "Epoch [54350/100000], Train Loss: 11.6998, Test Loss: 827.4620\n",
      "Epoch [54400/100000], Train Loss: 9.5130, Test Loss: 833.0730\n",
      "Epoch [54450/100000], Train Loss: 9.5636, Test Loss: 827.9490\n",
      "Epoch [54500/100000], Train Loss: 9.1330, Test Loss: 829.9851\n",
      "Epoch [54550/100000], Train Loss: 10.8713, Test Loss: 826.7954\n",
      "Epoch [54600/100000], Train Loss: 16.0362, Test Loss: 828.4871\n",
      "Epoch [54650/100000], Train Loss: 11.7954, Test Loss: 829.9363\n",
      "Epoch [54700/100000], Train Loss: 12.7327, Test Loss: 836.5085\n",
      "Epoch [54750/100000], Train Loss: 8.7746, Test Loss: 831.2891\n",
      "Epoch [54800/100000], Train Loss: 11.4651, Test Loss: 827.5516\n",
      "Epoch [54850/100000], Train Loss: 10.4060, Test Loss: 837.4015\n",
      "Epoch [54900/100000], Train Loss: 9.2824, Test Loss: 832.7201\n",
      "Epoch [54950/100000], Train Loss: 11.3297, Test Loss: 830.9872\n",
      "Epoch [55000/100000], Train Loss: 9.2625, Test Loss: 828.8388\n",
      "Epoch [55050/100000], Train Loss: 10.8650, Test Loss: 831.5710\n",
      "Epoch [55100/100000], Train Loss: 11.5869, Test Loss: 830.1705\n",
      "Epoch [55150/100000], Train Loss: 13.0422, Test Loss: 832.3036\n",
      "Epoch [55200/100000], Train Loss: 10.3701, Test Loss: 828.4622\n",
      "Epoch [55250/100000], Train Loss: 13.4905, Test Loss: 829.2526\n",
      "Epoch [55300/100000], Train Loss: 12.6194, Test Loss: 829.6938\n",
      "Epoch [55350/100000], Train Loss: 12.1116, Test Loss: 829.8651\n",
      "Epoch [55400/100000], Train Loss: 9.9153, Test Loss: 831.1193\n",
      "Epoch [55450/100000], Train Loss: 9.4586, Test Loss: 828.0107\n",
      "Epoch [55500/100000], Train Loss: 10.2493, Test Loss: 829.7844\n",
      "Epoch [55550/100000], Train Loss: 8.8869, Test Loss: 832.1531\n",
      "Epoch [55600/100000], Train Loss: 12.8776, Test Loss: 833.7093\n",
      "Epoch [55650/100000], Train Loss: 10.7772, Test Loss: 832.3589\n",
      "Epoch [55700/100000], Train Loss: 8.8581, Test Loss: 830.8509\n",
      "Epoch [55750/100000], Train Loss: 11.3941, Test Loss: 830.0337\n",
      "Epoch [55800/100000], Train Loss: 10.2934, Test Loss: 827.7431\n",
      "Epoch [55850/100000], Train Loss: 11.1234, Test Loss: 829.6788\n",
      "Epoch [55900/100000], Train Loss: 10.1441, Test Loss: 830.2689\n",
      "Epoch [55950/100000], Train Loss: 12.1863, Test Loss: 831.1885\n",
      "Epoch [56000/100000], Train Loss: 9.7152, Test Loss: 830.1942\n",
      "Epoch [56050/100000], Train Loss: 10.4153, Test Loss: 830.3559\n",
      "Epoch [56100/100000], Train Loss: 11.4529, Test Loss: 833.6321\n",
      "Epoch [56150/100000], Train Loss: 13.1356, Test Loss: 834.4486\n",
      "Epoch [56200/100000], Train Loss: 9.8248, Test Loss: 827.9758\n",
      "Epoch [56250/100000], Train Loss: 11.4555, Test Loss: 830.3096\n",
      "Epoch [56300/100000], Train Loss: 10.6458, Test Loss: 831.7164\n",
      "Epoch [56350/100000], Train Loss: 11.8101, Test Loss: 829.7833\n",
      "Epoch [56400/100000], Train Loss: 10.7231, Test Loss: 832.2157\n",
      "Epoch [56450/100000], Train Loss: 10.7902, Test Loss: 833.7828\n",
      "Epoch [56500/100000], Train Loss: 11.9029, Test Loss: 830.7724\n",
      "Epoch [56550/100000], Train Loss: 11.2719, Test Loss: 831.4493\n",
      "Epoch [56600/100000], Train Loss: 10.6997, Test Loss: 830.8320\n",
      "Epoch [56650/100000], Train Loss: 10.1966, Test Loss: 831.2573\n",
      "Epoch [56700/100000], Train Loss: 12.4398, Test Loss: 831.0378\n",
      "Epoch [56750/100000], Train Loss: 12.6014, Test Loss: 832.4314\n",
      "Epoch [56800/100000], Train Loss: 11.0622, Test Loss: 835.5923\n",
      "Epoch [56850/100000], Train Loss: 10.5352, Test Loss: 831.2201\n",
      "Epoch [56900/100000], Train Loss: 9.1496, Test Loss: 830.6653\n",
      "Epoch [56950/100000], Train Loss: 12.4579, Test Loss: 830.5873\n",
      "Epoch [57000/100000], Train Loss: 11.7988, Test Loss: 832.3301\n",
      "Epoch [57050/100000], Train Loss: 10.8872, Test Loss: 830.4913\n",
      "Epoch [57100/100000], Train Loss: 9.5193, Test Loss: 832.6386\n",
      "Epoch [57150/100000], Train Loss: 8.8513, Test Loss: 835.6222\n",
      "Epoch [57200/100000], Train Loss: 14.5932, Test Loss: 829.1392\n",
      "Epoch [57250/100000], Train Loss: 11.3246, Test Loss: 835.6372\n",
      "Epoch [57300/100000], Train Loss: 10.6555, Test Loss: 831.1169\n",
      "Epoch [57350/100000], Train Loss: 11.6149, Test Loss: 831.6108\n",
      "Epoch [57400/100000], Train Loss: 10.7657, Test Loss: 835.0942\n",
      "Epoch [57450/100000], Train Loss: 9.2407, Test Loss: 831.0822\n",
      "Epoch [57500/100000], Train Loss: 11.9053, Test Loss: 832.6342\n",
      "Epoch [57550/100000], Train Loss: 11.2091, Test Loss: 829.3922\n",
      "Epoch [57600/100000], Train Loss: 12.0296, Test Loss: 830.9322\n",
      "Epoch [57650/100000], Train Loss: 9.4817, Test Loss: 829.2704\n",
      "Epoch [57700/100000], Train Loss: 9.6862, Test Loss: 833.0855\n",
      "Epoch [57750/100000], Train Loss: 10.2326, Test Loss: 834.4310\n",
      "Epoch [57800/100000], Train Loss: 10.7824, Test Loss: 836.8803\n",
      "Epoch [57850/100000], Train Loss: 8.2166, Test Loss: 830.0724\n",
      "Epoch [57900/100000], Train Loss: 10.5860, Test Loss: 834.1870\n",
      "Epoch [57950/100000], Train Loss: 10.3442, Test Loss: 827.6935\n",
      "Epoch [58000/100000], Train Loss: 8.1332, Test Loss: 832.4784\n",
      "Epoch [58050/100000], Train Loss: 9.6927, Test Loss: 835.9105\n",
      "Epoch [58100/100000], Train Loss: 10.2290, Test Loss: 830.2717\n",
      "Epoch [58150/100000], Train Loss: 12.9985, Test Loss: 833.6875\n",
      "Epoch [58200/100000], Train Loss: 12.0676, Test Loss: 834.6457\n",
      "Epoch [58250/100000], Train Loss: 13.5089, Test Loss: 831.1150\n",
      "Epoch [58300/100000], Train Loss: 11.1313, Test Loss: 831.7937\n",
      "Epoch [58350/100000], Train Loss: 8.2926, Test Loss: 831.9386\n",
      "Epoch [58400/100000], Train Loss: 13.5560, Test Loss: 832.3440\n",
      "Epoch [58450/100000], Train Loss: 10.3624, Test Loss: 832.3555\n",
      "Epoch [58500/100000], Train Loss: 12.7531, Test Loss: 834.9175\n",
      "Epoch [58550/100000], Train Loss: 10.5238, Test Loss: 834.2326\n",
      "Epoch [58600/100000], Train Loss: 10.5351, Test Loss: 833.3379\n",
      "Epoch [58650/100000], Train Loss: 9.0712, Test Loss: 834.8418\n",
      "Epoch [58700/100000], Train Loss: 11.3083, Test Loss: 831.9230\n",
      "Epoch [58750/100000], Train Loss: 12.5764, Test Loss: 835.8785\n",
      "Epoch [58800/100000], Train Loss: 9.7862, Test Loss: 831.8229\n",
      "Epoch [58850/100000], Train Loss: 10.9127, Test Loss: 834.7102\n",
      "Epoch [58900/100000], Train Loss: 11.2257, Test Loss: 836.6005\n",
      "Epoch [58950/100000], Train Loss: 17.1773, Test Loss: 834.6714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59000/100000], Train Loss: 10.6745, Test Loss: 834.6907\n",
      "Epoch [59050/100000], Train Loss: 12.0102, Test Loss: 833.4319\n",
      "Epoch [59100/100000], Train Loss: 9.5931, Test Loss: 831.9789\n",
      "Epoch [59150/100000], Train Loss: 10.3600, Test Loss: 832.3781\n",
      "Epoch [59200/100000], Train Loss: 9.3446, Test Loss: 834.4984\n",
      "Epoch [59250/100000], Train Loss: 13.8763, Test Loss: 836.7547\n",
      "Epoch [59300/100000], Train Loss: 8.0309, Test Loss: 833.9063\n",
      "Epoch [59350/100000], Train Loss: 9.7510, Test Loss: 832.1968\n",
      "Epoch [59400/100000], Train Loss: 10.3727, Test Loss: 836.1598\n",
      "Epoch [59450/100000], Train Loss: 9.6664, Test Loss: 834.2008\n",
      "Epoch [59500/100000], Train Loss: 10.0812, Test Loss: 834.4655\n",
      "Epoch [59550/100000], Train Loss: 13.2163, Test Loss: 835.0565\n",
      "Epoch [59600/100000], Train Loss: 9.3535, Test Loss: 834.2406\n",
      "Epoch [59650/100000], Train Loss: 10.8985, Test Loss: 836.1580\n",
      "Epoch [59700/100000], Train Loss: 12.4546, Test Loss: 836.1463\n",
      "Epoch [59750/100000], Train Loss: 9.1098, Test Loss: 833.4429\n",
      "Epoch [59800/100000], Train Loss: 11.6907, Test Loss: 839.0124\n",
      "Epoch [59850/100000], Train Loss: 11.8826, Test Loss: 838.3830\n",
      "Epoch [59900/100000], Train Loss: 11.8949, Test Loss: 834.9695\n",
      "Epoch [59950/100000], Train Loss: 11.0730, Test Loss: 837.8684\n",
      "Epoch [60000/100000], Train Loss: 11.9605, Test Loss: 837.9557\n",
      "Epoch [60050/100000], Train Loss: 9.9023, Test Loss: 837.4429\n",
      "Epoch [60100/100000], Train Loss: 9.7811, Test Loss: 840.5827\n",
      "Epoch [60150/100000], Train Loss: 10.3322, Test Loss: 836.8185\n",
      "Epoch [60200/100000], Train Loss: 7.5843, Test Loss: 833.6943\n",
      "Epoch [60250/100000], Train Loss: 16.0499, Test Loss: 836.1736\n",
      "Epoch [60300/100000], Train Loss: 10.7090, Test Loss: 835.6830\n",
      "Epoch [60350/100000], Train Loss: 9.8788, Test Loss: 834.7865\n",
      "Epoch [60400/100000], Train Loss: 9.8477, Test Loss: 834.0389\n",
      "Epoch [60450/100000], Train Loss: 11.2197, Test Loss: 834.8534\n",
      "Epoch [60500/100000], Train Loss: 10.4358, Test Loss: 833.2461\n",
      "Epoch [60550/100000], Train Loss: 8.6017, Test Loss: 833.6288\n",
      "Epoch [60600/100000], Train Loss: 11.9668, Test Loss: 832.7161\n",
      "Epoch [60650/100000], Train Loss: 9.7964, Test Loss: 837.8271\n",
      "Epoch [60700/100000], Train Loss: 12.2177, Test Loss: 838.5679\n",
      "Epoch [60750/100000], Train Loss: 10.4468, Test Loss: 837.6338\n",
      "Epoch [60800/100000], Train Loss: 9.4134, Test Loss: 833.5867\n",
      "Epoch [60850/100000], Train Loss: 12.6963, Test Loss: 838.1144\n",
      "Epoch [60900/100000], Train Loss: 9.5665, Test Loss: 836.3724\n",
      "Epoch [60950/100000], Train Loss: 9.3719, Test Loss: 835.4631\n",
      "Epoch [61000/100000], Train Loss: 13.2043, Test Loss: 835.3561\n",
      "Epoch [61050/100000], Train Loss: 9.7196, Test Loss: 835.9030\n",
      "Epoch [61100/100000], Train Loss: 13.9251, Test Loss: 836.7719\n",
      "Epoch [61150/100000], Train Loss: 8.7037, Test Loss: 838.8075\n",
      "Epoch [61200/100000], Train Loss: 11.2390, Test Loss: 834.9704\n",
      "Epoch [61250/100000], Train Loss: 14.1689, Test Loss: 839.0513\n",
      "Epoch [61300/100000], Train Loss: 7.6240, Test Loss: 835.6124\n",
      "Epoch [61350/100000], Train Loss: 10.7611, Test Loss: 835.3253\n",
      "Epoch [61400/100000], Train Loss: 9.7909, Test Loss: 838.2176\n",
      "Epoch [61450/100000], Train Loss: 9.2205, Test Loss: 837.2702\n",
      "Epoch [61500/100000], Train Loss: 9.6725, Test Loss: 834.8800\n",
      "Epoch [61550/100000], Train Loss: 9.3417, Test Loss: 835.3284\n",
      "Epoch [61600/100000], Train Loss: 12.4041, Test Loss: 839.9815\n",
      "Epoch [61650/100000], Train Loss: 8.2986, Test Loss: 836.2613\n",
      "Epoch [61700/100000], Train Loss: 8.4058, Test Loss: 836.8809\n",
      "Epoch [61750/100000], Train Loss: 8.4794, Test Loss: 837.5749\n",
      "Epoch [61800/100000], Train Loss: 10.9222, Test Loss: 836.7158\n",
      "Epoch [61850/100000], Train Loss: 11.5134, Test Loss: 836.8956\n",
      "Epoch [61900/100000], Train Loss: 11.9083, Test Loss: 839.7642\n",
      "Epoch [61950/100000], Train Loss: 12.1418, Test Loss: 836.1075\n",
      "Epoch [62000/100000], Train Loss: 11.5139, Test Loss: 837.4632\n",
      "Epoch [62050/100000], Train Loss: 13.0492, Test Loss: 838.1329\n",
      "Epoch [62100/100000], Train Loss: 6.3579, Test Loss: 835.5277\n",
      "Epoch [62150/100000], Train Loss: 11.5666, Test Loss: 840.9489\n",
      "Epoch [62200/100000], Train Loss: 9.2392, Test Loss: 838.5217\n",
      "Epoch [62250/100000], Train Loss: 9.5948, Test Loss: 839.2611\n",
      "Epoch [62300/100000], Train Loss: 10.7290, Test Loss: 840.1328\n",
      "Epoch [62350/100000], Train Loss: 11.1522, Test Loss: 836.2655\n",
      "Epoch [62400/100000], Train Loss: 9.2579, Test Loss: 836.4327\n",
      "Epoch [62450/100000], Train Loss: 8.7247, Test Loss: 837.2589\n",
      "Epoch [62500/100000], Train Loss: 8.2297, Test Loss: 838.7155\n",
      "Epoch [62550/100000], Train Loss: 16.2386, Test Loss: 839.2200\n",
      "Epoch [62600/100000], Train Loss: 9.9133, Test Loss: 839.8028\n",
      "Epoch [62650/100000], Train Loss: 9.0979, Test Loss: 836.8696\n",
      "Epoch [62700/100000], Train Loss: 10.5722, Test Loss: 840.1180\n",
      "Epoch [62750/100000], Train Loss: 7.8549, Test Loss: 835.6075\n",
      "Epoch [62800/100000], Train Loss: 9.6815, Test Loss: 839.3116\n",
      "Epoch [62850/100000], Train Loss: 9.3145, Test Loss: 842.0805\n",
      "Epoch [62900/100000], Train Loss: 12.7829, Test Loss: 841.3340\n",
      "Epoch [62950/100000], Train Loss: 7.1503, Test Loss: 837.8812\n",
      "Epoch [63000/100000], Train Loss: 10.2615, Test Loss: 840.5487\n",
      "Epoch [63050/100000], Train Loss: 8.0277, Test Loss: 846.8158\n",
      "Epoch [63100/100000], Train Loss: 8.2497, Test Loss: 835.4164\n",
      "Epoch [63150/100000], Train Loss: 9.3994, Test Loss: 838.4714\n",
      "Epoch [63200/100000], Train Loss: 10.4947, Test Loss: 840.0778\n",
      "Epoch [63250/100000], Train Loss: 11.8239, Test Loss: 840.2485\n",
      "Epoch [63300/100000], Train Loss: 10.3069, Test Loss: 839.0910\n",
      "Epoch [63350/100000], Train Loss: 10.9667, Test Loss: 838.4340\n",
      "Epoch [63400/100000], Train Loss: 8.5757, Test Loss: 838.1366\n",
      "Epoch [63450/100000], Train Loss: 10.4760, Test Loss: 838.2399\n",
      "Epoch [63500/100000], Train Loss: 10.3478, Test Loss: 839.8354\n",
      "Epoch [63550/100000], Train Loss: 12.9613, Test Loss: 841.9681\n",
      "Epoch [63600/100000], Train Loss: 9.4889, Test Loss: 838.5787\n",
      "Epoch [63650/100000], Train Loss: 12.0664, Test Loss: 837.9428\n",
      "Epoch [63700/100000], Train Loss: 11.1250, Test Loss: 838.1423\n",
      "Epoch [63750/100000], Train Loss: 10.4244, Test Loss: 835.0016\n",
      "Epoch [63800/100000], Train Loss: 7.9514, Test Loss: 839.6896\n",
      "Epoch [63850/100000], Train Loss: 8.6609, Test Loss: 835.8322\n",
      "Epoch [63900/100000], Train Loss: 11.3600, Test Loss: 836.3212\n",
      "Epoch [63950/100000], Train Loss: 12.9575, Test Loss: 838.4365\n",
      "Epoch [64000/100000], Train Loss: 8.7958, Test Loss: 844.3632\n",
      "Epoch [64050/100000], Train Loss: 8.3259, Test Loss: 839.0925\n",
      "Epoch [64100/100000], Train Loss: 8.9099, Test Loss: 838.5449\n",
      "Epoch [64150/100000], Train Loss: 8.9669, Test Loss: 836.9988\n",
      "Epoch [64200/100000], Train Loss: 8.1930, Test Loss: 839.6456\n",
      "Epoch [64250/100000], Train Loss: 10.3772, Test Loss: 840.5141\n",
      "Epoch [64300/100000], Train Loss: 7.5856, Test Loss: 839.4653\n",
      "Epoch [64350/100000], Train Loss: 9.0996, Test Loss: 838.3052\n",
      "Epoch [64400/100000], Train Loss: 12.4514, Test Loss: 839.7160\n",
      "Epoch [64450/100000], Train Loss: 9.3834, Test Loss: 836.9998\n",
      "Epoch [64500/100000], Train Loss: 12.4594, Test Loss: 839.0386\n",
      "Epoch [64550/100000], Train Loss: 9.8661, Test Loss: 839.2070\n",
      "Epoch [64600/100000], Train Loss: 11.8307, Test Loss: 836.4316\n",
      "Epoch [64650/100000], Train Loss: 10.0459, Test Loss: 839.4174\n",
      "Epoch [64700/100000], Train Loss: 11.3173, Test Loss: 840.1059\n",
      "Epoch [64750/100000], Train Loss: 7.2734, Test Loss: 836.3807\n",
      "Epoch [64800/100000], Train Loss: 8.8873, Test Loss: 836.6897\n",
      "Epoch [64850/100000], Train Loss: 11.6745, Test Loss: 842.1734\n",
      "Epoch [64900/100000], Train Loss: 9.1967, Test Loss: 836.9780\n",
      "Epoch [64950/100000], Train Loss: 11.7222, Test Loss: 837.2707\n",
      "Epoch [65000/100000], Train Loss: 10.9760, Test Loss: 835.8993\n",
      "Epoch [65050/100000], Train Loss: 8.3906, Test Loss: 837.9123\n",
      "Epoch [65100/100000], Train Loss: 6.9112, Test Loss: 840.3373\n",
      "Epoch [65150/100000], Train Loss: 11.7482, Test Loss: 845.8936\n",
      "Epoch [65200/100000], Train Loss: 10.6163, Test Loss: 839.7636\n",
      "Epoch [65250/100000], Train Loss: 11.0463, Test Loss: 838.1584\n",
      "Epoch [65300/100000], Train Loss: 9.8716, Test Loss: 840.0229\n",
      "Epoch [65350/100000], Train Loss: 7.6347, Test Loss: 839.2925\n",
      "Epoch [65400/100000], Train Loss: 7.2995, Test Loss: 835.7461\n",
      "Epoch [65450/100000], Train Loss: 10.0312, Test Loss: 838.4498\n",
      "Epoch [65500/100000], Train Loss: 7.9065, Test Loss: 840.3102\n",
      "Epoch [65550/100000], Train Loss: 9.5606, Test Loss: 839.1009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65600/100000], Train Loss: 11.1483, Test Loss: 839.9576\n",
      "Epoch [65650/100000], Train Loss: 10.1098, Test Loss: 838.5074\n",
      "Epoch [65700/100000], Train Loss: 9.5471, Test Loss: 840.0960\n",
      "Epoch [65750/100000], Train Loss: 10.1567, Test Loss: 840.9774\n",
      "Epoch [65800/100000], Train Loss: 8.3148, Test Loss: 838.1496\n",
      "Epoch [65850/100000], Train Loss: 9.5666, Test Loss: 840.2423\n",
      "Epoch [65900/100000], Train Loss: 10.9838, Test Loss: 841.0431\n",
      "Epoch [65950/100000], Train Loss: 12.9069, Test Loss: 836.7241\n",
      "Epoch [66000/100000], Train Loss: 8.2014, Test Loss: 838.2823\n",
      "Epoch [66050/100000], Train Loss: 9.8290, Test Loss: 841.0767\n",
      "Epoch [66100/100000], Train Loss: 9.1828, Test Loss: 843.9002\n",
      "Epoch [66150/100000], Train Loss: 11.8684, Test Loss: 839.5117\n",
      "Epoch [66200/100000], Train Loss: 10.2918, Test Loss: 841.1453\n",
      "Epoch [66250/100000], Train Loss: 9.8985, Test Loss: 841.2714\n",
      "Epoch [66300/100000], Train Loss: 9.1509, Test Loss: 840.4678\n",
      "Epoch [66350/100000], Train Loss: 10.7726, Test Loss: 840.0644\n",
      "Epoch [66400/100000], Train Loss: 9.7712, Test Loss: 839.6828\n",
      "Epoch [66450/100000], Train Loss: 11.2731, Test Loss: 843.9506\n",
      "Epoch [66500/100000], Train Loss: 9.4481, Test Loss: 840.4421\n",
      "Epoch [66550/100000], Train Loss: 11.0466, Test Loss: 840.5772\n",
      "Epoch [66600/100000], Train Loss: 10.4059, Test Loss: 843.8293\n",
      "Epoch [66650/100000], Train Loss: 10.8505, Test Loss: 844.2565\n",
      "Epoch [66700/100000], Train Loss: 9.4133, Test Loss: 843.3128\n",
      "Epoch [66750/100000], Train Loss: 10.1301, Test Loss: 844.2112\n",
      "Epoch [66800/100000], Train Loss: 7.8801, Test Loss: 841.1487\n",
      "Epoch [66850/100000], Train Loss: 8.2042, Test Loss: 841.1120\n",
      "Epoch [66900/100000], Train Loss: 11.4628, Test Loss: 841.0211\n",
      "Epoch [66950/100000], Train Loss: 8.3775, Test Loss: 839.9489\n",
      "Epoch [67000/100000], Train Loss: 11.8205, Test Loss: 837.7367\n",
      "Epoch [67050/100000], Train Loss: 9.0030, Test Loss: 841.7160\n",
      "Epoch [67100/100000], Train Loss: 9.1591, Test Loss: 839.4945\n",
      "Epoch [67150/100000], Train Loss: 10.6361, Test Loss: 842.6635\n",
      "Epoch [67200/100000], Train Loss: 11.9781, Test Loss: 843.0435\n",
      "Epoch [67250/100000], Train Loss: 11.3308, Test Loss: 843.6193\n",
      "Epoch [67300/100000], Train Loss: 10.0160, Test Loss: 840.0174\n",
      "Epoch [67350/100000], Train Loss: 8.2502, Test Loss: 842.5562\n",
      "Epoch [67400/100000], Train Loss: 9.8105, Test Loss: 841.9859\n",
      "Epoch [67450/100000], Train Loss: 6.4892, Test Loss: 839.7010\n",
      "Epoch [67500/100000], Train Loss: 11.4154, Test Loss: 840.5796\n",
      "Epoch [67550/100000], Train Loss: 9.1204, Test Loss: 841.8940\n",
      "Epoch [67600/100000], Train Loss: 11.0033, Test Loss: 840.1701\n",
      "Epoch [67650/100000], Train Loss: 8.8833, Test Loss: 842.2512\n",
      "Epoch [67700/100000], Train Loss: 9.7987, Test Loss: 845.3516\n",
      "Epoch [67750/100000], Train Loss: 9.4009, Test Loss: 841.1139\n",
      "Epoch [67800/100000], Train Loss: 7.7335, Test Loss: 841.2912\n",
      "Epoch [67850/100000], Train Loss: 9.7284, Test Loss: 845.0503\n",
      "Epoch [67900/100000], Train Loss: 8.8385, Test Loss: 847.1555\n",
      "Epoch [67950/100000], Train Loss: 8.7592, Test Loss: 842.2770\n",
      "Epoch [68000/100000], Train Loss: 8.6487, Test Loss: 843.9375\n",
      "Epoch [68050/100000], Train Loss: 8.4946, Test Loss: 843.9832\n",
      "Epoch [68100/100000], Train Loss: 8.8257, Test Loss: 842.1740\n",
      "Epoch [68150/100000], Train Loss: 11.1792, Test Loss: 844.9730\n",
      "Epoch [68200/100000], Train Loss: 8.9247, Test Loss: 845.8448\n",
      "Epoch [68250/100000], Train Loss: 10.3495, Test Loss: 843.0660\n",
      "Epoch [68300/100000], Train Loss: 8.3392, Test Loss: 847.1778\n",
      "Epoch [68350/100000], Train Loss: 11.5351, Test Loss: 845.1075\n",
      "Epoch [68400/100000], Train Loss: 8.1493, Test Loss: 844.8428\n",
      "Epoch [68450/100000], Train Loss: 10.7167, Test Loss: 840.1906\n",
      "Epoch [68500/100000], Train Loss: 12.6503, Test Loss: 841.8508\n",
      "Epoch [68550/100000], Train Loss: 8.9846, Test Loss: 839.9832\n",
      "Epoch [68600/100000], Train Loss: 11.0917, Test Loss: 845.0892\n",
      "Epoch [68650/100000], Train Loss: 8.5381, Test Loss: 843.0610\n",
      "Epoch [68700/100000], Train Loss: 11.0521, Test Loss: 844.4045\n",
      "Epoch [68750/100000], Train Loss: 12.0032, Test Loss: 842.9100\n",
      "Epoch [68800/100000], Train Loss: 10.2782, Test Loss: 845.1583\n",
      "Epoch [68850/100000], Train Loss: 9.4238, Test Loss: 845.7967\n",
      "Epoch [68900/100000], Train Loss: 10.9846, Test Loss: 846.1046\n",
      "Epoch [68950/100000], Train Loss: 8.5995, Test Loss: 845.0748\n",
      "Epoch [69000/100000], Train Loss: 10.2871, Test Loss: 839.8928\n",
      "Epoch [69050/100000], Train Loss: 7.5714, Test Loss: 843.1846\n",
      "Epoch [69100/100000], Train Loss: 11.7421, Test Loss: 846.2613\n",
      "Epoch [69150/100000], Train Loss: 10.6824, Test Loss: 846.5313\n",
      "Epoch [69200/100000], Train Loss: 10.3850, Test Loss: 843.7665\n",
      "Epoch [69250/100000], Train Loss: 9.9745, Test Loss: 843.1201\n",
      "Epoch [69300/100000], Train Loss: 7.9009, Test Loss: 847.9861\n",
      "Epoch [69350/100000], Train Loss: 9.8083, Test Loss: 844.1516\n",
      "Epoch [69400/100000], Train Loss: 9.5358, Test Loss: 842.2669\n",
      "Epoch [69450/100000], Train Loss: 10.5604, Test Loss: 845.7752\n",
      "Epoch [69500/100000], Train Loss: 6.9818, Test Loss: 846.0018\n",
      "Epoch [69550/100000], Train Loss: 8.2214, Test Loss: 847.5942\n",
      "Epoch [69600/100000], Train Loss: 8.7156, Test Loss: 846.0337\n",
      "Epoch [69650/100000], Train Loss: 9.1677, Test Loss: 846.9213\n",
      "Epoch [69700/100000], Train Loss: 11.0798, Test Loss: 843.8592\n",
      "Epoch [69750/100000], Train Loss: 8.9893, Test Loss: 841.8357\n",
      "Epoch [69800/100000], Train Loss: 9.0884, Test Loss: 847.8521\n",
      "Epoch [69850/100000], Train Loss: 9.7252, Test Loss: 847.0852\n",
      "Epoch [69900/100000], Train Loss: 10.8963, Test Loss: 847.6941\n",
      "Epoch [69950/100000], Train Loss: 10.0275, Test Loss: 846.7926\n",
      "Epoch [70000/100000], Train Loss: 8.7372, Test Loss: 846.0902\n",
      "Epoch [70050/100000], Train Loss: 12.1857, Test Loss: 846.0368\n",
      "Epoch [70100/100000], Train Loss: 10.4316, Test Loss: 846.0356\n",
      "Epoch [70150/100000], Train Loss: 10.5578, Test Loss: 850.6502\n",
      "Epoch [70200/100000], Train Loss: 9.1044, Test Loss: 845.2898\n",
      "Epoch [70250/100000], Train Loss: 10.5849, Test Loss: 848.0264\n",
      "Epoch [70300/100000], Train Loss: 9.4820, Test Loss: 846.8336\n",
      "Epoch [70350/100000], Train Loss: 7.2567, Test Loss: 844.4116\n",
      "Epoch [70400/100000], Train Loss: 7.4854, Test Loss: 844.9515\n",
      "Epoch [70450/100000], Train Loss: 9.1722, Test Loss: 846.0570\n",
      "Epoch [70500/100000], Train Loss: 9.8239, Test Loss: 849.3633\n",
      "Epoch [70550/100000], Train Loss: 9.3128, Test Loss: 846.8743\n",
      "Epoch [70600/100000], Train Loss: 7.4699, Test Loss: 847.4333\n",
      "Epoch [70650/100000], Train Loss: 9.0205, Test Loss: 846.6993\n",
      "Epoch [70700/100000], Train Loss: 10.5366, Test Loss: 846.4120\n",
      "Epoch [70750/100000], Train Loss: 13.0106, Test Loss: 849.0760\n",
      "Epoch [70800/100000], Train Loss: 7.0468, Test Loss: 849.7311\n",
      "Epoch [70850/100000], Train Loss: 9.5504, Test Loss: 847.4239\n",
      "Epoch [70900/100000], Train Loss: 8.5230, Test Loss: 849.2792\n",
      "Epoch [70950/100000], Train Loss: 12.1087, Test Loss: 848.1169\n",
      "Epoch [71000/100000], Train Loss: 7.5368, Test Loss: 850.7406\n",
      "Epoch [71050/100000], Train Loss: 11.1932, Test Loss: 848.4070\n",
      "Epoch [71100/100000], Train Loss: 8.5825, Test Loss: 847.5994\n",
      "Epoch [71150/100000], Train Loss: 11.1709, Test Loss: 849.2463\n",
      "Epoch [71200/100000], Train Loss: 8.4946, Test Loss: 848.8621\n",
      "Epoch [71250/100000], Train Loss: 10.2806, Test Loss: 850.1526\n",
      "Epoch [71300/100000], Train Loss: 10.7073, Test Loss: 848.2879\n",
      "Epoch [71350/100000], Train Loss: 8.4534, Test Loss: 846.8113\n",
      "Epoch [71400/100000], Train Loss: 8.8534, Test Loss: 850.0894\n",
      "Epoch [71450/100000], Train Loss: 10.0003, Test Loss: 847.6407\n",
      "Epoch [71500/100000], Train Loss: 9.4802, Test Loss: 845.4356\n",
      "Epoch [71550/100000], Train Loss: 8.7975, Test Loss: 846.2377\n",
      "Epoch [71600/100000], Train Loss: 9.6875, Test Loss: 846.4361\n",
      "Epoch [71650/100000], Train Loss: 12.9908, Test Loss: 847.8139\n",
      "Epoch [71700/100000], Train Loss: 11.2934, Test Loss: 845.6013\n",
      "Epoch [71750/100000], Train Loss: 7.6878, Test Loss: 851.2745\n",
      "Epoch [71800/100000], Train Loss: 10.9247, Test Loss: 849.4761\n",
      "Epoch [71850/100000], Train Loss: 10.3470, Test Loss: 845.2170\n",
      "Epoch [71900/100000], Train Loss: 10.1151, Test Loss: 851.4097\n",
      "Epoch [71950/100000], Train Loss: 9.1395, Test Loss: 848.8028\n",
      "Epoch [72000/100000], Train Loss: 9.9948, Test Loss: 848.3282\n",
      "Epoch [72050/100000], Train Loss: 9.7489, Test Loss: 850.5284\n",
      "Epoch [72100/100000], Train Loss: 8.5852, Test Loss: 847.5782\n",
      "Epoch [72150/100000], Train Loss: 10.1407, Test Loss: 851.9900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72200/100000], Train Loss: 8.0831, Test Loss: 848.4412\n",
      "Epoch [72250/100000], Train Loss: 8.7055, Test Loss: 847.1529\n",
      "Epoch [72300/100000], Train Loss: 6.4586, Test Loss: 845.6225\n",
      "Epoch [72350/100000], Train Loss: 10.5158, Test Loss: 851.3880\n",
      "Epoch [72400/100000], Train Loss: 8.1927, Test Loss: 848.4639\n",
      "Epoch [72450/100000], Train Loss: 9.0485, Test Loss: 847.0727\n",
      "Epoch [72500/100000], Train Loss: 7.6380, Test Loss: 849.5614\n",
      "Epoch [72550/100000], Train Loss: 9.2133, Test Loss: 846.5179\n",
      "Epoch [72600/100000], Train Loss: 9.2196, Test Loss: 851.4476\n",
      "Epoch [72650/100000], Train Loss: 7.5379, Test Loss: 849.0383\n",
      "Epoch [72700/100000], Train Loss: 7.8949, Test Loss: 848.3131\n",
      "Epoch [72750/100000], Train Loss: 9.1111, Test Loss: 848.9771\n",
      "Epoch [72800/100000], Train Loss: 10.3567, Test Loss: 849.0487\n",
      "Epoch [72850/100000], Train Loss: 11.3846, Test Loss: 847.4341\n",
      "Epoch [72900/100000], Train Loss: 11.5694, Test Loss: 848.1097\n",
      "Epoch [72950/100000], Train Loss: 9.1734, Test Loss: 845.9612\n",
      "Epoch [73000/100000], Train Loss: 10.6264, Test Loss: 851.9589\n",
      "Epoch [73050/100000], Train Loss: 9.3291, Test Loss: 847.3039\n",
      "Epoch [73100/100000], Train Loss: 8.1602, Test Loss: 849.4647\n",
      "Epoch [73150/100000], Train Loss: 7.7418, Test Loss: 848.2236\n",
      "Epoch [73200/100000], Train Loss: 10.3042, Test Loss: 849.6557\n",
      "Epoch [73250/100000], Train Loss: 7.7788, Test Loss: 849.6686\n",
      "Epoch [73300/100000], Train Loss: 9.1584, Test Loss: 850.3427\n",
      "Epoch [73350/100000], Train Loss: 8.7111, Test Loss: 849.9525\n",
      "Epoch [73400/100000], Train Loss: 6.8562, Test Loss: 849.7605\n",
      "Epoch [73450/100000], Train Loss: 12.2105, Test Loss: 849.3818\n",
      "Epoch [73500/100000], Train Loss: 8.1944, Test Loss: 851.0897\n",
      "Epoch [73550/100000], Train Loss: 7.3530, Test Loss: 848.8718\n",
      "Epoch [73600/100000], Train Loss: 8.8840, Test Loss: 846.5282\n",
      "Epoch [73650/100000], Train Loss: 9.3973, Test Loss: 849.7620\n",
      "Epoch [73700/100000], Train Loss: 10.3504, Test Loss: 850.6984\n",
      "Epoch [73750/100000], Train Loss: 9.0704, Test Loss: 850.0186\n",
      "Epoch [73800/100000], Train Loss: 8.7922, Test Loss: 850.7823\n",
      "Epoch [73850/100000], Train Loss: 7.5355, Test Loss: 853.7537\n",
      "Epoch [73900/100000], Train Loss: 10.8390, Test Loss: 853.0388\n",
      "Epoch [73950/100000], Train Loss: 9.1808, Test Loss: 850.6664\n",
      "Epoch [74000/100000], Train Loss: 12.0040, Test Loss: 854.0137\n",
      "Epoch [74050/100000], Train Loss: 7.6891, Test Loss: 852.1963\n",
      "Epoch [74100/100000], Train Loss: 10.4578, Test Loss: 849.0072\n",
      "Epoch [74150/100000], Train Loss: 9.8299, Test Loss: 851.6809\n",
      "Epoch [74200/100000], Train Loss: 10.8720, Test Loss: 852.6002\n",
      "Epoch [74250/100000], Train Loss: 6.6970, Test Loss: 852.6223\n",
      "Epoch [74300/100000], Train Loss: 15.0010, Test Loss: 852.7327\n",
      "Epoch [74350/100000], Train Loss: 7.1165, Test Loss: 850.9860\n",
      "Epoch [74400/100000], Train Loss: 7.3440, Test Loss: 848.7785\n",
      "Epoch [74450/100000], Train Loss: 11.4685, Test Loss: 851.9520\n",
      "Epoch [74500/100000], Train Loss: 9.4169, Test Loss: 855.4361\n",
      "Epoch [74550/100000], Train Loss: 9.2194, Test Loss: 848.6640\n",
      "Epoch [74600/100000], Train Loss: 8.0221, Test Loss: 855.2795\n",
      "Epoch [74650/100000], Train Loss: 8.8588, Test Loss: 852.4019\n",
      "Epoch [74700/100000], Train Loss: 12.2925, Test Loss: 856.1391\n",
      "Epoch [74750/100000], Train Loss: 10.8365, Test Loss: 854.8638\n",
      "Epoch [74800/100000], Train Loss: 8.2474, Test Loss: 851.4946\n",
      "Epoch [74850/100000], Train Loss: 8.4022, Test Loss: 852.0562\n",
      "Epoch [74900/100000], Train Loss: 9.0915, Test Loss: 851.3228\n",
      "Epoch [74950/100000], Train Loss: 10.4273, Test Loss: 855.1466\n",
      "Epoch [75000/100000], Train Loss: 9.7891, Test Loss: 855.3589\n",
      "Epoch [75050/100000], Train Loss: 6.2375, Test Loss: 853.3820\n",
      "Epoch [75100/100000], Train Loss: 10.2354, Test Loss: 853.7535\n",
      "Epoch [75150/100000], Train Loss: 8.2368, Test Loss: 853.7690\n",
      "Epoch [75200/100000], Train Loss: 9.4840, Test Loss: 853.3824\n",
      "Epoch [75250/100000], Train Loss: 6.5972, Test Loss: 852.4096\n",
      "Epoch [75300/100000], Train Loss: 9.5952, Test Loss: 858.3532\n",
      "Epoch [75350/100000], Train Loss: 9.8501, Test Loss: 859.1792\n",
      "Epoch [75400/100000], Train Loss: 6.7360, Test Loss: 851.9235\n",
      "Epoch [75450/100000], Train Loss: 8.3577, Test Loss: 850.7637\n",
      "Epoch [75500/100000], Train Loss: 8.8093, Test Loss: 851.7366\n",
      "Epoch [75550/100000], Train Loss: 7.7380, Test Loss: 854.6068\n",
      "Epoch [75600/100000], Train Loss: 8.4295, Test Loss: 854.2468\n",
      "Epoch [75650/100000], Train Loss: 8.5200, Test Loss: 853.2008\n",
      "Epoch [75700/100000], Train Loss: 11.5325, Test Loss: 856.2813\n",
      "Epoch [75750/100000], Train Loss: 10.0154, Test Loss: 854.1462\n",
      "Epoch [75800/100000], Train Loss: 6.8409, Test Loss: 857.5587\n",
      "Epoch [75850/100000], Train Loss: 9.1004, Test Loss: 855.5240\n",
      "Epoch [75900/100000], Train Loss: 7.3676, Test Loss: 855.1446\n",
      "Epoch [75950/100000], Train Loss: 9.1672, Test Loss: 858.0962\n",
      "Epoch [76000/100000], Train Loss: 8.2008, Test Loss: 856.2066\n",
      "Epoch [76050/100000], Train Loss: 9.4824, Test Loss: 858.1095\n",
      "Epoch [76100/100000], Train Loss: 9.9001, Test Loss: 856.4900\n",
      "Epoch [76150/100000], Train Loss: 8.9582, Test Loss: 856.2608\n",
      "Epoch [76200/100000], Train Loss: 11.6808, Test Loss: 855.6377\n",
      "Epoch [76250/100000], Train Loss: 7.7249, Test Loss: 855.1013\n",
      "Epoch [76300/100000], Train Loss: 8.2087, Test Loss: 852.8971\n",
      "Epoch [76350/100000], Train Loss: 6.2698, Test Loss: 857.6531\n",
      "Epoch [76400/100000], Train Loss: 8.2773, Test Loss: 853.9702\n",
      "Epoch [76450/100000], Train Loss: 8.4571, Test Loss: 856.4101\n",
      "Epoch [76500/100000], Train Loss: 11.2704, Test Loss: 856.4729\n",
      "Epoch [76550/100000], Train Loss: 8.0344, Test Loss: 852.7901\n",
      "Epoch [76600/100000], Train Loss: 9.5040, Test Loss: 859.8853\n",
      "Epoch [76650/100000], Train Loss: 8.8109, Test Loss: 855.8341\n",
      "Epoch [76700/100000], Train Loss: 8.6174, Test Loss: 856.0498\n",
      "Epoch [76750/100000], Train Loss: 7.6373, Test Loss: 858.6998\n",
      "Epoch [76800/100000], Train Loss: 9.1805, Test Loss: 859.1061\n",
      "Epoch [76850/100000], Train Loss: 8.7374, Test Loss: 856.3755\n",
      "Epoch [76900/100000], Train Loss: 9.4350, Test Loss: 859.9189\n",
      "Epoch [76950/100000], Train Loss: 9.8933, Test Loss: 858.8915\n",
      "Epoch [77000/100000], Train Loss: 11.3592, Test Loss: 858.3621\n",
      "Epoch [77050/100000], Train Loss: 9.3416, Test Loss: 859.1710\n",
      "Epoch [77100/100000], Train Loss: 8.2054, Test Loss: 860.1310\n",
      "Epoch [77150/100000], Train Loss: 8.4365, Test Loss: 857.4535\n",
      "Epoch [77200/100000], Train Loss: 11.1332, Test Loss: 858.3897\n",
      "Epoch [77250/100000], Train Loss: 9.5261, Test Loss: 861.7411\n",
      "Epoch [77300/100000], Train Loss: 8.2226, Test Loss: 858.0957\n",
      "Epoch [77350/100000], Train Loss: 10.3548, Test Loss: 856.4349\n",
      "Epoch [77400/100000], Train Loss: 6.8772, Test Loss: 857.8441\n",
      "Epoch [77450/100000], Train Loss: 8.6647, Test Loss: 854.7500\n",
      "Epoch [77500/100000], Train Loss: 6.9517, Test Loss: 859.0709\n",
      "Epoch [77550/100000], Train Loss: 7.6271, Test Loss: 857.5831\n",
      "Epoch [77600/100000], Train Loss: 8.9567, Test Loss: 857.4907\n",
      "Epoch [77650/100000], Train Loss: 11.1928, Test Loss: 859.9818\n",
      "Epoch [77700/100000], Train Loss: 12.1966, Test Loss: 864.4492\n",
      "Epoch [77750/100000], Train Loss: 9.2711, Test Loss: 861.5271\n",
      "Epoch [77800/100000], Train Loss: 7.6266, Test Loss: 858.9856\n",
      "Epoch [77850/100000], Train Loss: 12.4883, Test Loss: 860.8375\n",
      "Epoch [77900/100000], Train Loss: 10.5946, Test Loss: 860.8031\n",
      "Epoch [77950/100000], Train Loss: 8.6015, Test Loss: 861.3054\n",
      "Epoch [78000/100000], Train Loss: 9.8357, Test Loss: 864.5777\n",
      "Epoch [78050/100000], Train Loss: 6.9442, Test Loss: 858.0509\n",
      "Epoch [78100/100000], Train Loss: 9.9798, Test Loss: 863.6984\n",
      "Epoch [78150/100000], Train Loss: 10.2263, Test Loss: 857.5117\n",
      "Epoch [78200/100000], Train Loss: 7.1007, Test Loss: 860.3105\n",
      "Epoch [78250/100000], Train Loss: 8.8894, Test Loss: 860.4499\n",
      "Epoch [78300/100000], Train Loss: 8.9869, Test Loss: 860.3401\n",
      "Epoch [78350/100000], Train Loss: 9.9646, Test Loss: 864.0594\n",
      "Epoch [78400/100000], Train Loss: 7.8582, Test Loss: 857.3902\n",
      "Epoch [78450/100000], Train Loss: 8.1203, Test Loss: 860.3977\n",
      "Epoch [78500/100000], Train Loss: 8.4204, Test Loss: 865.1732\n",
      "Epoch [78550/100000], Train Loss: 8.4547, Test Loss: 861.6660\n",
      "Epoch [78600/100000], Train Loss: 10.1427, Test Loss: 863.7938\n",
      "Epoch [78650/100000], Train Loss: 12.7354, Test Loss: 859.7713\n",
      "Epoch [78700/100000], Train Loss: 8.0138, Test Loss: 859.0979\n",
      "Epoch [78750/100000], Train Loss: 9.0120, Test Loss: 862.4112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78800/100000], Train Loss: 10.4019, Test Loss: 862.5617\n",
      "Epoch [78850/100000], Train Loss: 8.9513, Test Loss: 860.4501\n",
      "Epoch [78900/100000], Train Loss: 9.0436, Test Loss: 857.1628\n",
      "Epoch [78950/100000], Train Loss: 8.7329, Test Loss: 858.2717\n",
      "Epoch [79000/100000], Train Loss: 8.7760, Test Loss: 860.1155\n",
      "Epoch [79050/100000], Train Loss: 9.0223, Test Loss: 861.6517\n",
      "Epoch [79100/100000], Train Loss: 10.1974, Test Loss: 862.4511\n",
      "Epoch [79150/100000], Train Loss: 7.8756, Test Loss: 859.9700\n",
      "Epoch [79200/100000], Train Loss: 10.0167, Test Loss: 861.7267\n",
      "Epoch [79250/100000], Train Loss: 7.5278, Test Loss: 859.7411\n",
      "Epoch [79300/100000], Train Loss: 8.4466, Test Loss: 860.9393\n",
      "Epoch [79350/100000], Train Loss: 9.3406, Test Loss: 861.5098\n",
      "Epoch [79400/100000], Train Loss: 8.1459, Test Loss: 863.7869\n",
      "Epoch [79450/100000], Train Loss: 9.7164, Test Loss: 863.9904\n",
      "Epoch [79500/100000], Train Loss: 9.9763, Test Loss: 866.0798\n",
      "Epoch [79550/100000], Train Loss: 8.4841, Test Loss: 861.6021\n",
      "Epoch [79600/100000], Train Loss: 8.2921, Test Loss: 863.1846\n",
      "Epoch [79650/100000], Train Loss: 9.4612, Test Loss: 859.3702\n",
      "Epoch [79700/100000], Train Loss: 6.8210, Test Loss: 861.8325\n",
      "Epoch [79750/100000], Train Loss: 6.8275, Test Loss: 860.6498\n",
      "Epoch [79800/100000], Train Loss: 7.3709, Test Loss: 862.6320\n",
      "Epoch [79850/100000], Train Loss: 9.0966, Test Loss: 863.9650\n",
      "Epoch [79900/100000], Train Loss: 9.4083, Test Loss: 859.8710\n",
      "Epoch [79950/100000], Train Loss: 6.3972, Test Loss: 862.1147\n",
      "Epoch [80000/100000], Train Loss: 13.0494, Test Loss: 860.2379\n",
      "Epoch [80050/100000], Train Loss: 10.3479, Test Loss: 862.3911\n",
      "Epoch [80100/100000], Train Loss: 8.2750, Test Loss: 862.2440\n",
      "Epoch [80150/100000], Train Loss: 8.9797, Test Loss: 861.2046\n",
      "Epoch [80200/100000], Train Loss: 10.1380, Test Loss: 860.2888\n",
      "Epoch [80250/100000], Train Loss: 9.6764, Test Loss: 864.9055\n",
      "Epoch [80300/100000], Train Loss: 9.4089, Test Loss: 862.6528\n",
      "Epoch [80350/100000], Train Loss: 8.6513, Test Loss: 862.9924\n",
      "Epoch [80400/100000], Train Loss: 7.7693, Test Loss: 864.8716\n",
      "Epoch [80450/100000], Train Loss: 10.2422, Test Loss: 864.9359\n",
      "Epoch [80500/100000], Train Loss: 8.7026, Test Loss: 867.6000\n",
      "Epoch [80550/100000], Train Loss: 8.5591, Test Loss: 861.6791\n",
      "Epoch [80600/100000], Train Loss: 7.8020, Test Loss: 863.6939\n",
      "Epoch [80650/100000], Train Loss: 7.2099, Test Loss: 862.0067\n",
      "Epoch [80700/100000], Train Loss: 8.3595, Test Loss: 864.7401\n",
      "Epoch [80750/100000], Train Loss: 8.1108, Test Loss: 863.5964\n",
      "Epoch [80800/100000], Train Loss: 8.8152, Test Loss: 865.8423\n",
      "Epoch [80850/100000], Train Loss: 10.2541, Test Loss: 864.2815\n",
      "Epoch [80900/100000], Train Loss: 13.0739, Test Loss: 865.2719\n",
      "Epoch [80950/100000], Train Loss: 9.6091, Test Loss: 863.7558\n",
      "Epoch [81000/100000], Train Loss: 8.8967, Test Loss: 868.4485\n",
      "Epoch [81050/100000], Train Loss: 8.7512, Test Loss: 863.0560\n",
      "Epoch [81100/100000], Train Loss: 8.3561, Test Loss: 865.4439\n",
      "Epoch [81150/100000], Train Loss: 10.9527, Test Loss: 863.4381\n",
      "Epoch [81200/100000], Train Loss: 9.8798, Test Loss: 868.2824\n",
      "Epoch [81250/100000], Train Loss: 9.5155, Test Loss: 865.6251\n",
      "Epoch [81300/100000], Train Loss: 6.6693, Test Loss: 866.1092\n",
      "Epoch [81350/100000], Train Loss: 8.1131, Test Loss: 864.4955\n",
      "Epoch [81400/100000], Train Loss: 12.1969, Test Loss: 866.3809\n",
      "Epoch [81450/100000], Train Loss: 7.5154, Test Loss: 864.8733\n",
      "Epoch [81500/100000], Train Loss: 7.1037, Test Loss: 862.8490\n",
      "Epoch [81550/100000], Train Loss: 6.8952, Test Loss: 865.5229\n",
      "Epoch [81600/100000], Train Loss: 7.2998, Test Loss: 867.3685\n",
      "Epoch [81650/100000], Train Loss: 10.2528, Test Loss: 866.8689\n",
      "Epoch [81700/100000], Train Loss: 8.8896, Test Loss: 865.9146\n",
      "Epoch [81750/100000], Train Loss: 9.1430, Test Loss: 864.9933\n",
      "Epoch [81800/100000], Train Loss: 6.6017, Test Loss: 862.5706\n",
      "Epoch [81850/100000], Train Loss: 10.9991, Test Loss: 865.4878\n",
      "Epoch [81900/100000], Train Loss: 9.9495, Test Loss: 865.6815\n",
      "Epoch [81950/100000], Train Loss: 8.0150, Test Loss: 870.3386\n",
      "Epoch [82000/100000], Train Loss: 7.8173, Test Loss: 868.3495\n",
      "Epoch [82050/100000], Train Loss: 8.3903, Test Loss: 865.2888\n",
      "Epoch [82100/100000], Train Loss: 10.3487, Test Loss: 870.9926\n",
      "Epoch [82150/100000], Train Loss: 8.4473, Test Loss: 865.2079\n",
      "Epoch [82200/100000], Train Loss: 9.5150, Test Loss: 862.2371\n",
      "Epoch [82250/100000], Train Loss: 8.1582, Test Loss: 865.9472\n",
      "Epoch [82300/100000], Train Loss: 7.8803, Test Loss: 868.3305\n",
      "Epoch [82350/100000], Train Loss: 9.7675, Test Loss: 861.2876\n",
      "Epoch [82400/100000], Train Loss: 8.4791, Test Loss: 866.7500\n",
      "Epoch [82450/100000], Train Loss: 7.3066, Test Loss: 868.1548\n",
      "Epoch [82500/100000], Train Loss: 7.6008, Test Loss: 863.5119\n",
      "Epoch [82550/100000], Train Loss: 8.5064, Test Loss: 864.8740\n",
      "Epoch [82600/100000], Train Loss: 9.4222, Test Loss: 868.1703\n",
      "Epoch [82650/100000], Train Loss: 8.1812, Test Loss: 865.6225\n",
      "Epoch [82700/100000], Train Loss: 10.1169, Test Loss: 866.3149\n",
      "Epoch [82750/100000], Train Loss: 10.5613, Test Loss: 867.5139\n",
      "Epoch [82800/100000], Train Loss: 6.6973, Test Loss: 867.2434\n",
      "Epoch [82850/100000], Train Loss: 7.8619, Test Loss: 865.1329\n",
      "Epoch [82900/100000], Train Loss: 7.3661, Test Loss: 862.2440\n",
      "Epoch [82950/100000], Train Loss: 7.4310, Test Loss: 867.4295\n",
      "Epoch [83000/100000], Train Loss: 9.7991, Test Loss: 867.2261\n",
      "Epoch [83050/100000], Train Loss: 7.3360, Test Loss: 867.0235\n",
      "Epoch [83100/100000], Train Loss: 6.6867, Test Loss: 865.9749\n",
      "Epoch [83150/100000], Train Loss: 7.3942, Test Loss: 867.2278\n",
      "Epoch [83200/100000], Train Loss: 7.8613, Test Loss: 869.3788\n",
      "Epoch [83250/100000], Train Loss: 9.3867, Test Loss: 865.9620\n",
      "Epoch [83300/100000], Train Loss: 7.8882, Test Loss: 864.4464\n",
      "Epoch [83350/100000], Train Loss: 8.6816, Test Loss: 866.9905\n",
      "Epoch [83400/100000], Train Loss: 9.4259, Test Loss: 872.4774\n",
      "Epoch [83450/100000], Train Loss: 9.0466, Test Loss: 865.2543\n",
      "Epoch [83500/100000], Train Loss: 11.2546, Test Loss: 863.1497\n",
      "Epoch [83550/100000], Train Loss: 6.8851, Test Loss: 864.7261\n",
      "Epoch [83600/100000], Train Loss: 7.5493, Test Loss: 867.5874\n",
      "Epoch [83650/100000], Train Loss: 11.2444, Test Loss: 865.8155\n",
      "Epoch [83700/100000], Train Loss: 7.7609, Test Loss: 866.4356\n",
      "Epoch [83750/100000], Train Loss: 8.5864, Test Loss: 868.0746\n",
      "Epoch [83800/100000], Train Loss: 8.5377, Test Loss: 867.2678\n",
      "Epoch [83850/100000], Train Loss: 9.0613, Test Loss: 865.1334\n",
      "Epoch [83900/100000], Train Loss: 8.4442, Test Loss: 867.1766\n",
      "Epoch [83950/100000], Train Loss: 8.5503, Test Loss: 864.1881\n",
      "Epoch [84000/100000], Train Loss: 9.3134, Test Loss: 868.3098\n",
      "Epoch [84050/100000], Train Loss: 7.2749, Test Loss: 866.6221\n",
      "Epoch [84100/100000], Train Loss: 6.9961, Test Loss: 868.0919\n",
      "Epoch [84150/100000], Train Loss: 10.7051, Test Loss: 870.9257\n",
      "Epoch [84200/100000], Train Loss: 9.2989, Test Loss: 870.4460\n",
      "Epoch [84250/100000], Train Loss: 8.5512, Test Loss: 867.5144\n",
      "Epoch [84300/100000], Train Loss: 9.3969, Test Loss: 868.9793\n",
      "Epoch [84350/100000], Train Loss: 7.8221, Test Loss: 868.2457\n",
      "Epoch [84400/100000], Train Loss: 7.6127, Test Loss: 869.1926\n",
      "Epoch [84450/100000], Train Loss: 9.4002, Test Loss: 865.6717\n",
      "Epoch [84500/100000], Train Loss: 9.1635, Test Loss: 867.8060\n",
      "Epoch [84550/100000], Train Loss: 8.6693, Test Loss: 870.2515\n",
      "Epoch [84600/100000], Train Loss: 8.7749, Test Loss: 867.5024\n",
      "Epoch [84650/100000], Train Loss: 12.3663, Test Loss: 868.4043\n",
      "Epoch [84700/100000], Train Loss: 8.8878, Test Loss: 872.2445\n",
      "Epoch [84750/100000], Train Loss: 8.9024, Test Loss: 870.3056\n",
      "Epoch [84800/100000], Train Loss: 9.3834, Test Loss: 871.8853\n",
      "Epoch [84850/100000], Train Loss: 8.7925, Test Loss: 872.8026\n",
      "Epoch [84900/100000], Train Loss: 8.2114, Test Loss: 869.9013\n",
      "Epoch [84950/100000], Train Loss: 8.2992, Test Loss: 869.3029\n",
      "Epoch [85000/100000], Train Loss: 8.3881, Test Loss: 869.0539\n",
      "Epoch [85050/100000], Train Loss: 8.4212, Test Loss: 870.0915\n",
      "Epoch [85100/100000], Train Loss: 9.2651, Test Loss: 870.5357\n",
      "Epoch [85150/100000], Train Loss: 9.0351, Test Loss: 869.5147\n",
      "Epoch [85200/100000], Train Loss: 6.4432, Test Loss: 871.0724\n",
      "Epoch [85250/100000], Train Loss: 7.8475, Test Loss: 871.8401\n",
      "Epoch [85300/100000], Train Loss: 8.3033, Test Loss: 868.9016\n",
      "Epoch [85350/100000], Train Loss: 7.7683, Test Loss: 873.3560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85400/100000], Train Loss: 9.3102, Test Loss: 872.5427\n",
      "Epoch [85450/100000], Train Loss: 7.9876, Test Loss: 868.6339\n",
      "Epoch [85500/100000], Train Loss: 8.6819, Test Loss: 873.7477\n",
      "Epoch [85550/100000], Train Loss: 8.9065, Test Loss: 872.5449\n",
      "Epoch [85600/100000], Train Loss: 7.2574, Test Loss: 872.2218\n",
      "Epoch [85650/100000], Train Loss: 8.7709, Test Loss: 870.8086\n",
      "Epoch [85700/100000], Train Loss: 9.4837, Test Loss: 871.5210\n",
      "Epoch [85750/100000], Train Loss: 8.6723, Test Loss: 867.1786\n",
      "Epoch [85800/100000], Train Loss: 7.2144, Test Loss: 869.8399\n",
      "Epoch [85850/100000], Train Loss: 9.3071, Test Loss: 871.1199\n",
      "Epoch [85900/100000], Train Loss: 11.1149, Test Loss: 869.8269\n",
      "Epoch [85950/100000], Train Loss: 9.6547, Test Loss: 870.3798\n",
      "Epoch [86000/100000], Train Loss: 7.0305, Test Loss: 870.4927\n",
      "Epoch [86050/100000], Train Loss: 5.7779, Test Loss: 869.0738\n",
      "Epoch [86100/100000], Train Loss: 9.3921, Test Loss: 872.0528\n",
      "Epoch [86150/100000], Train Loss: 7.8037, Test Loss: 874.8879\n",
      "Epoch [86200/100000], Train Loss: 8.0539, Test Loss: 873.4238\n",
      "Epoch [86250/100000], Train Loss: 7.1827, Test Loss: 868.1606\n",
      "Epoch [86300/100000], Train Loss: 8.7840, Test Loss: 872.4825\n",
      "Epoch [86350/100000], Train Loss: 10.1081, Test Loss: 872.5493\n",
      "Epoch [86400/100000], Train Loss: 9.3548, Test Loss: 875.1127\n",
      "Epoch [86450/100000], Train Loss: 7.9850, Test Loss: 870.1242\n",
      "Epoch [86500/100000], Train Loss: 10.7728, Test Loss: 871.4512\n",
      "Epoch [86550/100000], Train Loss: 8.8901, Test Loss: 868.1563\n",
      "Epoch [86600/100000], Train Loss: 10.8154, Test Loss: 873.1473\n",
      "Epoch [86650/100000], Train Loss: 8.4592, Test Loss: 869.3648\n",
      "Epoch [86700/100000], Train Loss: 10.5548, Test Loss: 871.0550\n",
      "Epoch [86750/100000], Train Loss: 6.3553, Test Loss: 874.1439\n",
      "Epoch [86800/100000], Train Loss: 6.4723, Test Loss: 868.5046\n",
      "Epoch [86850/100000], Train Loss: 7.6056, Test Loss: 870.3442\n",
      "Epoch [86900/100000], Train Loss: 8.2673, Test Loss: 869.8641\n",
      "Epoch [86950/100000], Train Loss: 8.5227, Test Loss: 871.2217\n",
      "Epoch [87000/100000], Train Loss: 10.3214, Test Loss: 875.1969\n",
      "Epoch [87050/100000], Train Loss: 10.7823, Test Loss: 872.9644\n",
      "Epoch [87100/100000], Train Loss: 7.1044, Test Loss: 874.2524\n",
      "Epoch [87150/100000], Train Loss: 7.1929, Test Loss: 875.7186\n",
      "Epoch [87200/100000], Train Loss: 8.1591, Test Loss: 873.3476\n",
      "Epoch [87250/100000], Train Loss: 9.0736, Test Loss: 875.2464\n",
      "Epoch [87300/100000], Train Loss: 6.4430, Test Loss: 875.9530\n",
      "Epoch [87350/100000], Train Loss: 7.0763, Test Loss: 869.8738\n",
      "Epoch [87400/100000], Train Loss: 8.8231, Test Loss: 873.9083\n",
      "Epoch [87450/100000], Train Loss: 8.1238, Test Loss: 871.1161\n",
      "Epoch [87500/100000], Train Loss: 7.3590, Test Loss: 869.8067\n",
      "Epoch [87550/100000], Train Loss: 9.1301, Test Loss: 871.7916\n",
      "Epoch [87600/100000], Train Loss: 8.8627, Test Loss: 870.7653\n",
      "Epoch [87650/100000], Train Loss: 5.8845, Test Loss: 870.4828\n",
      "Epoch [87700/100000], Train Loss: 6.2954, Test Loss: 876.2587\n",
      "Epoch [87750/100000], Train Loss: 8.6510, Test Loss: 877.5059\n",
      "Epoch [87800/100000], Train Loss: 8.9175, Test Loss: 872.1939\n",
      "Epoch [87850/100000], Train Loss: 7.5034, Test Loss: 872.8374\n",
      "Epoch [87900/100000], Train Loss: 8.6742, Test Loss: 875.4291\n",
      "Epoch [87950/100000], Train Loss: 9.6665, Test Loss: 873.3237\n",
      "Epoch [88000/100000], Train Loss: 6.9938, Test Loss: 876.2047\n",
      "Epoch [88050/100000], Train Loss: 8.4588, Test Loss: 873.5431\n",
      "Epoch [88100/100000], Train Loss: 9.3757, Test Loss: 880.6190\n",
      "Epoch [88150/100000], Train Loss: 9.3498, Test Loss: 874.8064\n",
      "Epoch [88200/100000], Train Loss: 7.8765, Test Loss: 874.5827\n",
      "Epoch [88250/100000], Train Loss: 10.5063, Test Loss: 873.2803\n",
      "Epoch [88300/100000], Train Loss: 7.8277, Test Loss: 879.9482\n",
      "Epoch [88350/100000], Train Loss: 7.5172, Test Loss: 878.2693\n",
      "Epoch [88400/100000], Train Loss: 9.5188, Test Loss: 872.0679\n",
      "Epoch [88450/100000], Train Loss: 9.1080, Test Loss: 875.3970\n",
      "Epoch [88500/100000], Train Loss: 13.0232, Test Loss: 875.8523\n",
      "Epoch [88550/100000], Train Loss: 7.4959, Test Loss: 876.5043\n",
      "Epoch [88600/100000], Train Loss: 8.2134, Test Loss: 876.1974\n",
      "Epoch [88650/100000], Train Loss: 7.8306, Test Loss: 874.1057\n",
      "Epoch [88700/100000], Train Loss: 7.6705, Test Loss: 878.1261\n",
      "Epoch [88750/100000], Train Loss: 9.1868, Test Loss: 879.8084\n",
      "Epoch [88800/100000], Train Loss: 7.3826, Test Loss: 873.6182\n",
      "Epoch [88850/100000], Train Loss: 8.5003, Test Loss: 874.7025\n",
      "Epoch [88900/100000], Train Loss: 9.0375, Test Loss: 877.3927\n",
      "Epoch [88950/100000], Train Loss: 6.1842, Test Loss: 874.9380\n",
      "Epoch [89000/100000], Train Loss: 7.3639, Test Loss: 875.1226\n",
      "Epoch [89050/100000], Train Loss: 7.9677, Test Loss: 874.2468\n",
      "Epoch [89100/100000], Train Loss: 8.9406, Test Loss: 878.9611\n",
      "Epoch [89150/100000], Train Loss: 9.1964, Test Loss: 875.5573\n",
      "Epoch [89200/100000], Train Loss: 9.3352, Test Loss: 876.2705\n",
      "Epoch [89250/100000], Train Loss: 7.8047, Test Loss: 872.1024\n",
      "Epoch [89300/100000], Train Loss: 7.1851, Test Loss: 876.5318\n",
      "Epoch [89350/100000], Train Loss: 9.7294, Test Loss: 876.0305\n",
      "Epoch [89400/100000], Train Loss: 8.7093, Test Loss: 877.9972\n",
      "Epoch [89450/100000], Train Loss: 8.6095, Test Loss: 874.2444\n",
      "Epoch [89500/100000], Train Loss: 8.2880, Test Loss: 876.5955\n",
      "Epoch [89550/100000], Train Loss: 8.8917, Test Loss: 879.0058\n",
      "Epoch [89600/100000], Train Loss: 9.0923, Test Loss: 880.5113\n",
      "Epoch [89650/100000], Train Loss: 8.0646, Test Loss: 877.2321\n",
      "Epoch [89700/100000], Train Loss: 10.5556, Test Loss: 875.4067\n",
      "Epoch [89750/100000], Train Loss: 8.3793, Test Loss: 876.5375\n",
      "Epoch [89800/100000], Train Loss: 7.6896, Test Loss: 875.7465\n",
      "Epoch [89850/100000], Train Loss: 5.7798, Test Loss: 875.9092\n",
      "Epoch [89900/100000], Train Loss: 8.5936, Test Loss: 877.8655\n",
      "Epoch [89950/100000], Train Loss: 10.3385, Test Loss: 879.6569\n",
      "Epoch [90000/100000], Train Loss: 8.9181, Test Loss: 877.0923\n",
      "Epoch [90050/100000], Train Loss: 8.9380, Test Loss: 876.1217\n",
      "Epoch [90100/100000], Train Loss: 8.1754, Test Loss: 881.7038\n",
      "Epoch [90150/100000], Train Loss: 9.8580, Test Loss: 883.2683\n",
      "Epoch [90200/100000], Train Loss: 8.7249, Test Loss: 877.0450\n",
      "Epoch [90250/100000], Train Loss: 9.6422, Test Loss: 879.7974\n",
      "Epoch [90300/100000], Train Loss: 8.0104, Test Loss: 874.9119\n",
      "Epoch [90350/100000], Train Loss: 7.9432, Test Loss: 876.8243\n",
      "Epoch [90400/100000], Train Loss: 10.2217, Test Loss: 876.1947\n",
      "Epoch [90450/100000], Train Loss: 9.2618, Test Loss: 880.0967\n",
      "Epoch [90500/100000], Train Loss: 8.5194, Test Loss: 879.9338\n",
      "Epoch [90550/100000], Train Loss: 8.4904, Test Loss: 883.5378\n",
      "Epoch [90600/100000], Train Loss: 6.7110, Test Loss: 879.1083\n",
      "Epoch [90650/100000], Train Loss: 9.0251, Test Loss: 878.2533\n",
      "Epoch [90700/100000], Train Loss: 7.5434, Test Loss: 877.8690\n",
      "Epoch [90750/100000], Train Loss: 9.1677, Test Loss: 878.8797\n",
      "Epoch [90800/100000], Train Loss: 8.4354, Test Loss: 878.0225\n",
      "Epoch [90850/100000], Train Loss: 8.5829, Test Loss: 879.6068\n",
      "Epoch [90900/100000], Train Loss: 10.0136, Test Loss: 880.4589\n",
      "Epoch [90950/100000], Train Loss: 5.7375, Test Loss: 878.6275\n",
      "Epoch [91000/100000], Train Loss: 7.0571, Test Loss: 878.1973\n",
      "Epoch [91050/100000], Train Loss: 9.0881, Test Loss: 878.1730\n",
      "Epoch [91100/100000], Train Loss: 9.7365, Test Loss: 879.2410\n",
      "Epoch [91150/100000], Train Loss: 9.5484, Test Loss: 879.1483\n",
      "Epoch [91200/100000], Train Loss: 7.3726, Test Loss: 879.7050\n",
      "Epoch [91250/100000], Train Loss: 8.7162, Test Loss: 884.3044\n",
      "Epoch [91300/100000], Train Loss: 9.3482, Test Loss: 881.5274\n",
      "Epoch [91350/100000], Train Loss: 7.3950, Test Loss: 882.2867\n",
      "Epoch [91400/100000], Train Loss: 7.9662, Test Loss: 879.6689\n",
      "Epoch [91450/100000], Train Loss: 7.9628, Test Loss: 878.9822\n",
      "Epoch [91500/100000], Train Loss: 6.1348, Test Loss: 880.3481\n",
      "Epoch [91550/100000], Train Loss: 8.8117, Test Loss: 882.4252\n",
      "Epoch [91600/100000], Train Loss: 8.2062, Test Loss: 880.1094\n",
      "Epoch [91650/100000], Train Loss: 7.0568, Test Loss: 879.9346\n",
      "Epoch [91700/100000], Train Loss: 7.1047, Test Loss: 880.5260\n",
      "Epoch [91750/100000], Train Loss: 9.0150, Test Loss: 876.9943\n",
      "Epoch [91800/100000], Train Loss: 11.6277, Test Loss: 875.5195\n",
      "Epoch [91850/100000], Train Loss: 7.1042, Test Loss: 877.7682\n",
      "Epoch [91900/100000], Train Loss: 7.2556, Test Loss: 880.2935\n",
      "Epoch [91950/100000], Train Loss: 11.0521, Test Loss: 880.7268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92000/100000], Train Loss: 6.1522, Test Loss: 877.6734\n",
      "Epoch [92050/100000], Train Loss: 7.5351, Test Loss: 880.3723\n",
      "Epoch [92100/100000], Train Loss: 7.3491, Test Loss: 881.0698\n",
      "Epoch [92150/100000], Train Loss: 10.6129, Test Loss: 881.2434\n",
      "Epoch [92200/100000], Train Loss: 10.7064, Test Loss: 880.6673\n",
      "Epoch [92250/100000], Train Loss: 8.7643, Test Loss: 881.3886\n",
      "Epoch [92300/100000], Train Loss: 9.5666, Test Loss: 877.1963\n",
      "Epoch [92350/100000], Train Loss: 8.5844, Test Loss: 880.0765\n",
      "Epoch [92400/100000], Train Loss: 10.6180, Test Loss: 878.4962\n",
      "Epoch [92450/100000], Train Loss: 6.8863, Test Loss: 878.8243\n",
      "Epoch [92500/100000], Train Loss: 11.1200, Test Loss: 878.2755\n",
      "Epoch [92550/100000], Train Loss: 13.0345, Test Loss: 883.7736\n",
      "Epoch [92600/100000], Train Loss: 10.0267, Test Loss: 880.4549\n",
      "Epoch [92650/100000], Train Loss: 6.1147, Test Loss: 878.8649\n",
      "Epoch [92700/100000], Train Loss: 6.6657, Test Loss: 882.7895\n",
      "Epoch [92750/100000], Train Loss: 7.3679, Test Loss: 879.9977\n",
      "Epoch [92800/100000], Train Loss: 7.5459, Test Loss: 882.0964\n",
      "Epoch [92850/100000], Train Loss: 8.7636, Test Loss: 880.7444\n",
      "Epoch [92900/100000], Train Loss: 8.2960, Test Loss: 880.5532\n",
      "Epoch [92950/100000], Train Loss: 9.1592, Test Loss: 880.0612\n",
      "Epoch [93000/100000], Train Loss: 10.5247, Test Loss: 883.6017\n",
      "Epoch [93050/100000], Train Loss: 8.3483, Test Loss: 878.8445\n",
      "Epoch [93100/100000], Train Loss: 9.1292, Test Loss: 882.5107\n",
      "Epoch [93150/100000], Train Loss: 6.9809, Test Loss: 882.8210\n",
      "Epoch [93200/100000], Train Loss: 8.0555, Test Loss: 883.3569\n",
      "Epoch [93250/100000], Train Loss: 7.5044, Test Loss: 883.1223\n",
      "Epoch [93300/100000], Train Loss: 7.4569, Test Loss: 881.7760\n",
      "Epoch [93350/100000], Train Loss: 7.4666, Test Loss: 880.5309\n",
      "Epoch [93400/100000], Train Loss: 9.2233, Test Loss: 876.8829\n",
      "Epoch [93450/100000], Train Loss: 9.1351, Test Loss: 881.0722\n",
      "Epoch [93500/100000], Train Loss: 9.0228, Test Loss: 880.6323\n",
      "Epoch [93550/100000], Train Loss: 10.5753, Test Loss: 880.9197\n",
      "Epoch [93600/100000], Train Loss: 6.7389, Test Loss: 879.7232\n",
      "Epoch [93650/100000], Train Loss: 9.0766, Test Loss: 882.5107\n",
      "Epoch [93700/100000], Train Loss: 7.3294, Test Loss: 880.9835\n",
      "Epoch [93750/100000], Train Loss: 8.4327, Test Loss: 882.4604\n",
      "Epoch [93800/100000], Train Loss: 9.4612, Test Loss: 882.1903\n",
      "Epoch [93850/100000], Train Loss: 7.7614, Test Loss: 885.0067\n",
      "Epoch [93900/100000], Train Loss: 7.1557, Test Loss: 882.1744\n",
      "Epoch [93950/100000], Train Loss: 9.8728, Test Loss: 885.2568\n",
      "Epoch [94000/100000], Train Loss: 6.1997, Test Loss: 879.4016\n",
      "Epoch [94050/100000], Train Loss: 8.7028, Test Loss: 882.9850\n",
      "Epoch [94100/100000], Train Loss: 6.9634, Test Loss: 883.3151\n",
      "Epoch [94150/100000], Train Loss: 7.1242, Test Loss: 885.3083\n",
      "Epoch [94200/100000], Train Loss: 8.5141, Test Loss: 882.4232\n",
      "Epoch [94250/100000], Train Loss: 10.5224, Test Loss: 882.5874\n",
      "Epoch [94300/100000], Train Loss: 7.9801, Test Loss: 883.9049\n",
      "Epoch [94350/100000], Train Loss: 7.9082, Test Loss: 884.6720\n",
      "Epoch [94400/100000], Train Loss: 9.4082, Test Loss: 884.0018\n",
      "Epoch [94450/100000], Train Loss: 8.7249, Test Loss: 883.4048\n",
      "Epoch [94500/100000], Train Loss: 8.2435, Test Loss: 883.9760\n",
      "Epoch [94550/100000], Train Loss: 8.7791, Test Loss: 888.7086\n",
      "Epoch [94600/100000], Train Loss: 9.7955, Test Loss: 884.2409\n",
      "Epoch [94650/100000], Train Loss: 7.7459, Test Loss: 883.3869\n",
      "Epoch [94700/100000], Train Loss: 9.9008, Test Loss: 885.8442\n",
      "Epoch [94750/100000], Train Loss: 10.3718, Test Loss: 888.8708\n",
      "Epoch [94800/100000], Train Loss: 8.9552, Test Loss: 886.4496\n",
      "Epoch [94850/100000], Train Loss: 8.2144, Test Loss: 885.2875\n",
      "Epoch [94900/100000], Train Loss: 7.9693, Test Loss: 882.4190\n",
      "Epoch [94950/100000], Train Loss: 9.4467, Test Loss: 883.1414\n",
      "Epoch [95000/100000], Train Loss: 10.4388, Test Loss: 883.3373\n",
      "Epoch [95050/100000], Train Loss: 8.7179, Test Loss: 884.8313\n",
      "Epoch [95100/100000], Train Loss: 8.5031, Test Loss: 890.2320\n",
      "Epoch [95150/100000], Train Loss: 9.4530, Test Loss: 885.4521\n",
      "Epoch [95200/100000], Train Loss: 8.8041, Test Loss: 886.8393\n",
      "Epoch [95250/100000], Train Loss: 8.3154, Test Loss: 886.7521\n",
      "Epoch [95300/100000], Train Loss: 7.7034, Test Loss: 887.9732\n",
      "Epoch [95350/100000], Train Loss: 12.6785, Test Loss: 885.6064\n",
      "Epoch [95400/100000], Train Loss: 8.9090, Test Loss: 882.7492\n",
      "Epoch [95450/100000], Train Loss: 6.8240, Test Loss: 889.2152\n",
      "Epoch [95500/100000], Train Loss: 8.2951, Test Loss: 886.0531\n",
      "Epoch [95550/100000], Train Loss: 8.0185, Test Loss: 880.9004\n",
      "Epoch [95600/100000], Train Loss: 7.5482, Test Loss: 886.7537\n",
      "Epoch [95650/100000], Train Loss: 6.1674, Test Loss: 884.0459\n",
      "Epoch [95700/100000], Train Loss: 7.1020, Test Loss: 883.1901\n",
      "Epoch [95750/100000], Train Loss: 8.1680, Test Loss: 887.5666\n",
      "Epoch [95800/100000], Train Loss: 6.7676, Test Loss: 884.1488\n",
      "Epoch [95850/100000], Train Loss: 7.7662, Test Loss: 885.0931\n",
      "Epoch [95900/100000], Train Loss: 7.4934, Test Loss: 884.5551\n",
      "Epoch [95950/100000], Train Loss: 8.2619, Test Loss: 885.9712\n",
      "Epoch [96000/100000], Train Loss: 8.2295, Test Loss: 883.4582\n",
      "Epoch [96050/100000], Train Loss: 8.5516, Test Loss: 883.3775\n",
      "Epoch [96100/100000], Train Loss: 8.6209, Test Loss: 887.8727\n",
      "Epoch [96150/100000], Train Loss: 8.3775, Test Loss: 886.7279\n",
      "Epoch [96200/100000], Train Loss: 8.6156, Test Loss: 884.0740\n",
      "Epoch [96250/100000], Train Loss: 7.5651, Test Loss: 886.4282\n",
      "Epoch [96300/100000], Train Loss: 9.4544, Test Loss: 884.2625\n",
      "Epoch [96350/100000], Train Loss: 8.7043, Test Loss: 885.6605\n",
      "Epoch [96400/100000], Train Loss: 7.6743, Test Loss: 888.2241\n",
      "Epoch [96450/100000], Train Loss: 10.3366, Test Loss: 884.9910\n",
      "Epoch [96500/100000], Train Loss: 8.1920, Test Loss: 886.0952\n",
      "Epoch [96550/100000], Train Loss: 7.3496, Test Loss: 885.2806\n",
      "Epoch [96600/100000], Train Loss: 7.7877, Test Loss: 885.0681\n",
      "Epoch [96650/100000], Train Loss: 8.4629, Test Loss: 884.8515\n",
      "Epoch [96700/100000], Train Loss: 7.9942, Test Loss: 886.2987\n",
      "Epoch [96750/100000], Train Loss: 7.8725, Test Loss: 885.4242\n",
      "Epoch [96800/100000], Train Loss: 8.6564, Test Loss: 884.1685\n",
      "Epoch [96850/100000], Train Loss: 8.3920, Test Loss: 890.1523\n",
      "Epoch [96900/100000], Train Loss: 6.1005, Test Loss: 887.5039\n",
      "Epoch [96950/100000], Train Loss: 10.5284, Test Loss: 886.4092\n",
      "Epoch [97000/100000], Train Loss: 9.1786, Test Loss: 888.5255\n",
      "Epoch [97050/100000], Train Loss: 9.2235, Test Loss: 884.8158\n",
      "Epoch [97100/100000], Train Loss: 8.3990, Test Loss: 887.5386\n",
      "Epoch [97150/100000], Train Loss: 7.4116, Test Loss: 884.8592\n",
      "Epoch [97200/100000], Train Loss: 6.6501, Test Loss: 886.7605\n",
      "Epoch [97250/100000], Train Loss: 8.0891, Test Loss: 887.2906\n",
      "Epoch [97300/100000], Train Loss: 10.9125, Test Loss: 884.6711\n",
      "Epoch [97350/100000], Train Loss: 8.8047, Test Loss: 886.1938\n",
      "Epoch [97400/100000], Train Loss: 6.5565, Test Loss: 886.7188\n",
      "Epoch [97450/100000], Train Loss: 8.5257, Test Loss: 888.6726\n",
      "Epoch [97500/100000], Train Loss: 6.8812, Test Loss: 886.6372\n",
      "Epoch [97550/100000], Train Loss: 8.2744, Test Loss: 887.0311\n",
      "Epoch [97600/100000], Train Loss: 7.5515, Test Loss: 887.5334\n",
      "Epoch [97650/100000], Train Loss: 11.5647, Test Loss: 889.1022\n",
      "Epoch [97700/100000], Train Loss: 6.5791, Test Loss: 887.2072\n",
      "Epoch [97750/100000], Train Loss: 8.7015, Test Loss: 890.5311\n",
      "Epoch [97800/100000], Train Loss: 8.4856, Test Loss: 886.5005\n",
      "Epoch [97850/100000], Train Loss: 6.4155, Test Loss: 889.9782\n",
      "Epoch [97900/100000], Train Loss: 7.6353, Test Loss: 887.5699\n",
      "Epoch [97950/100000], Train Loss: 8.0136, Test Loss: 891.9835\n",
      "Epoch [98000/100000], Train Loss: 8.0530, Test Loss: 889.0672\n",
      "Epoch [98050/100000], Train Loss: 8.5586, Test Loss: 887.1119\n",
      "Epoch [98100/100000], Train Loss: 7.8523, Test Loss: 885.8117\n",
      "Epoch [98150/100000], Train Loss: 8.5302, Test Loss: 886.8209\n",
      "Epoch [98200/100000], Train Loss: 7.5157, Test Loss: 893.4401\n",
      "Epoch [98250/100000], Train Loss: 9.2037, Test Loss: 888.2102\n",
      "Epoch [98300/100000], Train Loss: 7.9145, Test Loss: 887.0621\n",
      "Epoch [98350/100000], Train Loss: 10.4266, Test Loss: 889.2229\n",
      "Epoch [98400/100000], Train Loss: 7.5233, Test Loss: 886.6275\n",
      "Epoch [98450/100000], Train Loss: 9.5601, Test Loss: 885.5814\n",
      "Epoch [98500/100000], Train Loss: 7.3367, Test Loss: 886.6547\n",
      "Epoch [98550/100000], Train Loss: 8.2924, Test Loss: 888.4968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98600/100000], Train Loss: 7.4937, Test Loss: 887.0353\n",
      "Epoch [98650/100000], Train Loss: 6.6809, Test Loss: 889.0153\n",
      "Epoch [98700/100000], Train Loss: 9.2440, Test Loss: 888.8895\n",
      "Epoch [98750/100000], Train Loss: 8.4013, Test Loss: 890.4963\n",
      "Epoch [98800/100000], Train Loss: 8.6080, Test Loss: 889.5926\n",
      "Epoch [98850/100000], Train Loss: 13.9136, Test Loss: 889.6476\n",
      "Epoch [98900/100000], Train Loss: 9.4969, Test Loss: 889.6435\n",
      "Epoch [98950/100000], Train Loss: 9.2274, Test Loss: 888.3491\n",
      "Epoch [99000/100000], Train Loss: 7.3041, Test Loss: 889.4617\n",
      "Epoch [99050/100000], Train Loss: 7.5467, Test Loss: 885.3584\n",
      "Epoch [99100/100000], Train Loss: 7.0843, Test Loss: 894.5729\n",
      "Epoch [99150/100000], Train Loss: 10.4507, Test Loss: 894.1858\n",
      "Epoch [99200/100000], Train Loss: 4.8888, Test Loss: 891.7507\n",
      "Epoch [99250/100000], Train Loss: 7.3137, Test Loss: 889.6231\n",
      "Epoch [99300/100000], Train Loss: 8.6671, Test Loss: 890.6228\n",
      "Epoch [99350/100000], Train Loss: 6.9339, Test Loss: 888.1042\n",
      "Epoch [99400/100000], Train Loss: 8.4322, Test Loss: 893.5702\n",
      "Epoch [99450/100000], Train Loss: 11.3521, Test Loss: 890.3267\n",
      "Epoch [99500/100000], Train Loss: 10.2116, Test Loss: 891.1385\n",
      "Epoch [99550/100000], Train Loss: 6.3739, Test Loss: 889.0429\n",
      "Epoch [99600/100000], Train Loss: 7.4124, Test Loss: 888.8155\n",
      "Epoch [99650/100000], Train Loss: 7.8663, Test Loss: 889.0281\n",
      "Epoch [99700/100000], Train Loss: 12.9777, Test Loss: 892.2456\n",
      "Epoch [99750/100000], Train Loss: 7.5091, Test Loss: 890.8038\n",
      "Epoch [99800/100000], Train Loss: 7.5860, Test Loss: 891.1263\n",
      "Epoch [99850/100000], Train Loss: 6.7754, Test Loss: 893.7777\n",
      "Epoch [99900/100000], Train Loss: 6.6053, Test Loss: 888.1642\n",
      "Epoch [99950/100000], Train Loss: 7.8642, Test Loss: 890.0626\n",
      "Epoch [100000/100000], Train Loss: 7.1147, Test Loss: 888.8866\n",
      "Execution time: 38680.1899600029 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to directory for saving model parameters\n",
    "#os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/SavedParamsAndTrainingMetrics')\n",
    "#np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "#np.save(ModelName + \"_TestLoss.npy\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot the data\n",
    "plt.plot(np.arange(num_epochs)+1, train_losses, label='Train Loss')\n",
    "plt.plot(np.arange(num_epochs)+1, test_losses, label='Test Loss')\n",
    "\n",
    "# Track the previous minimum test loss and its index\n",
    "prev_min_loss = test_losses[0]\n",
    "prev_min_index = 0\n",
    "\n",
    "# Annotate each local minimum test loss with arrows\n",
    "for idx, loss in enumerate(test_losses[1:], start=1):\n",
    "    if loss < prev_min_loss:\n",
    "        plt.annotate('Min', xy=(idx+1, loss), xytext=(idx+1, loss + 5000),\n",
    "                     arrowprops=dict(facecolor='red', shrink=0.05))\n",
    "        prev_min_loss = loss\n",
    "        prev_min_index = idx\n",
    "        \n",
    "# Add x and y labels\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "\n",
    "# Change axis size\n",
    "plt.rcParams['axes.labelsize'] = 45  # Change label font size\n",
    "\n",
    "# Change tick size\n",
    "plt.tick_params(axis='x', labelsize=30)  # Change tick size for x-axis\n",
    "plt.tick_params(axis='y', labelsize=30)  # Change tick size for y-axis\n",
    "\n",
    "# Plot legend, and display figure\n",
    "plt.legend(fontsize = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Aq(\n",
       "  (lin1): Linear(in_features=39500, out_features=200, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (lin2): Linear(in_features=200, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Aq()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Switch to directory for saving model metrics\n",
    "\n",
    "os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/ModelPerformanceMetrics')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model err:  0.034866553\n"
     ]
    }
   ],
   "source": [
    "## Test model on testing dataset and deterine RMSE\n",
    "\n",
    "outputs = model_aq(X_train) # Evaluate input spectra with MLP\n",
    "\n",
    "# Move tensors to CPU and convert to numpy arrays\n",
    "outputs_cpu = outputs.detach().cpu().numpy()\n",
    "y_train_cpu = y_train.detach().cpu().numpy()\n",
    "\n",
    "err = np.sqrt(mean_squared_error(outputs_cpu, y_train_cpu))  # Determine RMSE\n",
    "print('model err: ', err)  # Print RMSE\n",
    "\n",
    "np.save(ModelName + \"TrainRMSE\",err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model err:  0.43436542\n"
     ]
    }
   ],
   "source": [
    "## Test model on testing dataset and deterine RMSE\n",
    "\n",
    "model_aq.eval() # Change to evaluation mode (maybe not needed for this model)\n",
    "outputs = model_aq(X_test) # Evaluate input spectra with MLP\n",
    "\n",
    "# Move tensors to CPU and convert to numpy arrays\n",
    "outputs_cpu = outputs.detach().cpu().numpy()\n",
    "y_test_cpu = y_test.detach().cpu().numpy()\n",
    "\n",
    "err = np.sqrt(mean_squared_error(outputs_cpu, y_test_cpu))  # Determine RMSE\n",
    "print('model err: ', err)  # Print RMSE\n",
    "\n",
    "np.save(ModelName + \"TestRMSE\",err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model err:  0.3047901\n"
     ]
    }
   ],
   "source": [
    "## Test model on validation dataset and deterine RMSE\n",
    "\n",
    "model_aq.eval()  # Change to evaluation mode (maybe not needed for this model)\n",
    "outputs = model_aq(spectraVal)  # Evaluate input spectra with MLP\n",
    "\n",
    "# Move tensors to CPU and convert to numpy arrays\n",
    "outputs_cpu = outputs.detach().cpu().numpy()\n",
    "concVal_cpu = concVal.detach().cpu().numpy()\n",
    "\n",
    "err = np.sqrt(mean_squared_error(outputs_cpu, concVal_cpu))  # Determine RMSE\n",
    "print('model err: ', err)  # Print RMSE\n",
    "\n",
    "np.save(ModelName + \"ValRMSE\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(10):\n",
    "    GroundTruth = ValConc[i]\n",
    "    Prediction = model_aq(ValSpectra[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for metabolite in range(8):\n",
    "        per_err = 100*(GroundTruth[metabolite] - Prediction_cpu[0][metabolite]) / GroundTruth[metabolite]\n",
    "        APE.append(abs(per_err.cpu()))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "# Correct the MAPE for the example input with 4 of 8 metabolites present\n",
    "MAPEs[4] = (APEs[4][0] + APEs[4][1] + APEs[4][4] + APEs[4][6]) / 4\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
