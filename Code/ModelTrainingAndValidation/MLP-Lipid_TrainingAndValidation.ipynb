{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dependencies\n",
    "\n",
    "import numpy as np\n",
    "from os import path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nmrglue as ng\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "# Set default plot size\n",
    "plt.rcParams[\"figure.figsize\"] = (30,20)\n",
    "\n",
    "# Define number of epochs used later in training\n",
    "num_epochs = 50000\n",
    "\n",
    "# Name variable used for saving model metrics, name should reflect model used, dataset used, and other information such as # of epochs\n",
    "ModelName = \"MLP-Lipid_\" + str(num_epochs) +\"ep\"\n",
    "\n",
    "# Set the random seed\n",
    "os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/ModelPerformanceMetrics') #Save random seed used\n",
    "seed = 4\n",
    "torch.manual_seed(seed)\n",
    "np.save(ModelName + \"_Seed.npy\", seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training and testing datasets, validation datasets, and representative example spectra \n",
    "\n",
    "# Switch to directory containing datasets\n",
    "os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/GeneratedDataAndVariables')\n",
    "\n",
    "# Load training data and max value from testing and training datasets\n",
    "spectra_1 = np.load('LipidTrainTestVal_Spectra.npy')\n",
    "conc1_1 = np.load('LipidTrainTestVal_Spectra_Concentrations.npy')\n",
    "spectra = spectra_1[:50000]\n",
    "conc1 = conc1_1[:50000]\n",
    "# Select validation dataset\n",
    "spectraVal = spectra_1[50000:]\n",
    "concVal = conc1_1[50000:]\n",
    "\n",
    "# Load experimental lipid mixture validation spectra\n",
    "ExpValSpectra = np.load(\"ExperimentalLipidMixture_Spectra.npy\")\n",
    "ExpValConc = np.load(\"ExperimentalLipidMixture_Conc.npy\")\n",
    "\n",
    "# Load hepatic lipid extract validation spectra\n",
    "LiverSpectra = np.load(\"LiverLipid_Spectra.npy\")\n",
    "#LiverConc = np.load(\"____.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU for training.\n"
     ]
    }
   ],
   "source": [
    "## Prepare to switch data from CPU to GPU\n",
    "\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # A CUDA device object\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")           # A CPU object\n",
    "    print(\"CUDA is not available. Using CPU for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up data for testing and training\n",
    "\n",
    "# Split into testing and training data\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(spectra, conc1, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Tensorize and prepare datasets\n",
    "X_train = torch.tensor(X_train1).float()\n",
    "y_train = torch.tensor(y_train1).float()\n",
    "X_test = torch.tensor(X_test1).float()\n",
    "y_test = torch.tensor(y_test1).float()\n",
    "\n",
    "\n",
    "# Move the input data to the GPU device\n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "spectraVal = torch.tensor(spectraVal).float().to(device)  \n",
    "ExpValSpectra = torch.tensor(ExpValSpectra).float().to(device)   \n",
    "\n",
    "# Move the target data to the GPU device\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)\n",
    "concVal = torch.tensor(concVal).float().to(device)\n",
    "ExpValConc = torch.tensor(ExpValConc).float().to(device)\n",
    "\n",
    "# More data prep?\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "Test_datasets = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size = 128, shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(Test_datasets, batch_size = 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define NN model object, define some parameters, and instantiate model\n",
    "\n",
    "# Define some model & training parameters\n",
    "size_hidden1 = 200\n",
    "size_hidden2 = 18\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NMR_Model_Lipid(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(14000, size_hidden1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "    def forward(self, input):\n",
    "        return (self.lin2(self.relu1(self.lin1(input))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    best_test_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item() * inputs.size(0)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:  # The last number here denotes how often to print loss metrics in terms of epochs\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "                  f'Train Loss: {train_loss:.4f}, '\n",
    "                  f'Test Loss: {test_loss:.4f}')\n",
    "\n",
    "\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            # Save model when test loss improves\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, save_path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def train_or_load_model(model, train_loader, test_loader, num_epochs, save_path):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    is_model_trained = False  # Initialize flag\n",
    "\n",
    "    if os.path.isfile(save_path):\n",
    "        print(\"Loading pretrained model from {}\".format(save_path))\n",
    "        checkpoint = torch.load(save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer = optim.Adam(model.parameters())  \n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"No pretrained model found. Training from scratch.\")\n",
    "        #optimizer = optim.Adam(model.parameters())  \n",
    "        train_losses, test_losses = train_and_save_best_model(model, train_loader, test_loader, num_epochs, save_path)\n",
    "        is_model_trained = True  # Set flag to True after training\n",
    "        # Save losses per epoch\n",
    "        np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "        np.save(ModelName + \"_TestLoss.npy\", test_losses)\n",
    "    \n",
    "    return train_losses, test_losses, is_model_trained  # Return the losses and flag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pretrained model found. Training from scratch.\n",
      "Epoch [5/50000], Train Loss: 81804.5162, Test Loss: 16850.7872\n",
      "Epoch [10/50000], Train Loss: 28455.0080, Test Loss: 6268.4592\n",
      "Epoch [15/50000], Train Loss: 12901.3001, Test Loss: 3007.4297\n",
      "Epoch [20/50000], Train Loss: 9972.9523, Test Loss: 2333.6769\n",
      "Epoch [25/50000], Train Loss: 8637.5290, Test Loss: 1976.8993\n",
      "Epoch [30/50000], Train Loss: 7646.9918, Test Loss: 1780.1028\n",
      "Epoch [35/50000], Train Loss: 7116.2339, Test Loss: 1761.4325\n",
      "Epoch [40/50000], Train Loss: 6709.3976, Test Loss: 1590.4102\n",
      "Epoch [45/50000], Train Loss: 6510.9454, Test Loss: 1533.0719\n",
      "Epoch [50/50000], Train Loss: 6351.9016, Test Loss: 1503.7372\n",
      "Epoch [55/50000], Train Loss: 6186.7882, Test Loss: 1557.7331\n",
      "Epoch [60/50000], Train Loss: 6053.8472, Test Loss: 1457.4000\n",
      "Epoch [65/50000], Train Loss: 5843.4287, Test Loss: 1521.2942\n",
      "Epoch [70/50000], Train Loss: 5613.7450, Test Loss: 1418.0904\n",
      "Epoch [75/50000], Train Loss: 5548.6556, Test Loss: 1449.0491\n",
      "Epoch [80/50000], Train Loss: 5346.3684, Test Loss: 1379.2682\n",
      "Epoch [85/50000], Train Loss: 5154.7506, Test Loss: 1314.5669\n",
      "Epoch [90/50000], Train Loss: 4949.1417, Test Loss: 1339.3721\n",
      "Epoch [95/50000], Train Loss: 4896.1351, Test Loss: 1335.1854\n",
      "Epoch [100/50000], Train Loss: 4839.3391, Test Loss: 1356.3358\n",
      "Epoch [105/50000], Train Loss: 4758.1011, Test Loss: 1182.5204\n",
      "Epoch [110/50000], Train Loss: 4636.3906, Test Loss: 1197.7653\n",
      "Epoch [115/50000], Train Loss: 4565.3374, Test Loss: 1278.1112\n",
      "Epoch [120/50000], Train Loss: 4508.7230, Test Loss: 1391.1032\n",
      "Epoch [125/50000], Train Loss: 4468.5249, Test Loss: 1245.6892\n",
      "Epoch [130/50000], Train Loss: 4462.5213, Test Loss: 1155.4226\n",
      "Epoch [135/50000], Train Loss: 4465.7643, Test Loss: 1183.4273\n",
      "Epoch [140/50000], Train Loss: 4326.0458, Test Loss: 1168.6138\n",
      "Epoch [145/50000], Train Loss: 4382.4422, Test Loss: 1357.6289\n",
      "Epoch [150/50000], Train Loss: 4300.4868, Test Loss: 1149.4912\n",
      "Epoch [155/50000], Train Loss: 4307.3608, Test Loss: 1233.1072\n",
      "Epoch [160/50000], Train Loss: 4246.3001, Test Loss: 1309.6049\n",
      "Epoch [165/50000], Train Loss: 4299.3329, Test Loss: 1137.1862\n",
      "Epoch [170/50000], Train Loss: 4202.9839, Test Loss: 1311.3821\n",
      "Epoch [175/50000], Train Loss: 4158.6522, Test Loss: 1092.5545\n",
      "Epoch [180/50000], Train Loss: 4184.3264, Test Loss: 1104.6337\n",
      "Epoch [185/50000], Train Loss: 4125.9069, Test Loss: 1127.7853\n",
      "Epoch [190/50000], Train Loss: 4057.1098, Test Loss: 1151.6339\n",
      "Epoch [195/50000], Train Loss: 4017.2637, Test Loss: 1115.1260\n",
      "Epoch [200/50000], Train Loss: 3996.2223, Test Loss: 1096.4838\n",
      "Epoch [205/50000], Train Loss: 4028.1703, Test Loss: 1082.6579\n",
      "Epoch [210/50000], Train Loss: 3997.1039, Test Loss: 1122.2962\n",
      "Epoch [215/50000], Train Loss: 3948.9488, Test Loss: 1073.1273\n",
      "Epoch [220/50000], Train Loss: 3960.7174, Test Loss: 1082.6676\n",
      "Epoch [225/50000], Train Loss: 3913.8763, Test Loss: 1171.3457\n",
      "Epoch [230/50000], Train Loss: 3939.2569, Test Loss: 1158.1280\n",
      "Epoch [235/50000], Train Loss: 3912.9017, Test Loss: 1106.7795\n",
      "Epoch [240/50000], Train Loss: 3802.1436, Test Loss: 1066.9447\n",
      "Epoch [245/50000], Train Loss: 3783.4401, Test Loss: 1253.5060\n",
      "Epoch [250/50000], Train Loss: 3851.9167, Test Loss: 1041.9670\n",
      "Epoch [255/50000], Train Loss: 3769.0422, Test Loss: 1021.6518\n",
      "Epoch [260/50000], Train Loss: 3684.7874, Test Loss: 1036.3843\n",
      "Epoch [265/50000], Train Loss: 3603.0906, Test Loss: 996.0518\n",
      "Epoch [270/50000], Train Loss: 3607.2517, Test Loss: 1061.6710\n",
      "Epoch [275/50000], Train Loss: 3567.0967, Test Loss: 1007.6264\n",
      "Epoch [280/50000], Train Loss: 3502.5389, Test Loss: 1019.3723\n",
      "Epoch [285/50000], Train Loss: 3541.8336, Test Loss: 1039.4803\n",
      "Epoch [290/50000], Train Loss: 3483.3105, Test Loss: 1039.8515\n",
      "Epoch [295/50000], Train Loss: 3460.4236, Test Loss: 1064.8484\n",
      "Epoch [300/50000], Train Loss: 3414.0535, Test Loss: 1083.9541\n",
      "Epoch [305/50000], Train Loss: 3383.5885, Test Loss: 949.0168\n",
      "Epoch [310/50000], Train Loss: 3373.5621, Test Loss: 1023.1509\n",
      "Epoch [315/50000], Train Loss: 3370.9917, Test Loss: 984.7112\n",
      "Epoch [320/50000], Train Loss: 3377.3887, Test Loss: 960.0323\n",
      "Epoch [325/50000], Train Loss: 3368.3260, Test Loss: 1031.7806\n",
      "Epoch [330/50000], Train Loss: 3281.2775, Test Loss: 973.3227\n",
      "Epoch [335/50000], Train Loss: 3385.3166, Test Loss: 974.1287\n",
      "Epoch [340/50000], Train Loss: 3320.2241, Test Loss: 965.2480\n",
      "Epoch [345/50000], Train Loss: 3265.8667, Test Loss: 992.5761\n",
      "Epoch [350/50000], Train Loss: 3275.0343, Test Loss: 975.0026\n",
      "Epoch [355/50000], Train Loss: 3212.5454, Test Loss: 1076.3902\n",
      "Epoch [360/50000], Train Loss: 3212.1170, Test Loss: 1084.9162\n",
      "Epoch [365/50000], Train Loss: 3183.2283, Test Loss: 1006.1872\n",
      "Epoch [370/50000], Train Loss: 3170.0277, Test Loss: 979.2250\n",
      "Epoch [375/50000], Train Loss: 3185.6144, Test Loss: 947.2369\n",
      "Epoch [380/50000], Train Loss: 3157.9080, Test Loss: 932.8029\n",
      "Epoch [385/50000], Train Loss: 3145.9274, Test Loss: 910.1735\n",
      "Epoch [390/50000], Train Loss: 3063.7007, Test Loss: 928.3068\n",
      "Epoch [395/50000], Train Loss: 3094.3682, Test Loss: 920.2504\n",
      "Epoch [400/50000], Train Loss: 3081.7497, Test Loss: 924.9527\n",
      "Epoch [405/50000], Train Loss: 3061.1099, Test Loss: 907.8372\n",
      "Epoch [410/50000], Train Loss: 3053.0786, Test Loss: 916.3544\n",
      "Epoch [415/50000], Train Loss: 2985.3289, Test Loss: 911.1123\n",
      "Epoch [420/50000], Train Loss: 2958.1191, Test Loss: 928.7298\n",
      "Epoch [425/50000], Train Loss: 2961.4128, Test Loss: 1022.4696\n",
      "Epoch [430/50000], Train Loss: 2940.3392, Test Loss: 963.5650\n",
      "Epoch [435/50000], Train Loss: 2884.5844, Test Loss: 871.7447\n",
      "Epoch [440/50000], Train Loss: 2888.0762, Test Loss: 928.0521\n",
      "Epoch [445/50000], Train Loss: 2826.6690, Test Loss: 889.1074\n",
      "Epoch [450/50000], Train Loss: 2854.6605, Test Loss: 928.3377\n",
      "Epoch [455/50000], Train Loss: 2797.5367, Test Loss: 886.7943\n",
      "Epoch [460/50000], Train Loss: 2842.9051, Test Loss: 878.9714\n",
      "Epoch [465/50000], Train Loss: 2779.4199, Test Loss: 867.6142\n",
      "Epoch [470/50000], Train Loss: 2752.3256, Test Loss: 835.8867\n",
      "Epoch [475/50000], Train Loss: 2758.2734, Test Loss: 851.8597\n",
      "Epoch [480/50000], Train Loss: 2753.0415, Test Loss: 930.5215\n",
      "Epoch [485/50000], Train Loss: 2709.6405, Test Loss: 874.0075\n",
      "Epoch [490/50000], Train Loss: 2675.2348, Test Loss: 877.8916\n",
      "Epoch [495/50000], Train Loss: 2706.7108, Test Loss: 832.8521\n",
      "Epoch [500/50000], Train Loss: 2638.1174, Test Loss: 968.8014\n",
      "Epoch [505/50000], Train Loss: 2651.8258, Test Loss: 855.8405\n",
      "Epoch [510/50000], Train Loss: 2705.0786, Test Loss: 869.2316\n",
      "Epoch [515/50000], Train Loss: 2627.3931, Test Loss: 831.0615\n",
      "Epoch [520/50000], Train Loss: 2633.0280, Test Loss: 869.7389\n",
      "Epoch [525/50000], Train Loss: 2611.0868, Test Loss: 890.1027\n",
      "Epoch [530/50000], Train Loss: 2552.3958, Test Loss: 888.7553\n",
      "Epoch [535/50000], Train Loss: 2562.5715, Test Loss: 873.2082\n",
      "Epoch [540/50000], Train Loss: 2586.8278, Test Loss: 801.8405\n",
      "Epoch [545/50000], Train Loss: 2505.3793, Test Loss: 835.1542\n",
      "Epoch [550/50000], Train Loss: 2518.3120, Test Loss: 799.4673\n",
      "Epoch [555/50000], Train Loss: 2492.9024, Test Loss: 800.5990\n",
      "Epoch [560/50000], Train Loss: 2479.3934, Test Loss: 805.7702\n",
      "Epoch [565/50000], Train Loss: 2494.7463, Test Loss: 773.7822\n",
      "Epoch [570/50000], Train Loss: 2459.7381, Test Loss: 777.5105\n",
      "Epoch [575/50000], Train Loss: 2462.9935, Test Loss: 828.6856\n",
      "Epoch [580/50000], Train Loss: 2380.9277, Test Loss: 811.4211\n",
      "Epoch [585/50000], Train Loss: 2384.6707, Test Loss: 800.0002\n",
      "Epoch [590/50000], Train Loss: 2403.5234, Test Loss: 919.7520\n",
      "Epoch [595/50000], Train Loss: 2384.5899, Test Loss: 766.0957\n",
      "Epoch [600/50000], Train Loss: 2377.6508, Test Loss: 749.6992\n",
      "Epoch [605/50000], Train Loss: 2326.4484, Test Loss: 773.1861\n",
      "Epoch [610/50000], Train Loss: 2315.1953, Test Loss: 764.3480\n",
      "Epoch [615/50000], Train Loss: 2290.6155, Test Loss: 833.9744\n",
      "Epoch [620/50000], Train Loss: 2278.3357, Test Loss: 836.0363\n",
      "Epoch [625/50000], Train Loss: 2255.0307, Test Loss: 789.9126\n",
      "Epoch [630/50000], Train Loss: 2224.7007, Test Loss: 833.1045\n",
      "Epoch [635/50000], Train Loss: 2251.3149, Test Loss: 756.0380\n",
      "Epoch [640/50000], Train Loss: 2162.5598, Test Loss: 738.1041\n",
      "Epoch [645/50000], Train Loss: 2149.2716, Test Loss: 784.2878\n",
      "Epoch [650/50000], Train Loss: 2169.5105, Test Loss: 740.0325\n",
      "Epoch [655/50000], Train Loss: 2144.0725, Test Loss: 700.8220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [660/50000], Train Loss: 2071.5007, Test Loss: 723.3160\n",
      "Epoch [665/50000], Train Loss: 2123.7506, Test Loss: 727.6304\n",
      "Epoch [670/50000], Train Loss: 2099.6772, Test Loss: 705.0286\n",
      "Epoch [675/50000], Train Loss: 1997.3057, Test Loss: 716.3282\n",
      "Epoch [680/50000], Train Loss: 2022.8780, Test Loss: 699.9739\n",
      "Epoch [685/50000], Train Loss: 2002.6112, Test Loss: 669.7305\n",
      "Epoch [690/50000], Train Loss: 2001.0309, Test Loss: 687.5374\n",
      "Epoch [695/50000], Train Loss: 1967.0403, Test Loss: 655.3052\n",
      "Epoch [700/50000], Train Loss: 1948.7293, Test Loss: 657.7539\n",
      "Epoch [705/50000], Train Loss: 1886.8882, Test Loss: 667.1796\n",
      "Epoch [710/50000], Train Loss: 1850.0766, Test Loss: 664.4703\n",
      "Epoch [715/50000], Train Loss: 1843.3731, Test Loss: 651.0299\n",
      "Epoch [720/50000], Train Loss: 1843.7327, Test Loss: 633.8499\n",
      "Epoch [725/50000], Train Loss: 1806.0883, Test Loss: 637.2542\n",
      "Epoch [730/50000], Train Loss: 1798.5910, Test Loss: 638.5854\n",
      "Epoch [735/50000], Train Loss: 1751.7418, Test Loss: 662.9299\n",
      "Epoch [740/50000], Train Loss: 1787.5029, Test Loss: 621.7748\n",
      "Epoch [745/50000], Train Loss: 1726.9393, Test Loss: 614.6492\n",
      "Epoch [750/50000], Train Loss: 1673.3821, Test Loss: 599.9101\n",
      "Epoch [755/50000], Train Loss: 1691.2197, Test Loss: 778.7690\n",
      "Epoch [760/50000], Train Loss: 1666.7008, Test Loss: 691.0364\n",
      "Epoch [765/50000], Train Loss: 1637.5346, Test Loss: 568.2252\n",
      "Epoch [770/50000], Train Loss: 1624.6555, Test Loss: 589.3459\n",
      "Epoch [775/50000], Train Loss: 1600.2903, Test Loss: 574.2862\n",
      "Epoch [780/50000], Train Loss: 1609.7239, Test Loss: 579.6146\n",
      "Epoch [785/50000], Train Loss: 1584.1692, Test Loss: 559.0538\n",
      "Epoch [790/50000], Train Loss: 1593.2887, Test Loss: 545.5922\n",
      "Epoch [795/50000], Train Loss: 1509.6652, Test Loss: 639.2071\n",
      "Epoch [800/50000], Train Loss: 1477.7469, Test Loss: 550.2365\n",
      "Epoch [805/50000], Train Loss: 1470.8417, Test Loss: 538.1203\n",
      "Epoch [810/50000], Train Loss: 1507.2946, Test Loss: 541.9681\n",
      "Epoch [815/50000], Train Loss: 1396.2722, Test Loss: 515.0772\n",
      "Epoch [820/50000], Train Loss: 1438.0697, Test Loss: 515.1224\n",
      "Epoch [825/50000], Train Loss: 1375.8279, Test Loss: 549.1386\n",
      "Epoch [830/50000], Train Loss: 1419.3433, Test Loss: 505.2555\n",
      "Epoch [835/50000], Train Loss: 1365.8964, Test Loss: 506.6211\n",
      "Epoch [840/50000], Train Loss: 1356.8779, Test Loss: 485.0949\n",
      "Epoch [845/50000], Train Loss: 1322.5286, Test Loss: 484.6752\n",
      "Epoch [850/50000], Train Loss: 1295.2395, Test Loss: 465.3511\n",
      "Epoch [855/50000], Train Loss: 1287.3871, Test Loss: 544.7423\n",
      "Epoch [860/50000], Train Loss: 1273.3321, Test Loss: 580.4601\n",
      "Epoch [865/50000], Train Loss: 1282.2199, Test Loss: 478.5550\n",
      "Epoch [870/50000], Train Loss: 1289.0476, Test Loss: 460.9225\n",
      "Epoch [875/50000], Train Loss: 1249.0212, Test Loss: 439.2066\n",
      "Epoch [880/50000], Train Loss: 1201.1171, Test Loss: 501.7498\n",
      "Epoch [885/50000], Train Loss: 1206.5731, Test Loss: 491.7880\n",
      "Epoch [890/50000], Train Loss: 1226.3159, Test Loss: 416.0973\n",
      "Epoch [895/50000], Train Loss: 1168.9185, Test Loss: 433.8721\n",
      "Epoch [900/50000], Train Loss: 1179.3417, Test Loss: 416.8143\n",
      "Epoch [905/50000], Train Loss: 1155.8886, Test Loss: 555.6923\n",
      "Epoch [910/50000], Train Loss: 1133.3567, Test Loss: 409.6847\n",
      "Epoch [915/50000], Train Loss: 1148.4391, Test Loss: 435.0884\n",
      "Epoch [920/50000], Train Loss: 1141.0684, Test Loss: 438.7501\n",
      "Epoch [925/50000], Train Loss: 1085.3991, Test Loss: 411.2256\n",
      "Epoch [930/50000], Train Loss: 1088.0695, Test Loss: 465.2178\n",
      "Epoch [935/50000], Train Loss: 1063.0158, Test Loss: 405.2070\n",
      "Epoch [940/50000], Train Loss: 1051.5772, Test Loss: 406.7611\n",
      "Epoch [945/50000], Train Loss: 1018.6686, Test Loss: 400.3709\n",
      "Epoch [950/50000], Train Loss: 1032.5776, Test Loss: 393.9039\n",
      "Epoch [955/50000], Train Loss: 1014.7417, Test Loss: 460.0147\n",
      "Epoch [960/50000], Train Loss: 1008.1874, Test Loss: 392.0003\n",
      "Epoch [965/50000], Train Loss: 997.3197, Test Loss: 448.5969\n",
      "Epoch [970/50000], Train Loss: 1157.1230, Test Loss: 373.7180\n",
      "Epoch [975/50000], Train Loss: 986.4785, Test Loss: 358.6323\n",
      "Epoch [980/50000], Train Loss: 1002.4037, Test Loss: 398.5100\n",
      "Epoch [985/50000], Train Loss: 952.9214, Test Loss: 351.9064\n",
      "Epoch [990/50000], Train Loss: 987.6441, Test Loss: 349.6705\n",
      "Epoch [995/50000], Train Loss: 1049.6489, Test Loss: 532.0432\n",
      "Epoch [1000/50000], Train Loss: 931.3217, Test Loss: 349.3252\n",
      "Epoch [1005/50000], Train Loss: 930.3068, Test Loss: 395.3811\n",
      "Epoch [1010/50000], Train Loss: 967.5996, Test Loss: 1016.6438\n",
      "Epoch [1015/50000], Train Loss: 949.0328, Test Loss: 367.1284\n",
      "Epoch [1020/50000], Train Loss: 910.2173, Test Loss: 355.5446\n",
      "Epoch [1025/50000], Train Loss: 923.2054, Test Loss: 362.9888\n",
      "Epoch [1030/50000], Train Loss: 886.6075, Test Loss: 334.6023\n",
      "Epoch [1035/50000], Train Loss: 867.9703, Test Loss: 331.7648\n",
      "Epoch [1040/50000], Train Loss: 885.0615, Test Loss: 403.5818\n",
      "Epoch [1045/50000], Train Loss: 860.8386, Test Loss: 351.5386\n",
      "Epoch [1050/50000], Train Loss: 851.8376, Test Loss: 398.4641\n",
      "Epoch [1055/50000], Train Loss: 865.1417, Test Loss: 312.8659\n",
      "Epoch [1060/50000], Train Loss: 859.8601, Test Loss: 336.1328\n",
      "Epoch [1065/50000], Train Loss: 841.1380, Test Loss: 308.3457\n",
      "Epoch [1070/50000], Train Loss: 826.9542, Test Loss: 302.2431\n",
      "Epoch [1075/50000], Train Loss: 805.5559, Test Loss: 301.2169\n",
      "Epoch [1080/50000], Train Loss: 783.3620, Test Loss: 293.7126\n",
      "Epoch [1085/50000], Train Loss: 801.4733, Test Loss: 464.9108\n",
      "Epoch [1090/50000], Train Loss: 771.3377, Test Loss: 308.7586\n",
      "Epoch [1095/50000], Train Loss: 837.9869, Test Loss: 313.1348\n",
      "Epoch [1100/50000], Train Loss: 764.0839, Test Loss: 313.9445\n",
      "Epoch [1105/50000], Train Loss: 780.7616, Test Loss: 319.1230\n",
      "Epoch [1110/50000], Train Loss: 799.6183, Test Loss: 314.8043\n",
      "Epoch [1115/50000], Train Loss: 746.1798, Test Loss: 310.9652\n",
      "Epoch [1120/50000], Train Loss: 726.3541, Test Loss: 317.3804\n",
      "Epoch [1125/50000], Train Loss: 772.2743, Test Loss: 278.4285\n",
      "Epoch [1130/50000], Train Loss: 752.5278, Test Loss: 296.2861\n",
      "Epoch [1135/50000], Train Loss: 692.5479, Test Loss: 293.4163\n",
      "Epoch [1140/50000], Train Loss: 746.3434, Test Loss: 277.9780\n",
      "Epoch [1145/50000], Train Loss: 738.1143, Test Loss: 287.6738\n",
      "Epoch [1150/50000], Train Loss: 755.7821, Test Loss: 284.9980\n",
      "Epoch [1155/50000], Train Loss: 731.6396, Test Loss: 445.5550\n",
      "Epoch [1160/50000], Train Loss: 769.3211, Test Loss: 271.6382\n",
      "Epoch [1165/50000], Train Loss: 747.8031, Test Loss: 274.3325\n",
      "Epoch [1170/50000], Train Loss: 702.9036, Test Loss: 287.8118\n",
      "Epoch [1175/50000], Train Loss: 694.5110, Test Loss: 281.4944\n",
      "Epoch [1180/50000], Train Loss: 688.7912, Test Loss: 258.7723\n",
      "Epoch [1185/50000], Train Loss: 717.5056, Test Loss: 252.7862\n",
      "Epoch [1190/50000], Train Loss: 720.3255, Test Loss: 323.9107\n",
      "Epoch [1195/50000], Train Loss: 707.7927, Test Loss: 257.2277\n",
      "Epoch [1200/50000], Train Loss: 651.5866, Test Loss: 270.5058\n",
      "Epoch [1205/50000], Train Loss: 666.8228, Test Loss: 343.2365\n",
      "Epoch [1210/50000], Train Loss: 653.9435, Test Loss: 245.9957\n",
      "Epoch [1215/50000], Train Loss: 659.5866, Test Loss: 306.7112\n",
      "Epoch [1220/50000], Train Loss: 682.4112, Test Loss: 353.4824\n",
      "Epoch [1225/50000], Train Loss: 669.7926, Test Loss: 270.3342\n",
      "Epoch [1230/50000], Train Loss: 607.5377, Test Loss: 275.2194\n",
      "Epoch [1235/50000], Train Loss: 639.7687, Test Loss: 267.3597\n",
      "Epoch [1240/50000], Train Loss: 603.6116, Test Loss: 282.8546\n",
      "Epoch [1245/50000], Train Loss: 662.7357, Test Loss: 278.9783\n",
      "Epoch [1250/50000], Train Loss: 809.3888, Test Loss: 261.6252\n",
      "Epoch [1255/50000], Train Loss: 670.6010, Test Loss: 233.0395\n",
      "Epoch [1260/50000], Train Loss: 621.9852, Test Loss: 231.4303\n",
      "Epoch [1265/50000], Train Loss: 618.5805, Test Loss: 237.3003\n",
      "Epoch [1270/50000], Train Loss: 591.1448, Test Loss: 229.4297\n",
      "Epoch [1275/50000], Train Loss: 597.3597, Test Loss: 278.5114\n",
      "Epoch [1280/50000], Train Loss: 615.7939, Test Loss: 233.5446\n",
      "Epoch [1285/50000], Train Loss: 622.7052, Test Loss: 240.3274\n",
      "Epoch [1290/50000], Train Loss: 604.1751, Test Loss: 234.4487\n",
      "Epoch [1295/50000], Train Loss: 577.3254, Test Loss: 264.9652\n",
      "Epoch [1300/50000], Train Loss: 589.1456, Test Loss: 249.3627\n",
      "Epoch [1305/50000], Train Loss: 589.5820, Test Loss: 247.9341\n",
      "Epoch [1310/50000], Train Loss: 633.4429, Test Loss: 264.1238\n",
      "Epoch [1315/50000], Train Loss: 572.7977, Test Loss: 223.7908\n",
      "Epoch [1320/50000], Train Loss: 569.3139, Test Loss: 261.2497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1325/50000], Train Loss: 713.4962, Test Loss: 1163.4988\n",
      "Epoch [1330/50000], Train Loss: 569.5069, Test Loss: 211.9466\n",
      "Epoch [1335/50000], Train Loss: 613.9553, Test Loss: 239.2079\n",
      "Epoch [1340/50000], Train Loss: 600.0359, Test Loss: 249.6871\n",
      "Epoch [1345/50000], Train Loss: 560.3228, Test Loss: 242.9990\n",
      "Epoch [1350/50000], Train Loss: 579.1544, Test Loss: 244.7997\n",
      "Epoch [1355/50000], Train Loss: 540.7605, Test Loss: 228.7995\n",
      "Epoch [1360/50000], Train Loss: 667.1557, Test Loss: 210.9728\n",
      "Epoch [1365/50000], Train Loss: 609.3060, Test Loss: 224.8927\n",
      "Epoch [1370/50000], Train Loss: 538.6145, Test Loss: 243.4965\n",
      "Epoch [1375/50000], Train Loss: 547.4244, Test Loss: 233.1134\n",
      "Epoch [1380/50000], Train Loss: 945.9956, Test Loss: 219.2427\n",
      "Epoch [1385/50000], Train Loss: 546.4855, Test Loss: 218.8598\n",
      "Epoch [1390/50000], Train Loss: 606.4464, Test Loss: 226.9676\n",
      "Epoch [1395/50000], Train Loss: 573.2668, Test Loss: 212.1380\n",
      "Epoch [1400/50000], Train Loss: 578.8543, Test Loss: 216.3453\n",
      "Epoch [1405/50000], Train Loss: 555.1142, Test Loss: 221.1102\n",
      "Epoch [1410/50000], Train Loss: 624.4557, Test Loss: 246.3497\n",
      "Epoch [1415/50000], Train Loss: 515.2842, Test Loss: 264.0338\n",
      "Epoch [1420/50000], Train Loss: 507.8868, Test Loss: 230.1552\n",
      "Epoch [1425/50000], Train Loss: 532.5908, Test Loss: 229.2164\n",
      "Epoch [1430/50000], Train Loss: 532.9078, Test Loss: 229.0716\n",
      "Epoch [1435/50000], Train Loss: 510.8748, Test Loss: 215.3366\n",
      "Epoch [1440/50000], Train Loss: 532.4997, Test Loss: 207.2108\n",
      "Epoch [1445/50000], Train Loss: 487.3559, Test Loss: 236.7677\n",
      "Epoch [1450/50000], Train Loss: 491.8300, Test Loss: 276.4968\n",
      "Epoch [1455/50000], Train Loss: 516.7341, Test Loss: 198.4511\n",
      "Epoch [1460/50000], Train Loss: 489.3576, Test Loss: 201.1655\n",
      "Epoch [1465/50000], Train Loss: 482.3582, Test Loss: 240.0243\n",
      "Epoch [1470/50000], Train Loss: 540.9082, Test Loss: 234.4079\n",
      "Epoch [1475/50000], Train Loss: 492.6027, Test Loss: 202.0862\n",
      "Epoch [1480/50000], Train Loss: 471.0268, Test Loss: 202.5986\n",
      "Epoch [1485/50000], Train Loss: 511.0137, Test Loss: 200.7862\n",
      "Epoch [1490/50000], Train Loss: 506.6136, Test Loss: 233.1665\n",
      "Epoch [1495/50000], Train Loss: 476.3284, Test Loss: 215.8182\n",
      "Epoch [1500/50000], Train Loss: 506.9382, Test Loss: 195.2176\n",
      "Epoch [1505/50000], Train Loss: 526.1561, Test Loss: 396.1524\n",
      "Epoch [1510/50000], Train Loss: 499.5607, Test Loss: 194.0558\n",
      "Epoch [1515/50000], Train Loss: 530.3398, Test Loss: 194.0465\n",
      "Epoch [1520/50000], Train Loss: 467.5419, Test Loss: 210.6705\n",
      "Epoch [1525/50000], Train Loss: 552.2185, Test Loss: 220.7386\n",
      "Epoch [1530/50000], Train Loss: 557.2696, Test Loss: 199.4487\n",
      "Epoch [1535/50000], Train Loss: 789.3540, Test Loss: 2052.4990\n",
      "Epoch [1540/50000], Train Loss: 480.0197, Test Loss: 198.5815\n",
      "Epoch [1545/50000], Train Loss: 494.8191, Test Loss: 203.3932\n",
      "Epoch [1550/50000], Train Loss: 513.7776, Test Loss: 191.8298\n",
      "Epoch [1555/50000], Train Loss: 1049.4651, Test Loss: 185.4367\n",
      "Epoch [1560/50000], Train Loss: 476.3587, Test Loss: 188.1456\n",
      "Epoch [1565/50000], Train Loss: 508.0837, Test Loss: 188.3695\n",
      "Epoch [1570/50000], Train Loss: 461.0954, Test Loss: 220.7008\n",
      "Epoch [1575/50000], Train Loss: 483.3116, Test Loss: 213.0423\n",
      "Epoch [1580/50000], Train Loss: 464.3100, Test Loss: 184.6674\n",
      "Epoch [1585/50000], Train Loss: 458.8632, Test Loss: 190.9120\n",
      "Epoch [1590/50000], Train Loss: 454.8169, Test Loss: 182.0217\n",
      "Epoch [1595/50000], Train Loss: 480.3490, Test Loss: 188.3916\n",
      "Epoch [1600/50000], Train Loss: 500.9237, Test Loss: 194.5310\n",
      "Epoch [1605/50000], Train Loss: 470.6576, Test Loss: 183.8419\n",
      "Epoch [1610/50000], Train Loss: 438.5456, Test Loss: 228.2671\n",
      "Epoch [1615/50000], Train Loss: 459.8631, Test Loss: 194.0531\n",
      "Epoch [1620/50000], Train Loss: 471.8823, Test Loss: 209.6218\n",
      "Epoch [1625/50000], Train Loss: 433.0723, Test Loss: 204.3773\n",
      "Epoch [1630/50000], Train Loss: 464.6819, Test Loss: 192.7943\n",
      "Epoch [1635/50000], Train Loss: 462.4826, Test Loss: 226.3236\n",
      "Epoch [1640/50000], Train Loss: 471.7039, Test Loss: 187.7105\n",
      "Epoch [1645/50000], Train Loss: 422.6813, Test Loss: 179.1968\n",
      "Epoch [1650/50000], Train Loss: 443.1525, Test Loss: 204.6803\n",
      "Epoch [1655/50000], Train Loss: 427.0585, Test Loss: 187.7109\n",
      "Epoch [1660/50000], Train Loss: 600.9941, Test Loss: 172.8667\n",
      "Epoch [1665/50000], Train Loss: 461.2119, Test Loss: 229.4609\n",
      "Epoch [1670/50000], Train Loss: 469.7518, Test Loss: 191.7545\n",
      "Epoch [1675/50000], Train Loss: 612.7325, Test Loss: 195.3424\n",
      "Epoch [1680/50000], Train Loss: 435.1760, Test Loss: 252.4944\n",
      "Epoch [1685/50000], Train Loss: 430.5151, Test Loss: 213.7904\n",
      "Epoch [1690/50000], Train Loss: 409.3554, Test Loss: 191.3402\n",
      "Epoch [1695/50000], Train Loss: 454.3001, Test Loss: 210.5893\n",
      "Epoch [1700/50000], Train Loss: 491.3731, Test Loss: 193.8289\n",
      "Epoch [1705/50000], Train Loss: 476.0686, Test Loss: 180.6355\n",
      "Epoch [1710/50000], Train Loss: 398.5572, Test Loss: 188.1037\n",
      "Epoch [1715/50000], Train Loss: 437.0171, Test Loss: 199.0954\n",
      "Epoch [1720/50000], Train Loss: 425.2594, Test Loss: 186.7901\n",
      "Epoch [1725/50000], Train Loss: 449.5440, Test Loss: 182.7321\n",
      "Epoch [1730/50000], Train Loss: 416.3967, Test Loss: 198.9078\n",
      "Epoch [1735/50000], Train Loss: 532.5810, Test Loss: 174.6945\n",
      "Epoch [1740/50000], Train Loss: 390.4137, Test Loss: 178.3357\n",
      "Epoch [1745/50000], Train Loss: 413.0235, Test Loss: 173.5936\n",
      "Epoch [1750/50000], Train Loss: 718.3166, Test Loss: 233.4442\n",
      "Epoch [1755/50000], Train Loss: 411.8950, Test Loss: 174.8266\n",
      "Epoch [1760/50000], Train Loss: 427.6900, Test Loss: 185.0470\n",
      "Epoch [1765/50000], Train Loss: 417.9731, Test Loss: 188.9409\n",
      "Epoch [1770/50000], Train Loss: 410.7300, Test Loss: 269.7148\n",
      "Epoch [1775/50000], Train Loss: 559.3758, Test Loss: 173.8483\n",
      "Epoch [1780/50000], Train Loss: 382.5681, Test Loss: 211.3058\n",
      "Epoch [1785/50000], Train Loss: 399.6425, Test Loss: 188.1344\n",
      "Epoch [1790/50000], Train Loss: 420.0697, Test Loss: 171.6875\n",
      "Epoch [1795/50000], Train Loss: 661.3509, Test Loss: 264.2330\n",
      "Epoch [1800/50000], Train Loss: 401.3174, Test Loss: 181.2791\n",
      "Epoch [1805/50000], Train Loss: 370.0655, Test Loss: 174.4160\n",
      "Epoch [1810/50000], Train Loss: 601.3616, Test Loss: 179.5419\n",
      "Epoch [1815/50000], Train Loss: 383.0489, Test Loss: 164.1729\n",
      "Epoch [1820/50000], Train Loss: 585.7581, Test Loss: 233.2953\n",
      "Epoch [1825/50000], Train Loss: 383.8552, Test Loss: 207.3264\n",
      "Epoch [1830/50000], Train Loss: 390.1573, Test Loss: 177.5173\n",
      "Epoch [1835/50000], Train Loss: 403.3507, Test Loss: 164.0812\n",
      "Epoch [1840/50000], Train Loss: 390.4450, Test Loss: 170.1000\n",
      "Epoch [1845/50000], Train Loss: 380.1403, Test Loss: 164.6841\n",
      "Epoch [1850/50000], Train Loss: 422.4820, Test Loss: 198.3106\n",
      "Epoch [1855/50000], Train Loss: 361.4112, Test Loss: 187.0867\n",
      "Epoch [1860/50000], Train Loss: 351.5982, Test Loss: 160.7360\n",
      "Epoch [1865/50000], Train Loss: 392.9117, Test Loss: 162.9121\n",
      "Epoch [1870/50000], Train Loss: 387.2412, Test Loss: 170.2379\n",
      "Epoch [1875/50000], Train Loss: 371.5188, Test Loss: 162.7190\n",
      "Epoch [1880/50000], Train Loss: 399.7958, Test Loss: 164.6502\n",
      "Epoch [1885/50000], Train Loss: 497.0713, Test Loss: 212.1986\n",
      "Epoch [1890/50000], Train Loss: 394.3494, Test Loss: 168.8066\n",
      "Epoch [1895/50000], Train Loss: 374.0769, Test Loss: 164.5778\n",
      "Epoch [1900/50000], Train Loss: 432.9953, Test Loss: 171.2388\n",
      "Epoch [1905/50000], Train Loss: 373.7831, Test Loss: 150.6734\n",
      "Epoch [1910/50000], Train Loss: 604.9519, Test Loss: 160.9842\n",
      "Epoch [1915/50000], Train Loss: 359.8486, Test Loss: 176.1629\n",
      "Epoch [1920/50000], Train Loss: 416.7556, Test Loss: 176.9479\n",
      "Epoch [1925/50000], Train Loss: 499.5061, Test Loss: 149.5411\n",
      "Epoch [1930/50000], Train Loss: 398.3450, Test Loss: 192.6628\n",
      "Epoch [1935/50000], Train Loss: 398.5462, Test Loss: 144.5893\n",
      "Epoch [1940/50000], Train Loss: 348.5924, Test Loss: 204.1335\n",
      "Epoch [1945/50000], Train Loss: 425.4544, Test Loss: 186.2903\n",
      "Epoch [1950/50000], Train Loss: 355.6642, Test Loss: 166.2146\n",
      "Epoch [1955/50000], Train Loss: 377.6995, Test Loss: 156.6062\n",
      "Epoch [1960/50000], Train Loss: 354.6624, Test Loss: 170.9873\n",
      "Epoch [1965/50000], Train Loss: 343.4422, Test Loss: 160.8519\n",
      "Epoch [1970/50000], Train Loss: 356.4410, Test Loss: 160.6536\n",
      "Epoch [1975/50000], Train Loss: 340.3364, Test Loss: 171.5156\n",
      "Epoch [1980/50000], Train Loss: 457.9418, Test Loss: 145.2720\n",
      "Epoch [1985/50000], Train Loss: 369.2803, Test Loss: 207.4047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1990/50000], Train Loss: 368.8336, Test Loss: 202.0444\n",
      "Epoch [1995/50000], Train Loss: 352.4051, Test Loss: 171.5566\n",
      "Epoch [2000/50000], Train Loss: 363.4564, Test Loss: 162.1349\n",
      "Epoch [2005/50000], Train Loss: 360.7616, Test Loss: 163.7889\n",
      "Epoch [2010/50000], Train Loss: 323.2005, Test Loss: 180.0361\n",
      "Epoch [2015/50000], Train Loss: 342.7874, Test Loss: 151.5593\n",
      "Epoch [2020/50000], Train Loss: 371.7499, Test Loss: 152.3226\n",
      "Epoch [2025/50000], Train Loss: 323.9653, Test Loss: 156.9727\n",
      "Epoch [2030/50000], Train Loss: 359.0849, Test Loss: 146.5511\n",
      "Epoch [2035/50000], Train Loss: 337.3079, Test Loss: 172.3406\n",
      "Epoch [2040/50000], Train Loss: 445.5844, Test Loss: 146.3151\n",
      "Epoch [2045/50000], Train Loss: 382.0163, Test Loss: 158.2148\n",
      "Epoch [2050/50000], Train Loss: 397.0763, Test Loss: 165.5895\n",
      "Epoch [2055/50000], Train Loss: 338.1855, Test Loss: 199.9060\n",
      "Epoch [2060/50000], Train Loss: 351.6463, Test Loss: 154.2610\n",
      "Epoch [2065/50000], Train Loss: 346.3221, Test Loss: 149.0328\n",
      "Epoch [2070/50000], Train Loss: 467.4633, Test Loss: 151.9637\n",
      "Epoch [2075/50000], Train Loss: 336.6210, Test Loss: 189.0609\n",
      "Epoch [2080/50000], Train Loss: 320.9372, Test Loss: 143.4211\n",
      "Epoch [2085/50000], Train Loss: 341.3889, Test Loss: 142.6642\n",
      "Epoch [2090/50000], Train Loss: 335.4019, Test Loss: 140.8121\n",
      "Epoch [2095/50000], Train Loss: 339.4094, Test Loss: 158.1842\n",
      "Epoch [2100/50000], Train Loss: 710.2862, Test Loss: 202.0410\n",
      "Epoch [2105/50000], Train Loss: 340.4953, Test Loss: 200.1854\n",
      "Epoch [2110/50000], Train Loss: 334.4478, Test Loss: 149.3367\n",
      "Epoch [2115/50000], Train Loss: 345.4964, Test Loss: 149.5701\n",
      "Epoch [2120/50000], Train Loss: 314.4993, Test Loss: 143.2171\n",
      "Epoch [2125/50000], Train Loss: 337.6434, Test Loss: 144.2176\n",
      "Epoch [2130/50000], Train Loss: 323.4545, Test Loss: 155.1933\n",
      "Epoch [2135/50000], Train Loss: 331.5920, Test Loss: 149.5923\n",
      "Epoch [2140/50000], Train Loss: 321.0831, Test Loss: 148.4429\n",
      "Epoch [2145/50000], Train Loss: 310.5477, Test Loss: 172.3651\n",
      "Epoch [2150/50000], Train Loss: 363.5017, Test Loss: 142.1431\n",
      "Epoch [2155/50000], Train Loss: 312.4750, Test Loss: 136.5947\n",
      "Epoch [2160/50000], Train Loss: 312.6320, Test Loss: 147.3217\n",
      "Epoch [2165/50000], Train Loss: 488.8672, Test Loss: 140.6472\n",
      "Epoch [2170/50000], Train Loss: 500.1617, Test Loss: 151.2264\n",
      "Epoch [2175/50000], Train Loss: 388.5002, Test Loss: 218.5022\n",
      "Epoch [2180/50000], Train Loss: 309.5264, Test Loss: 135.8873\n",
      "Epoch [2185/50000], Train Loss: 295.8346, Test Loss: 159.0388\n",
      "Epoch [2190/50000], Train Loss: 333.6708, Test Loss: 172.2896\n",
      "Epoch [2195/50000], Train Loss: 308.9504, Test Loss: 145.1957\n",
      "Epoch [2200/50000], Train Loss: 399.2691, Test Loss: 166.8922\n",
      "Epoch [2205/50000], Train Loss: 412.9046, Test Loss: 161.4404\n",
      "Epoch [2210/50000], Train Loss: 598.5845, Test Loss: 129.2623\n",
      "Epoch [2215/50000], Train Loss: 303.1062, Test Loss: 140.9118\n",
      "Epoch [2220/50000], Train Loss: 308.1694, Test Loss: 141.2104\n",
      "Epoch [2225/50000], Train Loss: 332.2252, Test Loss: 133.9338\n",
      "Epoch [2230/50000], Train Loss: 318.4087, Test Loss: 130.8212\n",
      "Epoch [2235/50000], Train Loss: 303.2704, Test Loss: 161.4127\n",
      "Epoch [2240/50000], Train Loss: 316.2731, Test Loss: 134.4684\n",
      "Epoch [2245/50000], Train Loss: 271.8501, Test Loss: 145.0819\n",
      "Epoch [2250/50000], Train Loss: 312.4779, Test Loss: 153.9618\n",
      "Epoch [2255/50000], Train Loss: 347.7577, Test Loss: 134.1642\n",
      "Epoch [2260/50000], Train Loss: 768.8459, Test Loss: 143.4092\n",
      "Epoch [2265/50000], Train Loss: 431.2127, Test Loss: 130.3737\n",
      "Epoch [2270/50000], Train Loss: 273.4032, Test Loss: 154.9667\n",
      "Epoch [2275/50000], Train Loss: 313.4900, Test Loss: 135.5765\n",
      "Epoch [2280/50000], Train Loss: 302.3015, Test Loss: 152.8408\n",
      "Epoch [2285/50000], Train Loss: 298.1349, Test Loss: 128.1507\n",
      "Epoch [2290/50000], Train Loss: 515.7503, Test Loss: 467.2654\n",
      "Epoch [2295/50000], Train Loss: 302.4562, Test Loss: 148.0892\n",
      "Epoch [2300/50000], Train Loss: 301.4922, Test Loss: 127.0095\n",
      "Epoch [2305/50000], Train Loss: 312.7599, Test Loss: 158.2847\n",
      "Epoch [2310/50000], Train Loss: 298.9582, Test Loss: 138.5189\n",
      "Epoch [2315/50000], Train Loss: 316.3917, Test Loss: 139.4550\n",
      "Epoch [2320/50000], Train Loss: 286.7749, Test Loss: 124.8290\n",
      "Epoch [2325/50000], Train Loss: 493.7943, Test Loss: 144.0877\n",
      "Epoch [2330/50000], Train Loss: 509.4320, Test Loss: 151.2642\n",
      "Epoch [2335/50000], Train Loss: 528.3776, Test Loss: 266.5614\n",
      "Epoch [2340/50000], Train Loss: 323.2961, Test Loss: 136.5778\n",
      "Epoch [2345/50000], Train Loss: 319.4936, Test Loss: 133.7743\n",
      "Epoch [2350/50000], Train Loss: 267.7620, Test Loss: 129.4040\n",
      "Epoch [2355/50000], Train Loss: 271.5761, Test Loss: 128.6770\n",
      "Epoch [2360/50000], Train Loss: 271.6609, Test Loss: 125.3760\n",
      "Epoch [2365/50000], Train Loss: 303.8185, Test Loss: 136.7402\n",
      "Epoch [2370/50000], Train Loss: 325.9480, Test Loss: 276.5210\n",
      "Epoch [2375/50000], Train Loss: 246.3754, Test Loss: 132.0018\n",
      "Epoch [2380/50000], Train Loss: 283.5300, Test Loss: 155.2508\n",
      "Epoch [2385/50000], Train Loss: 261.0514, Test Loss: 136.1410\n",
      "Epoch [2390/50000], Train Loss: 661.5909, Test Loss: 501.9360\n",
      "Epoch [2395/50000], Train Loss: 282.6269, Test Loss: 141.9900\n",
      "Epoch [2400/50000], Train Loss: 283.2442, Test Loss: 121.5852\n",
      "Epoch [2405/50000], Train Loss: 292.3388, Test Loss: 133.8386\n",
      "Epoch [2410/50000], Train Loss: 291.4445, Test Loss: 155.5052\n",
      "Epoch [2415/50000], Train Loss: 268.2274, Test Loss: 140.1947\n",
      "Epoch [2420/50000], Train Loss: 292.1983, Test Loss: 124.6704\n",
      "Epoch [2425/50000], Train Loss: 388.5228, Test Loss: 121.6680\n",
      "Epoch [2430/50000], Train Loss: 383.1558, Test Loss: 129.0041\n",
      "Epoch [2435/50000], Train Loss: 272.5100, Test Loss: 127.1056\n",
      "Epoch [2440/50000], Train Loss: 272.1159, Test Loss: 142.9928\n",
      "Epoch [2445/50000], Train Loss: 291.1069, Test Loss: 130.4209\n",
      "Epoch [2450/50000], Train Loss: 245.1466, Test Loss: 130.4149\n",
      "Epoch [2455/50000], Train Loss: 333.7134, Test Loss: 145.1460\n",
      "Epoch [2460/50000], Train Loss: 320.7897, Test Loss: 140.0773\n",
      "Epoch [2465/50000], Train Loss: 290.2325, Test Loss: 158.8891\n",
      "Epoch [2470/50000], Train Loss: 270.6539, Test Loss: 146.3684\n",
      "Epoch [2475/50000], Train Loss: 307.9818, Test Loss: 119.8127\n",
      "Epoch [2480/50000], Train Loss: 288.8072, Test Loss: 147.1049\n",
      "Epoch [2485/50000], Train Loss: 686.5894, Test Loss: 142.9242\n",
      "Epoch [2490/50000], Train Loss: 276.4445, Test Loss: 132.3469\n",
      "Epoch [2495/50000], Train Loss: 239.7586, Test Loss: 123.5081\n",
      "Epoch [2500/50000], Train Loss: 285.5454, Test Loss: 124.7016\n",
      "Epoch [2505/50000], Train Loss: 263.9048, Test Loss: 131.6126\n",
      "Epoch [2510/50000], Train Loss: 312.5979, Test Loss: 128.5077\n",
      "Epoch [2515/50000], Train Loss: 326.4409, Test Loss: 216.9278\n",
      "Epoch [2520/50000], Train Loss: 516.7808, Test Loss: 135.8023\n",
      "Epoch [2525/50000], Train Loss: 265.9254, Test Loss: 120.2079\n",
      "Epoch [2530/50000], Train Loss: 256.2221, Test Loss: 128.8115\n",
      "Epoch [2535/50000], Train Loss: 246.8138, Test Loss: 124.7177\n",
      "Epoch [2540/50000], Train Loss: 284.8497, Test Loss: 120.4540\n",
      "Epoch [2545/50000], Train Loss: 242.5836, Test Loss: 114.4311\n",
      "Epoch [2550/50000], Train Loss: 253.4934, Test Loss: 127.7592\n",
      "Epoch [2555/50000], Train Loss: 272.4384, Test Loss: 167.6077\n",
      "Epoch [2560/50000], Train Loss: 260.3271, Test Loss: 130.8964\n",
      "Epoch [2565/50000], Train Loss: 288.5367, Test Loss: 128.0927\n",
      "Epoch [2570/50000], Train Loss: 233.7415, Test Loss: 139.0473\n",
      "Epoch [2575/50000], Train Loss: 261.7674, Test Loss: 139.3966\n",
      "Epoch [2580/50000], Train Loss: 232.9762, Test Loss: 139.8540\n",
      "Epoch [2585/50000], Train Loss: 306.6554, Test Loss: 150.7097\n",
      "Epoch [2590/50000], Train Loss: 224.8202, Test Loss: 116.9367\n",
      "Epoch [2595/50000], Train Loss: 254.9463, Test Loss: 122.1491\n",
      "Epoch [2600/50000], Train Loss: 430.6276, Test Loss: 132.9887\n",
      "Epoch [2605/50000], Train Loss: 254.6773, Test Loss: 145.8310\n",
      "Epoch [2610/50000], Train Loss: 276.8438, Test Loss: 113.9077\n",
      "Epoch [2615/50000], Train Loss: 269.7742, Test Loss: 120.5802\n",
      "Epoch [2620/50000], Train Loss: 256.9264, Test Loss: 131.5597\n",
      "Epoch [2625/50000], Train Loss: 259.0914, Test Loss: 133.2183\n",
      "Epoch [2630/50000], Train Loss: 271.0512, Test Loss: 115.6164\n",
      "Epoch [2635/50000], Train Loss: 280.0148, Test Loss: 155.8691\n",
      "Epoch [2640/50000], Train Loss: 274.0789, Test Loss: 112.2395\n",
      "Epoch [2645/50000], Train Loss: 344.5153, Test Loss: 127.0663\n",
      "Epoch [2650/50000], Train Loss: 243.9952, Test Loss: 117.3422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2655/50000], Train Loss: 291.0359, Test Loss: 135.0556\n",
      "Epoch [2660/50000], Train Loss: 615.5026, Test Loss: 113.7413\n",
      "Epoch [2665/50000], Train Loss: 252.5446, Test Loss: 123.9955\n",
      "Epoch [2670/50000], Train Loss: 238.1563, Test Loss: 156.2178\n",
      "Epoch [2675/50000], Train Loss: 297.4396, Test Loss: 168.0928\n",
      "Epoch [2680/50000], Train Loss: 258.7153, Test Loss: 137.7385\n",
      "Epoch [2685/50000], Train Loss: 284.5667, Test Loss: 152.6006\n",
      "Epoch [2690/50000], Train Loss: 263.7489, Test Loss: 114.2242\n",
      "Epoch [2695/50000], Train Loss: 275.2814, Test Loss: 421.4542\n",
      "Epoch [2700/50000], Train Loss: 254.7679, Test Loss: 114.7315\n",
      "Epoch [2705/50000], Train Loss: 297.1209, Test Loss: 126.0189\n",
      "Epoch [2710/50000], Train Loss: 239.2882, Test Loss: 121.4520\n",
      "Epoch [2715/50000], Train Loss: 265.7288, Test Loss: 139.2210\n",
      "Epoch [2720/50000], Train Loss: 261.6972, Test Loss: 127.0905\n",
      "Epoch [2725/50000], Train Loss: 248.5646, Test Loss: 132.8225\n",
      "Epoch [2730/50000], Train Loss: 356.2495, Test Loss: 986.5766\n",
      "Epoch [2735/50000], Train Loss: 254.8824, Test Loss: 198.2993\n",
      "Epoch [2740/50000], Train Loss: 218.7329, Test Loss: 107.9565\n",
      "Epoch [2745/50000], Train Loss: 231.7964, Test Loss: 111.3746\n",
      "Epoch [2750/50000], Train Loss: 230.1327, Test Loss: 119.6975\n",
      "Epoch [2755/50000], Train Loss: 257.4193, Test Loss: 110.6019\n",
      "Epoch [2760/50000], Train Loss: 241.1633, Test Loss: 136.2964\n",
      "Epoch [2765/50000], Train Loss: 217.5009, Test Loss: 131.9307\n",
      "Epoch [2770/50000], Train Loss: 251.9141, Test Loss: 119.9773\n",
      "Epoch [2775/50000], Train Loss: 236.8682, Test Loss: 130.6553\n",
      "Epoch [2780/50000], Train Loss: 216.6305, Test Loss: 116.1332\n",
      "Epoch [2785/50000], Train Loss: 243.5798, Test Loss: 158.4415\n",
      "Epoch [2790/50000], Train Loss: 251.0077, Test Loss: 113.1710\n",
      "Epoch [2795/50000], Train Loss: 242.9142, Test Loss: 123.6711\n",
      "Epoch [2800/50000], Train Loss: 258.8598, Test Loss: 126.2379\n",
      "Epoch [2805/50000], Train Loss: 226.0441, Test Loss: 140.4327\n",
      "Epoch [2810/50000], Train Loss: 235.2346, Test Loss: 114.0534\n",
      "Epoch [2815/50000], Train Loss: 233.5473, Test Loss: 117.6791\n",
      "Epoch [2820/50000], Train Loss: 227.5450, Test Loss: 116.6133\n",
      "Epoch [2825/50000], Train Loss: 232.4523, Test Loss: 135.4650\n",
      "Epoch [2830/50000], Train Loss: 233.8656, Test Loss: 115.2063\n",
      "Epoch [2835/50000], Train Loss: 244.6924, Test Loss: 113.4359\n",
      "Epoch [2840/50000], Train Loss: 253.1179, Test Loss: 108.8458\n",
      "Epoch [2845/50000], Train Loss: 255.3935, Test Loss: 141.0070\n",
      "Epoch [2850/50000], Train Loss: 255.5801, Test Loss: 114.6912\n",
      "Epoch [2855/50000], Train Loss: 225.5431, Test Loss: 121.1230\n",
      "Epoch [2860/50000], Train Loss: 336.5787, Test Loss: 116.4557\n",
      "Epoch [2865/50000], Train Loss: 204.4400, Test Loss: 119.2382\n",
      "Epoch [2870/50000], Train Loss: 223.7749, Test Loss: 130.1219\n",
      "Epoch [2875/50000], Train Loss: 249.4735, Test Loss: 205.2349\n",
      "Epoch [2880/50000], Train Loss: 231.0331, Test Loss: 157.9864\n",
      "Epoch [2885/50000], Train Loss: 220.7475, Test Loss: 108.2846\n",
      "Epoch [2890/50000], Train Loss: 222.2842, Test Loss: 121.0753\n",
      "Epoch [2895/50000], Train Loss: 304.7951, Test Loss: 111.1277\n",
      "Epoch [2900/50000], Train Loss: 358.7629, Test Loss: 105.4288\n",
      "Epoch [2905/50000], Train Loss: 268.6217, Test Loss: 117.6450\n",
      "Epoch [2910/50000], Train Loss: 223.7215, Test Loss: 116.1283\n",
      "Epoch [2915/50000], Train Loss: 394.7622, Test Loss: 136.3361\n",
      "Epoch [2920/50000], Train Loss: 245.2178, Test Loss: 106.9137\n",
      "Epoch [2925/50000], Train Loss: 215.7065, Test Loss: 145.3963\n",
      "Epoch [2930/50000], Train Loss: 230.8882, Test Loss: 149.5075\n",
      "Epoch [2935/50000], Train Loss: 243.0260, Test Loss: 111.4156\n",
      "Epoch [2940/50000], Train Loss: 239.9569, Test Loss: 101.5429\n",
      "Epoch [2945/50000], Train Loss: 211.4243, Test Loss: 102.1304\n",
      "Epoch [2950/50000], Train Loss: 233.5510, Test Loss: 140.3343\n",
      "Epoch [2955/50000], Train Loss: 228.5614, Test Loss: 115.4227\n",
      "Epoch [2960/50000], Train Loss: 236.0551, Test Loss: 111.9043\n",
      "Epoch [2965/50000], Train Loss: 231.5432, Test Loss: 120.8738\n",
      "Epoch [2970/50000], Train Loss: 217.8342, Test Loss: 109.4093\n",
      "Epoch [2975/50000], Train Loss: 202.9720, Test Loss: 110.7200\n",
      "Epoch [2980/50000], Train Loss: 736.2661, Test Loss: 96.9471\n",
      "Epoch [2985/50000], Train Loss: 215.7803, Test Loss: 103.1388\n",
      "Epoch [2990/50000], Train Loss: 199.1650, Test Loss: 110.9248\n",
      "Epoch [2995/50000], Train Loss: 254.6733, Test Loss: 103.1965\n",
      "Epoch [3000/50000], Train Loss: 295.6771, Test Loss: 117.0685\n",
      "Epoch [3005/50000], Train Loss: 217.4025, Test Loss: 127.0456\n",
      "Epoch [3010/50000], Train Loss: 224.1235, Test Loss: 98.9969\n",
      "Epoch [3015/50000], Train Loss: 330.9946, Test Loss: 111.1373\n",
      "Epoch [3020/50000], Train Loss: 249.6148, Test Loss: 133.1366\n",
      "Epoch [3025/50000], Train Loss: 189.3393, Test Loss: 107.5604\n",
      "Epoch [3030/50000], Train Loss: 247.5910, Test Loss: 109.7271\n",
      "Epoch [3035/50000], Train Loss: 212.0663, Test Loss: 115.8293\n",
      "Epoch [3040/50000], Train Loss: 296.7246, Test Loss: 107.1220\n",
      "Epoch [3045/50000], Train Loss: 272.7143, Test Loss: 103.9517\n",
      "Epoch [3050/50000], Train Loss: 220.9305, Test Loss: 111.7056\n",
      "Epoch [3055/50000], Train Loss: 270.3645, Test Loss: 123.0863\n",
      "Epoch [3060/50000], Train Loss: 231.6102, Test Loss: 108.0191\n",
      "Epoch [3065/50000], Train Loss: 321.5979, Test Loss: 98.7344\n",
      "Epoch [3070/50000], Train Loss: 220.8426, Test Loss: 113.2286\n",
      "Epoch [3075/50000], Train Loss: 218.8755, Test Loss: 151.3420\n",
      "Epoch [3080/50000], Train Loss: 216.9458, Test Loss: 102.2251\n",
      "Epoch [3085/50000], Train Loss: 221.4407, Test Loss: 131.9594\n",
      "Epoch [3090/50000], Train Loss: 216.0052, Test Loss: 156.2229\n",
      "Epoch [3095/50000], Train Loss: 200.4487, Test Loss: 111.3791\n",
      "Epoch [3100/50000], Train Loss: 452.5905, Test Loss: 123.3611\n",
      "Epoch [3105/50000], Train Loss: 253.1669, Test Loss: 144.9270\n",
      "Epoch [3110/50000], Train Loss: 220.3096, Test Loss: 113.6437\n",
      "Epoch [3115/50000], Train Loss: 251.2856, Test Loss: 454.3727\n",
      "Epoch [3120/50000], Train Loss: 234.3827, Test Loss: 107.1774\n",
      "Epoch [3125/50000], Train Loss: 187.3896, Test Loss: 102.9850\n",
      "Epoch [3130/50000], Train Loss: 224.9030, Test Loss: 130.4124\n",
      "Epoch [3135/50000], Train Loss: 207.3899, Test Loss: 115.5758\n",
      "Epoch [3140/50000], Train Loss: 209.7352, Test Loss: 110.2741\n",
      "Epoch [3145/50000], Train Loss: 232.0370, Test Loss: 148.6466\n",
      "Epoch [3150/50000], Train Loss: 294.4912, Test Loss: 110.5931\n",
      "Epoch [3155/50000], Train Loss: 207.4309, Test Loss: 102.1019\n",
      "Epoch [3160/50000], Train Loss: 231.7567, Test Loss: 102.4987\n",
      "Epoch [3165/50000], Train Loss: 187.3477, Test Loss: 97.1762\n",
      "Epoch [3170/50000], Train Loss: 195.8527, Test Loss: 109.1365\n",
      "Epoch [3175/50000], Train Loss: 220.9820, Test Loss: 126.0326\n",
      "Epoch [3180/50000], Train Loss: 179.9121, Test Loss: 115.7403\n",
      "Epoch [3185/50000], Train Loss: 198.1757, Test Loss: 107.9558\n",
      "Epoch [3190/50000], Train Loss: 221.1703, Test Loss: 127.3585\n",
      "Epoch [3195/50000], Train Loss: 219.2261, Test Loss: 129.1367\n",
      "Epoch [3200/50000], Train Loss: 230.7641, Test Loss: 100.5789\n",
      "Epoch [3205/50000], Train Loss: 202.1682, Test Loss: 122.0364\n",
      "Epoch [3210/50000], Train Loss: 558.0649, Test Loss: 132.3270\n",
      "Epoch [3215/50000], Train Loss: 198.0946, Test Loss: 104.6282\n",
      "Epoch [3220/50000], Train Loss: 185.5977, Test Loss: 97.5491\n",
      "Epoch [3225/50000], Train Loss: 242.1828, Test Loss: 113.5690\n",
      "Epoch [3230/50000], Train Loss: 216.9573, Test Loss: 102.7243\n",
      "Epoch [3235/50000], Train Loss: 261.9459, Test Loss: 103.0146\n",
      "Epoch [3240/50000], Train Loss: 459.1688, Test Loss: 107.8555\n",
      "Epoch [3245/50000], Train Loss: 204.3714, Test Loss: 132.5778\n",
      "Epoch [3250/50000], Train Loss: 201.6203, Test Loss: 95.8734\n",
      "Epoch [3255/50000], Train Loss: 191.5417, Test Loss: 101.9467\n",
      "Epoch [3260/50000], Train Loss: 213.5381, Test Loss: 169.1118\n",
      "Epoch [3265/50000], Train Loss: 225.5862, Test Loss: 106.7250\n",
      "Epoch [3270/50000], Train Loss: 189.2330, Test Loss: 97.7039\n",
      "Epoch [3275/50000], Train Loss: 183.2883, Test Loss: 117.1007\n",
      "Epoch [3280/50000], Train Loss: 302.1784, Test Loss: 102.2360\n",
      "Epoch [3285/50000], Train Loss: 195.0618, Test Loss: 96.6789\n",
      "Epoch [3290/50000], Train Loss: 180.1656, Test Loss: 112.3052\n",
      "Epoch [3295/50000], Train Loss: 259.8249, Test Loss: 109.4912\n",
      "Epoch [3300/50000], Train Loss: 218.7156, Test Loss: 140.3025\n",
      "Epoch [3305/50000], Train Loss: 212.8900, Test Loss: 109.5401\n",
      "Epoch [3310/50000], Train Loss: 247.6642, Test Loss: 101.2471\n",
      "Epoch [3315/50000], Train Loss: 196.0805, Test Loss: 105.6317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3320/50000], Train Loss: 196.4945, Test Loss: 131.1334\n",
      "Epoch [3325/50000], Train Loss: 219.5826, Test Loss: 110.9843\n",
      "Epoch [3330/50000], Train Loss: 300.0541, Test Loss: 233.8971\n",
      "Epoch [3335/50000], Train Loss: 234.9348, Test Loss: 104.7236\n",
      "Epoch [3340/50000], Train Loss: 228.1568, Test Loss: 110.3026\n",
      "Epoch [3345/50000], Train Loss: 190.4562, Test Loss: 103.9464\n",
      "Epoch [3350/50000], Train Loss: 207.5117, Test Loss: 96.8565\n",
      "Epoch [3355/50000], Train Loss: 180.7019, Test Loss: 102.5646\n",
      "Epoch [3360/50000], Train Loss: 181.2995, Test Loss: 92.7459\n",
      "Epoch [3365/50000], Train Loss: 190.3642, Test Loss: 98.0289\n",
      "Epoch [3370/50000], Train Loss: 195.3831, Test Loss: 107.4541\n",
      "Epoch [3375/50000], Train Loss: 172.5334, Test Loss: 92.3020\n",
      "Epoch [3380/50000], Train Loss: 209.5362, Test Loss: 93.0386\n",
      "Epoch [3385/50000], Train Loss: 203.0890, Test Loss: 124.9049\n",
      "Epoch [3390/50000], Train Loss: 181.3153, Test Loss: 109.5992\n",
      "Epoch [3395/50000], Train Loss: 206.0674, Test Loss: 95.3196\n",
      "Epoch [3400/50000], Train Loss: 183.1353, Test Loss: 99.2628\n",
      "Epoch [3405/50000], Train Loss: 228.1801, Test Loss: 94.9012\n",
      "Epoch [3410/50000], Train Loss: 234.9553, Test Loss: 123.2720\n",
      "Epoch [3415/50000], Train Loss: 178.4525, Test Loss: 89.1363\n",
      "Epoch [3420/50000], Train Loss: 198.4056, Test Loss: 133.1834\n",
      "Epoch [3425/50000], Train Loss: 184.4859, Test Loss: 101.2019\n",
      "Epoch [3430/50000], Train Loss: 193.1610, Test Loss: 105.8615\n",
      "Epoch [3435/50000], Train Loss: 194.6264, Test Loss: 105.2946\n",
      "Epoch [3440/50000], Train Loss: 188.6940, Test Loss: 106.7817\n",
      "Epoch [3445/50000], Train Loss: 174.6780, Test Loss: 97.0917\n",
      "Epoch [3450/50000], Train Loss: 220.9553, Test Loss: 141.5837\n",
      "Epoch [3455/50000], Train Loss: 309.9510, Test Loss: 98.8593\n",
      "Epoch [3460/50000], Train Loss: 192.0759, Test Loss: 100.0521\n",
      "Epoch [3465/50000], Train Loss: 186.1582, Test Loss: 138.3103\n",
      "Epoch [3470/50000], Train Loss: 183.0309, Test Loss: 103.8571\n",
      "Epoch [3475/50000], Train Loss: 179.1709, Test Loss: 107.9755\n",
      "Epoch [3480/50000], Train Loss: 377.6579, Test Loss: 93.6684\n",
      "Epoch [3485/50000], Train Loss: 176.6515, Test Loss: 125.8598\n",
      "Epoch [3490/50000], Train Loss: 191.5941, Test Loss: 125.4195\n",
      "Epoch [3495/50000], Train Loss: 181.9460, Test Loss: 94.6438\n",
      "Epoch [3500/50000], Train Loss: 184.2246, Test Loss: 101.0911\n",
      "Epoch [3505/50000], Train Loss: 200.3064, Test Loss: 92.8134\n",
      "Epoch [3510/50000], Train Loss: 195.1765, Test Loss: 108.2106\n",
      "Epoch [3515/50000], Train Loss: 172.0114, Test Loss: 116.0338\n",
      "Epoch [3520/50000], Train Loss: 182.1560, Test Loss: 89.2432\n",
      "Epoch [3525/50000], Train Loss: 193.1509, Test Loss: 105.9523\n",
      "Epoch [3530/50000], Train Loss: 239.1138, Test Loss: 115.4086\n",
      "Epoch [3535/50000], Train Loss: 178.0059, Test Loss: 100.8493\n",
      "Epoch [3540/50000], Train Loss: 173.5892, Test Loss: 117.9286\n",
      "Epoch [3545/50000], Train Loss: 178.0585, Test Loss: 106.0588\n",
      "Epoch [3550/50000], Train Loss: 172.2902, Test Loss: 91.5000\n",
      "Epoch [3555/50000], Train Loss: 290.5958, Test Loss: 95.6982\n",
      "Epoch [3560/50000], Train Loss: 544.8980, Test Loss: 106.4867\n",
      "Epoch [3565/50000], Train Loss: 178.6970, Test Loss: 164.9861\n",
      "Epoch [3570/50000], Train Loss: 198.8598, Test Loss: 99.3277\n",
      "Epoch [3575/50000], Train Loss: 184.2885, Test Loss: 97.6381\n",
      "Epoch [3580/50000], Train Loss: 185.3517, Test Loss: 100.0740\n",
      "Epoch [3585/50000], Train Loss: 236.3065, Test Loss: 439.0361\n",
      "Epoch [3590/50000], Train Loss: 169.6212, Test Loss: 110.1066\n",
      "Epoch [3595/50000], Train Loss: 169.3539, Test Loss: 95.4099\n",
      "Epoch [3600/50000], Train Loss: 178.4203, Test Loss: 95.9348\n",
      "Epoch [3605/50000], Train Loss: 181.5667, Test Loss: 97.8085\n",
      "Epoch [3610/50000], Train Loss: 183.8602, Test Loss: 105.6926\n",
      "Epoch [3615/50000], Train Loss: 165.8751, Test Loss: 105.4092\n",
      "Epoch [3620/50000], Train Loss: 219.3919, Test Loss: 103.4338\n",
      "Epoch [3625/50000], Train Loss: 198.2666, Test Loss: 119.8473\n",
      "Epoch [3630/50000], Train Loss: 166.5208, Test Loss: 89.4627\n",
      "Epoch [3635/50000], Train Loss: 583.5759, Test Loss: 97.2414\n",
      "Epoch [3640/50000], Train Loss: 180.9863, Test Loss: 94.1250\n",
      "Epoch [3645/50000], Train Loss: 231.2998, Test Loss: 100.6065\n",
      "Epoch [3650/50000], Train Loss: 180.2246, Test Loss: 92.1139\n",
      "Epoch [3655/50000], Train Loss: 172.7884, Test Loss: 98.7679\n",
      "Epoch [3660/50000], Train Loss: 155.8298, Test Loss: 91.3018\n",
      "Epoch [3665/50000], Train Loss: 159.4415, Test Loss: 93.0123\n",
      "Epoch [3670/50000], Train Loss: 204.5078, Test Loss: 139.2839\n",
      "Epoch [3675/50000], Train Loss: 170.7127, Test Loss: 85.5755\n",
      "Epoch [3680/50000], Train Loss: 286.4664, Test Loss: 99.9836\n",
      "Epoch [3685/50000], Train Loss: 191.8482, Test Loss: 103.2769\n",
      "Epoch [3690/50000], Train Loss: 167.6253, Test Loss: 112.4678\n",
      "Epoch [3695/50000], Train Loss: 181.1073, Test Loss: 92.7346\n",
      "Epoch [3700/50000], Train Loss: 175.7390, Test Loss: 100.5333\n",
      "Epoch [3705/50000], Train Loss: 263.1605, Test Loss: 92.7850\n",
      "Epoch [3710/50000], Train Loss: 313.6976, Test Loss: 101.4277\n",
      "Epoch [3715/50000], Train Loss: 168.0046, Test Loss: 96.0050\n",
      "Epoch [3720/50000], Train Loss: 169.6268, Test Loss: 84.0028\n",
      "Epoch [3725/50000], Train Loss: 304.5550, Test Loss: 108.6267\n",
      "Epoch [3730/50000], Train Loss: 184.9250, Test Loss: 94.1052\n",
      "Epoch [3735/50000], Train Loss: 179.9475, Test Loss: 97.8981\n",
      "Epoch [3740/50000], Train Loss: 176.4419, Test Loss: 86.9108\n",
      "Epoch [3745/50000], Train Loss: 140.8710, Test Loss: 83.7609\n",
      "Epoch [3750/50000], Train Loss: 169.2661, Test Loss: 93.9018\n",
      "Epoch [3755/50000], Train Loss: 174.6188, Test Loss: 146.4369\n",
      "Epoch [3760/50000], Train Loss: 179.9218, Test Loss: 96.0134\n",
      "Epoch [3765/50000], Train Loss: 305.5681, Test Loss: 89.1789\n",
      "Epoch [3770/50000], Train Loss: 170.7061, Test Loss: 95.6617\n",
      "Epoch [3775/50000], Train Loss: 174.9122, Test Loss: 99.7322\n",
      "Epoch [3780/50000], Train Loss: 175.1784, Test Loss: 107.9414\n",
      "Epoch [3785/50000], Train Loss: 158.3611, Test Loss: 106.7451\n",
      "Epoch [3790/50000], Train Loss: 181.2391, Test Loss: 86.9918\n",
      "Epoch [3795/50000], Train Loss: 238.2883, Test Loss: 153.4652\n",
      "Epoch [3800/50000], Train Loss: 157.2138, Test Loss: 103.3349\n",
      "Epoch [3805/50000], Train Loss: 167.4894, Test Loss: 85.4964\n",
      "Epoch [3810/50000], Train Loss: 168.7910, Test Loss: 87.9541\n",
      "Epoch [3815/50000], Train Loss: 148.8623, Test Loss: 89.0225\n",
      "Epoch [3820/50000], Train Loss: 252.1884, Test Loss: 239.8571\n",
      "Epoch [3825/50000], Train Loss: 225.6656, Test Loss: 89.6560\n",
      "Epoch [3830/50000], Train Loss: 266.5533, Test Loss: 92.8642\n",
      "Epoch [3835/50000], Train Loss: 161.4755, Test Loss: 91.4789\n",
      "Epoch [3840/50000], Train Loss: 168.2153, Test Loss: 81.6900\n",
      "Epoch [3845/50000], Train Loss: 151.4300, Test Loss: 85.3109\n",
      "Epoch [3850/50000], Train Loss: 170.3049, Test Loss: 90.2414\n",
      "Epoch [3855/50000], Train Loss: 165.6218, Test Loss: 100.1360\n",
      "Epoch [3860/50000], Train Loss: 187.1515, Test Loss: 92.5451\n",
      "Epoch [3865/50000], Train Loss: 166.6836, Test Loss: 112.5181\n",
      "Epoch [3870/50000], Train Loss: 159.7417, Test Loss: 82.8220\n",
      "Epoch [3875/50000], Train Loss: 189.9416, Test Loss: 213.9389\n",
      "Epoch [3880/50000], Train Loss: 170.8782, Test Loss: 94.2020\n",
      "Epoch [3885/50000], Train Loss: 147.2950, Test Loss: 108.3194\n",
      "Epoch [3890/50000], Train Loss: 180.7631, Test Loss: 108.5621\n",
      "Epoch [3895/50000], Train Loss: 165.5597, Test Loss: 107.3565\n",
      "Epoch [3900/50000], Train Loss: 209.8329, Test Loss: 95.5642\n",
      "Epoch [3905/50000], Train Loss: 163.5058, Test Loss: 120.7610\n",
      "Epoch [3910/50000], Train Loss: 168.0037, Test Loss: 97.4393\n",
      "Epoch [3915/50000], Train Loss: 175.2447, Test Loss: 95.2775\n",
      "Epoch [3920/50000], Train Loss: 150.4934, Test Loss: 89.3673\n",
      "Epoch [3925/50000], Train Loss: 154.3612, Test Loss: 88.7827\n",
      "Epoch [3930/50000], Train Loss: 230.9012, Test Loss: 93.4798\n",
      "Epoch [3935/50000], Train Loss: 357.7611, Test Loss: 83.5562\n",
      "Epoch [3940/50000], Train Loss: 171.5743, Test Loss: 112.7476\n",
      "Epoch [3945/50000], Train Loss: 191.8053, Test Loss: 95.7741\n",
      "Epoch [3950/50000], Train Loss: 171.0234, Test Loss: 87.8384\n",
      "Epoch [3955/50000], Train Loss: 181.6267, Test Loss: 96.5239\n",
      "Epoch [3960/50000], Train Loss: 170.2790, Test Loss: 92.8608\n",
      "Epoch [3965/50000], Train Loss: 160.0383, Test Loss: 93.8463\n",
      "Epoch [3970/50000], Train Loss: 160.1610, Test Loss: 119.4595\n",
      "Epoch [3975/50000], Train Loss: 219.6107, Test Loss: 97.1620\n",
      "Epoch [3980/50000], Train Loss: 422.5025, Test Loss: 96.3634\n",
      "Epoch [3985/50000], Train Loss: 149.8637, Test Loss: 100.6415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3990/50000], Train Loss: 172.5244, Test Loss: 104.7298\n",
      "Epoch [3995/50000], Train Loss: 171.7292, Test Loss: 131.5518\n",
      "Epoch [4000/50000], Train Loss: 151.4547, Test Loss: 90.7055\n",
      "Epoch [4005/50000], Train Loss: 155.1848, Test Loss: 85.5521\n",
      "Epoch [4010/50000], Train Loss: 278.5001, Test Loss: 90.1113\n",
      "Epoch [4015/50000], Train Loss: 533.2102, Test Loss: 98.3537\n",
      "Epoch [4020/50000], Train Loss: 170.6165, Test Loss: 145.6058\n",
      "Epoch [4025/50000], Train Loss: 182.9934, Test Loss: 84.7430\n",
      "Epoch [4030/50000], Train Loss: 163.6653, Test Loss: 91.2051\n",
      "Epoch [4035/50000], Train Loss: 181.3302, Test Loss: 86.5391\n",
      "Epoch [4040/50000], Train Loss: 147.3652, Test Loss: 84.1755\n",
      "Epoch [4045/50000], Train Loss: 267.0453, Test Loss: 89.3744\n",
      "Epoch [4050/50000], Train Loss: 151.3985, Test Loss: 80.6732\n",
      "Epoch [4055/50000], Train Loss: 150.8773, Test Loss: 85.6242\n",
      "Epoch [4060/50000], Train Loss: 163.7691, Test Loss: 88.1393\n",
      "Epoch [4065/50000], Train Loss: 167.9990, Test Loss: 81.4490\n",
      "Epoch [4070/50000], Train Loss: 160.0384, Test Loss: 97.4415\n",
      "Epoch [4075/50000], Train Loss: 179.4239, Test Loss: 89.0474\n",
      "Epoch [4080/50000], Train Loss: 169.7108, Test Loss: 96.6458\n",
      "Epoch [4085/50000], Train Loss: 165.6438, Test Loss: 106.9963\n",
      "Epoch [4090/50000], Train Loss: 144.5601, Test Loss: 91.2578\n",
      "Epoch [4095/50000], Train Loss: 159.5912, Test Loss: 92.5693\n",
      "Epoch [4100/50000], Train Loss: 159.6738, Test Loss: 86.4328\n",
      "Epoch [4105/50000], Train Loss: 162.2441, Test Loss: 90.0438\n",
      "Epoch [4110/50000], Train Loss: 138.8235, Test Loss: 91.7655\n",
      "Epoch [4115/50000], Train Loss: 214.7758, Test Loss: 110.8178\n",
      "Epoch [4120/50000], Train Loss: 154.3876, Test Loss: 84.9331\n",
      "Epoch [4125/50000], Train Loss: 150.2799, Test Loss: 111.8799\n",
      "Epoch [4130/50000], Train Loss: 161.6412, Test Loss: 88.1200\n",
      "Epoch [4135/50000], Train Loss: 144.7508, Test Loss: 104.4472\n",
      "Epoch [4140/50000], Train Loss: 230.5374, Test Loss: 107.4875\n",
      "Epoch [4145/50000], Train Loss: 150.3300, Test Loss: 113.4862\n",
      "Epoch [4150/50000], Train Loss: 152.4404, Test Loss: 92.5863\n",
      "Epoch [4155/50000], Train Loss: 132.4064, Test Loss: 87.9939\n",
      "Epoch [4160/50000], Train Loss: 260.1681, Test Loss: 86.0121\n",
      "Epoch [4165/50000], Train Loss: 169.3655, Test Loss: 113.3007\n",
      "Epoch [4170/50000], Train Loss: 173.6723, Test Loss: 85.6842\n",
      "Epoch [4175/50000], Train Loss: 155.5235, Test Loss: 93.6752\n",
      "Epoch [4180/50000], Train Loss: 198.6592, Test Loss: 106.1229\n",
      "Epoch [4185/50000], Train Loss: 149.2450, Test Loss: 93.3184\n",
      "Epoch [4190/50000], Train Loss: 323.1337, Test Loss: 177.5626\n",
      "Epoch [4195/50000], Train Loss: 152.3343, Test Loss: 81.4045\n",
      "Epoch [4200/50000], Train Loss: 147.3720, Test Loss: 93.6782\n",
      "Epoch [4205/50000], Train Loss: 165.8816, Test Loss: 95.5827\n",
      "Epoch [4210/50000], Train Loss: 140.7603, Test Loss: 91.7533\n",
      "Epoch [4215/50000], Train Loss: 159.9686, Test Loss: 101.5511\n",
      "Epoch [4220/50000], Train Loss: 156.9000, Test Loss: 156.0738\n",
      "Epoch [4225/50000], Train Loss: 160.3144, Test Loss: 94.1946\n",
      "Epoch [4230/50000], Train Loss: 155.7051, Test Loss: 97.1133\n",
      "Epoch [4235/50000], Train Loss: 155.6179, Test Loss: 86.9406\n",
      "Epoch [4240/50000], Train Loss: 176.1869, Test Loss: 138.1881\n",
      "Epoch [4245/50000], Train Loss: 172.9568, Test Loss: 79.2013\n",
      "Epoch [4250/50000], Train Loss: 150.9552, Test Loss: 95.1216\n",
      "Epoch [4255/50000], Train Loss: 137.5286, Test Loss: 83.1269\n",
      "Epoch [4260/50000], Train Loss: 151.1500, Test Loss: 83.0275\n",
      "Epoch [4265/50000], Train Loss: 173.3088, Test Loss: 100.5773\n",
      "Epoch [4270/50000], Train Loss: 240.0888, Test Loss: 568.9680\n",
      "Epoch [4275/50000], Train Loss: 153.4943, Test Loss: 91.5770\n",
      "Epoch [4280/50000], Train Loss: 151.2726, Test Loss: 110.6268\n",
      "Epoch [4285/50000], Train Loss: 137.8312, Test Loss: 87.7378\n",
      "Epoch [4290/50000], Train Loss: 136.2271, Test Loss: 83.5317\n",
      "Epoch [4295/50000], Train Loss: 154.6888, Test Loss: 91.8025\n",
      "Epoch [4300/50000], Train Loss: 168.1282, Test Loss: 81.9914\n",
      "Epoch [4305/50000], Train Loss: 227.0578, Test Loss: 91.5143\n",
      "Epoch [4310/50000], Train Loss: 157.1529, Test Loss: 80.3671\n",
      "Epoch [4315/50000], Train Loss: 281.3284, Test Loss: 111.4251\n",
      "Epoch [4320/50000], Train Loss: 153.2851, Test Loss: 81.7932\n",
      "Epoch [4325/50000], Train Loss: 167.0469, Test Loss: 83.4970\n",
      "Epoch [4330/50000], Train Loss: 130.5401, Test Loss: 82.9215\n",
      "Epoch [4335/50000], Train Loss: 132.0244, Test Loss: 90.8701\n",
      "Epoch [4340/50000], Train Loss: 150.1857, Test Loss: 86.1871\n",
      "Epoch [4345/50000], Train Loss: 212.2976, Test Loss: 78.5026\n",
      "Epoch [4350/50000], Train Loss: 151.4640, Test Loss: 105.9840\n",
      "Epoch [4355/50000], Train Loss: 118.9998, Test Loss: 79.5695\n",
      "Epoch [4360/50000], Train Loss: 140.9352, Test Loss: 91.1773\n",
      "Epoch [4365/50000], Train Loss: 175.9274, Test Loss: 105.5356\n",
      "Epoch [4370/50000], Train Loss: 158.2887, Test Loss: 85.3738\n",
      "Epoch [4375/50000], Train Loss: 145.1856, Test Loss: 78.5998\n",
      "Epoch [4380/50000], Train Loss: 144.8745, Test Loss: 77.3880\n",
      "Epoch [4385/50000], Train Loss: 138.7407, Test Loss: 91.9162\n",
      "Epoch [4390/50000], Train Loss: 159.3167, Test Loss: 86.3644\n",
      "Epoch [4395/50000], Train Loss: 138.7499, Test Loss: 76.4402\n",
      "Epoch [4400/50000], Train Loss: 180.6344, Test Loss: 83.5649\n",
      "Epoch [4405/50000], Train Loss: 160.1265, Test Loss: 91.7059\n",
      "Epoch [4410/50000], Train Loss: 225.1755, Test Loss: 79.5910\n",
      "Epoch [4415/50000], Train Loss: 145.8209, Test Loss: 79.5191\n",
      "Epoch [4420/50000], Train Loss: 179.8737, Test Loss: 118.7336\n",
      "Epoch [4425/50000], Train Loss: 141.5193, Test Loss: 76.2495\n",
      "Epoch [4430/50000], Train Loss: 152.9154, Test Loss: 89.4931\n",
      "Epoch [4435/50000], Train Loss: 150.7260, Test Loss: 89.7649\n",
      "Epoch [4440/50000], Train Loss: 171.7582, Test Loss: 100.9072\n",
      "Epoch [4445/50000], Train Loss: 155.2827, Test Loss: 79.2538\n",
      "Epoch [4450/50000], Train Loss: 139.5292, Test Loss: 80.7112\n",
      "Epoch [4455/50000], Train Loss: 177.5500, Test Loss: 86.0764\n",
      "Epoch [4460/50000], Train Loss: 124.5490, Test Loss: 84.4276\n",
      "Epoch [4465/50000], Train Loss: 144.3280, Test Loss: 101.2527\n",
      "Epoch [4470/50000], Train Loss: 378.9019, Test Loss: 88.2810\n",
      "Epoch [4475/50000], Train Loss: 147.5793, Test Loss: 89.5919\n",
      "Epoch [4480/50000], Train Loss: 431.4420, Test Loss: 79.6743\n",
      "Epoch [4485/50000], Train Loss: 140.7617, Test Loss: 84.3119\n",
      "Epoch [4490/50000], Train Loss: 134.9842, Test Loss: 113.7225\n",
      "Epoch [4495/50000], Train Loss: 143.0849, Test Loss: 99.9504\n",
      "Epoch [4500/50000], Train Loss: 142.8990, Test Loss: 87.6532\n",
      "Epoch [4505/50000], Train Loss: 145.4218, Test Loss: 120.9853\n",
      "Epoch [4510/50000], Train Loss: 134.8609, Test Loss: 101.8475\n",
      "Epoch [4515/50000], Train Loss: 162.8329, Test Loss: 99.9383\n",
      "Epoch [4520/50000], Train Loss: 143.2918, Test Loss: 80.4968\n",
      "Epoch [4525/50000], Train Loss: 160.9926, Test Loss: 76.7840\n",
      "Epoch [4530/50000], Train Loss: 328.3075, Test Loss: 74.2867\n",
      "Epoch [4535/50000], Train Loss: 130.5045, Test Loss: 86.3932\n",
      "Epoch [4540/50000], Train Loss: 308.8607, Test Loss: 77.9311\n",
      "Epoch [4545/50000], Train Loss: 156.0738, Test Loss: 120.4295\n",
      "Epoch [4550/50000], Train Loss: 131.5849, Test Loss: 111.0424\n",
      "Epoch [4555/50000], Train Loss: 140.8167, Test Loss: 81.6730\n",
      "Epoch [4560/50000], Train Loss: 168.3745, Test Loss: 107.4675\n",
      "Epoch [4565/50000], Train Loss: 184.6366, Test Loss: 101.8057\n",
      "Epoch [4570/50000], Train Loss: 143.1206, Test Loss: 82.3594\n",
      "Epoch [4575/50000], Train Loss: 142.1831, Test Loss: 98.5520\n",
      "Epoch [4580/50000], Train Loss: 343.9703, Test Loss: 76.8675\n",
      "Epoch [4585/50000], Train Loss: 157.6184, Test Loss: 78.4384\n",
      "Epoch [4590/50000], Train Loss: 143.3596, Test Loss: 88.0424\n",
      "Epoch [4595/50000], Train Loss: 114.6001, Test Loss: 84.0826\n",
      "Epoch [4600/50000], Train Loss: 133.5497, Test Loss: 86.5239\n",
      "Epoch [4605/50000], Train Loss: 126.6448, Test Loss: 78.6481\n",
      "Epoch [4610/50000], Train Loss: 125.0190, Test Loss: 77.2355\n",
      "Epoch [4615/50000], Train Loss: 136.1338, Test Loss: 75.1678\n",
      "Epoch [4620/50000], Train Loss: 141.3825, Test Loss: 79.8123\n",
      "Epoch [4625/50000], Train Loss: 400.4030, Test Loss: 91.4316\n",
      "Epoch [4630/50000], Train Loss: 124.2362, Test Loss: 87.4241\n",
      "Epoch [4635/50000], Train Loss: 132.5920, Test Loss: 83.3025\n",
      "Epoch [4640/50000], Train Loss: 220.2318, Test Loss: 79.6111\n",
      "Epoch [4645/50000], Train Loss: 139.8538, Test Loss: 88.2807\n",
      "Epoch [4650/50000], Train Loss: 141.5142, Test Loss: 100.3943\n",
      "Epoch [4655/50000], Train Loss: 216.1819, Test Loss: 102.0072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4660/50000], Train Loss: 146.1210, Test Loss: 93.8692\n",
      "Epoch [4665/50000], Train Loss: 146.7849, Test Loss: 78.4368\n",
      "Epoch [4670/50000], Train Loss: 165.3936, Test Loss: 87.9022\n",
      "Epoch [4675/50000], Train Loss: 132.3805, Test Loss: 72.3866\n",
      "Epoch [4680/50000], Train Loss: 136.0207, Test Loss: 79.6800\n",
      "Epoch [4685/50000], Train Loss: 136.4176, Test Loss: 83.3584\n",
      "Epoch [4690/50000], Train Loss: 137.7442, Test Loss: 77.3549\n",
      "Epoch [4695/50000], Train Loss: 143.1215, Test Loss: 77.6892\n",
      "Epoch [4700/50000], Train Loss: 145.3907, Test Loss: 76.5703\n",
      "Epoch [4705/50000], Train Loss: 159.9806, Test Loss: 89.1540\n",
      "Epoch [4710/50000], Train Loss: 164.0702, Test Loss: 127.4112\n",
      "Epoch [4715/50000], Train Loss: 126.2693, Test Loss: 77.9885\n",
      "Epoch [4720/50000], Train Loss: 142.1423, Test Loss: 85.2067\n",
      "Epoch [4725/50000], Train Loss: 139.3665, Test Loss: 74.8576\n",
      "Epoch [4730/50000], Train Loss: 138.8270, Test Loss: 86.7414\n",
      "Epoch [4735/50000], Train Loss: 287.1234, Test Loss: 77.7020\n",
      "Epoch [4740/50000], Train Loss: 572.5459, Test Loss: 94.0129\n",
      "Epoch [4745/50000], Train Loss: 128.5541, Test Loss: 87.6714\n",
      "Epoch [4750/50000], Train Loss: 155.6291, Test Loss: 87.1259\n",
      "Epoch [4755/50000], Train Loss: 109.2309, Test Loss: 72.1891\n",
      "Epoch [4760/50000], Train Loss: 143.0528, Test Loss: 82.0491\n",
      "Epoch [4765/50000], Train Loss: 172.8527, Test Loss: 101.5803\n",
      "Epoch [4770/50000], Train Loss: 141.7595, Test Loss: 105.9909\n",
      "Epoch [4775/50000], Train Loss: 126.2274, Test Loss: 76.5946\n",
      "Epoch [4780/50000], Train Loss: 155.0451, Test Loss: 73.1321\n",
      "Epoch [4785/50000], Train Loss: 143.1559, Test Loss: 83.1041\n",
      "Epoch [4790/50000], Train Loss: 135.5879, Test Loss: 83.2031\n",
      "Epoch [4795/50000], Train Loss: 159.4280, Test Loss: 447.7450\n",
      "Epoch [4800/50000], Train Loss: 147.0939, Test Loss: 78.5717\n",
      "Epoch [4805/50000], Train Loss: 146.8212, Test Loss: 72.7372\n",
      "Epoch [4810/50000], Train Loss: 132.7640, Test Loss: 73.5245\n",
      "Epoch [4815/50000], Train Loss: 132.3689, Test Loss: 82.0369\n",
      "Epoch [4820/50000], Train Loss: 137.7875, Test Loss: 95.3102\n",
      "Epoch [4825/50000], Train Loss: 130.4788, Test Loss: 120.6526\n",
      "Epoch [4830/50000], Train Loss: 533.8658, Test Loss: 76.0576\n",
      "Epoch [4835/50000], Train Loss: 144.5678, Test Loss: 88.6572\n",
      "Epoch [4840/50000], Train Loss: 134.9903, Test Loss: 78.4973\n",
      "Epoch [4845/50000], Train Loss: 142.0338, Test Loss: 86.8239\n",
      "Epoch [4850/50000], Train Loss: 129.1811, Test Loss: 130.3194\n",
      "Epoch [4855/50000], Train Loss: 135.4506, Test Loss: 88.3365\n",
      "Epoch [4860/50000], Train Loss: 124.0757, Test Loss: 102.7558\n",
      "Epoch [4865/50000], Train Loss: 224.6576, Test Loss: 76.3783\n",
      "Epoch [4870/50000], Train Loss: 117.5261, Test Loss: 88.8687\n",
      "Epoch [4875/50000], Train Loss: 138.3949, Test Loss: 77.1531\n",
      "Epoch [4880/50000], Train Loss: 131.3663, Test Loss: 77.3811\n",
      "Epoch [4885/50000], Train Loss: 133.8265, Test Loss: 88.4871\n",
      "Epoch [4890/50000], Train Loss: 118.9129, Test Loss: 76.0446\n",
      "Epoch [4895/50000], Train Loss: 124.8087, Test Loss: 88.2274\n",
      "Epoch [4900/50000], Train Loss: 114.4601, Test Loss: 79.7607\n",
      "Epoch [4905/50000], Train Loss: 133.9107, Test Loss: 76.1693\n",
      "Epoch [4910/50000], Train Loss: 122.5857, Test Loss: 76.9394\n",
      "Epoch [4915/50000], Train Loss: 143.9768, Test Loss: 82.5432\n",
      "Epoch [4920/50000], Train Loss: 128.2834, Test Loss: 85.5711\n",
      "Epoch [4925/50000], Train Loss: 146.8242, Test Loss: 74.8340\n",
      "Epoch [4930/50000], Train Loss: 192.0003, Test Loss: 103.0906\n",
      "Epoch [4935/50000], Train Loss: 134.7301, Test Loss: 77.8089\n",
      "Epoch [4940/50000], Train Loss: 120.3933, Test Loss: 84.2195\n",
      "Epoch [4945/50000], Train Loss: 133.2209, Test Loss: 89.0279\n",
      "Epoch [4950/50000], Train Loss: 132.1293, Test Loss: 95.8619\n",
      "Epoch [4955/50000], Train Loss: 171.8064, Test Loss: 74.5242\n",
      "Epoch [4960/50000], Train Loss: 683.3415, Test Loss: 73.0015\n",
      "Epoch [4965/50000], Train Loss: 134.8928, Test Loss: 79.2430\n",
      "Epoch [4970/50000], Train Loss: 123.6811, Test Loss: 80.2066\n",
      "Epoch [4975/50000], Train Loss: 150.5351, Test Loss: 110.8569\n",
      "Epoch [4980/50000], Train Loss: 140.1005, Test Loss: 93.1340\n",
      "Epoch [4985/50000], Train Loss: 160.1669, Test Loss: 88.9158\n",
      "Epoch [4990/50000], Train Loss: 109.1615, Test Loss: 72.5854\n",
      "Epoch [4995/50000], Train Loss: 137.3664, Test Loss: 84.5225\n",
      "Epoch [5000/50000], Train Loss: 299.5674, Test Loss: 204.1251\n",
      "Epoch [5005/50000], Train Loss: 134.3565, Test Loss: 80.5783\n",
      "Epoch [5010/50000], Train Loss: 125.2487, Test Loss: 80.4152\n",
      "Epoch [5015/50000], Train Loss: 123.3522, Test Loss: 74.6096\n",
      "Epoch [5020/50000], Train Loss: 118.8465, Test Loss: 69.2358\n",
      "Epoch [5025/50000], Train Loss: 128.2621, Test Loss: 87.8934\n",
      "Epoch [5030/50000], Train Loss: 137.2761, Test Loss: 73.1257\n",
      "Epoch [5035/50000], Train Loss: 131.4130, Test Loss: 78.3760\n",
      "Epoch [5040/50000], Train Loss: 136.3963, Test Loss: 108.6425\n",
      "Epoch [5045/50000], Train Loss: 139.0996, Test Loss: 83.4905\n",
      "Epoch [5050/50000], Train Loss: 125.8092, Test Loss: 78.6241\n",
      "Epoch [5055/50000], Train Loss: 129.1546, Test Loss: 115.4136\n",
      "Epoch [5060/50000], Train Loss: 118.8649, Test Loss: 123.0825\n",
      "Epoch [5065/50000], Train Loss: 279.2480, Test Loss: 79.7765\n",
      "Epoch [5070/50000], Train Loss: 112.3357, Test Loss: 93.2613\n",
      "Epoch [5075/50000], Train Loss: 123.8035, Test Loss: 73.4877\n",
      "Epoch [5080/50000], Train Loss: 296.9371, Test Loss: 80.5857\n",
      "Epoch [5085/50000], Train Loss: 132.9945, Test Loss: 77.2595\n",
      "Epoch [5090/50000], Train Loss: 139.1943, Test Loss: 77.5365\n",
      "Epoch [5095/50000], Train Loss: 120.1899, Test Loss: 88.4716\n",
      "Epoch [5100/50000], Train Loss: 130.6870, Test Loss: 83.3727\n",
      "Epoch [5105/50000], Train Loss: 129.3921, Test Loss: 75.0782\n",
      "Epoch [5110/50000], Train Loss: 350.5755, Test Loss: 80.9475\n",
      "Epoch [5115/50000], Train Loss: 122.1368, Test Loss: 93.1066\n",
      "Epoch [5120/50000], Train Loss: 140.4878, Test Loss: 82.9215\n",
      "Epoch [5125/50000], Train Loss: 126.6598, Test Loss: 83.2260\n",
      "Epoch [5130/50000], Train Loss: 149.3373, Test Loss: 78.4854\n",
      "Epoch [5135/50000], Train Loss: 215.1890, Test Loss: 76.8847\n",
      "Epoch [5140/50000], Train Loss: 183.6193, Test Loss: 84.8892\n",
      "Epoch [5145/50000], Train Loss: 202.9920, Test Loss: 75.0352\n",
      "Epoch [5150/50000], Train Loss: 159.8837, Test Loss: 75.9674\n",
      "Epoch [5155/50000], Train Loss: 124.8863, Test Loss: 78.7477\n",
      "Epoch [5160/50000], Train Loss: 167.1503, Test Loss: 75.5442\n",
      "Epoch [5165/50000], Train Loss: 120.5822, Test Loss: 120.9542\n",
      "Epoch [5170/50000], Train Loss: 110.2358, Test Loss: 70.2613\n",
      "Epoch [5175/50000], Train Loss: 131.4240, Test Loss: 86.6920\n",
      "Epoch [5180/50000], Train Loss: 125.9059, Test Loss: 72.8582\n",
      "Epoch [5185/50000], Train Loss: 121.7379, Test Loss: 75.1573\n",
      "Epoch [5190/50000], Train Loss: 133.5542, Test Loss: 70.8410\n",
      "Epoch [5195/50000], Train Loss: 126.1830, Test Loss: 95.2780\n",
      "Epoch [5200/50000], Train Loss: 133.9179, Test Loss: 102.7423\n",
      "Epoch [5205/50000], Train Loss: 131.3795, Test Loss: 80.6335\n",
      "Epoch [5210/50000], Train Loss: 203.4799, Test Loss: 73.3390\n",
      "Epoch [5215/50000], Train Loss: 131.1903, Test Loss: 92.5271\n",
      "Epoch [5220/50000], Train Loss: 152.6284, Test Loss: 70.8392\n",
      "Epoch [5225/50000], Train Loss: 134.2123, Test Loss: 76.1703\n",
      "Epoch [5230/50000], Train Loss: 129.8617, Test Loss: 113.1120\n",
      "Epoch [5235/50000], Train Loss: 126.2478, Test Loss: 79.7025\n",
      "Epoch [5240/50000], Train Loss: 173.3921, Test Loss: 148.0645\n",
      "Epoch [5245/50000], Train Loss: 135.7837, Test Loss: 95.0342\n",
      "Epoch [5250/50000], Train Loss: 124.4063, Test Loss: 77.4871\n",
      "Epoch [5255/50000], Train Loss: 106.6883, Test Loss: 82.4565\n",
      "Epoch [5260/50000], Train Loss: 107.0949, Test Loss: 83.2265\n",
      "Epoch [5265/50000], Train Loss: 118.3696, Test Loss: 73.7627\n",
      "Epoch [5270/50000], Train Loss: 109.9304, Test Loss: 73.4241\n",
      "Epoch [5275/50000], Train Loss: 114.8652, Test Loss: 88.2171\n",
      "Epoch [5280/50000], Train Loss: 108.7546, Test Loss: 71.6266\n",
      "Epoch [5285/50000], Train Loss: 107.8832, Test Loss: 75.7890\n",
      "Epoch [5290/50000], Train Loss: 123.9191, Test Loss: 74.8281\n",
      "Epoch [5295/50000], Train Loss: 148.6484, Test Loss: 84.5586\n",
      "Epoch [5300/50000], Train Loss: 167.5250, Test Loss: 75.3046\n",
      "Epoch [5305/50000], Train Loss: 110.9022, Test Loss: 74.5904\n",
      "Epoch [5310/50000], Train Loss: 142.7906, Test Loss: 71.1170\n",
      "Epoch [5315/50000], Train Loss: 120.5129, Test Loss: 95.2372\n",
      "Epoch [5320/50000], Train Loss: 129.7523, Test Loss: 71.5980\n",
      "Epoch [5325/50000], Train Loss: 125.7138, Test Loss: 77.2881\n",
      "Epoch [5330/50000], Train Loss: 117.7958, Test Loss: 74.0059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5335/50000], Train Loss: 220.9191, Test Loss: 72.5927\n",
      "Epoch [5340/50000], Train Loss: 129.8521, Test Loss: 73.1265\n",
      "Epoch [5345/50000], Train Loss: 116.2187, Test Loss: 78.1656\n",
      "Epoch [5350/50000], Train Loss: 171.2327, Test Loss: 72.4517\n",
      "Epoch [5355/50000], Train Loss: 143.7701, Test Loss: 120.2572\n",
      "Epoch [5360/50000], Train Loss: 229.3499, Test Loss: 93.3399\n",
      "Epoch [5365/50000], Train Loss: 129.6995, Test Loss: 76.6618\n",
      "Epoch [5370/50000], Train Loss: 115.8055, Test Loss: 70.8569\n",
      "Epoch [5375/50000], Train Loss: 124.4930, Test Loss: 81.2921\n",
      "Epoch [5380/50000], Train Loss: 116.2745, Test Loss: 73.8338\n",
      "Epoch [5385/50000], Train Loss: 123.0626, Test Loss: 71.0170\n",
      "Epoch [5390/50000], Train Loss: 136.7344, Test Loss: 76.0187\n",
      "Epoch [5395/50000], Train Loss: 145.6304, Test Loss: 99.5849\n",
      "Epoch [5400/50000], Train Loss: 121.7271, Test Loss: 96.2037\n",
      "Epoch [5405/50000], Train Loss: 111.7216, Test Loss: 75.3192\n",
      "Epoch [5410/50000], Train Loss: 135.8433, Test Loss: 121.0934\n",
      "Epoch [5415/50000], Train Loss: 130.3991, Test Loss: 70.3717\n",
      "Epoch [5420/50000], Train Loss: 129.7040, Test Loss: 77.5384\n",
      "Epoch [5425/50000], Train Loss: 113.4892, Test Loss: 79.5543\n",
      "Epoch [5430/50000], Train Loss: 114.7048, Test Loss: 77.2580\n",
      "Epoch [5435/50000], Train Loss: 125.3034, Test Loss: 88.3131\n",
      "Epoch [5440/50000], Train Loss: 138.9834, Test Loss: 81.4855\n",
      "Epoch [5445/50000], Train Loss: 124.3737, Test Loss: 85.2876\n",
      "Epoch [5450/50000], Train Loss: 104.3875, Test Loss: 80.2244\n",
      "Epoch [5455/50000], Train Loss: 295.8797, Test Loss: 70.4610\n",
      "Epoch [5460/50000], Train Loss: 128.5273, Test Loss: 80.0721\n",
      "Epoch [5465/50000], Train Loss: 116.9633, Test Loss: 98.3243\n",
      "Epoch [5470/50000], Train Loss: 224.8486, Test Loss: 259.1464\n",
      "Epoch [5475/50000], Train Loss: 122.9294, Test Loss: 78.4078\n",
      "Epoch [5480/50000], Train Loss: 102.8302, Test Loss: 68.6195\n",
      "Epoch [5485/50000], Train Loss: 116.0380, Test Loss: 86.2994\n",
      "Epoch [5490/50000], Train Loss: 111.1061, Test Loss: 78.8983\n",
      "Epoch [5495/50000], Train Loss: 135.6716, Test Loss: 70.7655\n",
      "Epoch [5500/50000], Train Loss: 187.0341, Test Loss: 157.8358\n",
      "Epoch [5505/50000], Train Loss: 128.6827, Test Loss: 100.1846\n",
      "Epoch [5510/50000], Train Loss: 121.3192, Test Loss: 73.6293\n",
      "Epoch [5515/50000], Train Loss: 100.5723, Test Loss: 73.1158\n",
      "Epoch [5520/50000], Train Loss: 127.9132, Test Loss: 80.3840\n",
      "Epoch [5525/50000], Train Loss: 112.6027, Test Loss: 80.0080\n",
      "Epoch [5530/50000], Train Loss: 116.2858, Test Loss: 78.6376\n",
      "Epoch [5535/50000], Train Loss: 108.2074, Test Loss: 75.3207\n",
      "Epoch [5540/50000], Train Loss: 177.4580, Test Loss: 192.1125\n",
      "Epoch [5545/50000], Train Loss: 119.8091, Test Loss: 72.4999\n",
      "Epoch [5550/50000], Train Loss: 129.1664, Test Loss: 82.6259\n",
      "Epoch [5555/50000], Train Loss: 176.5712, Test Loss: 97.1386\n",
      "Epoch [5560/50000], Train Loss: 114.1626, Test Loss: 83.0246\n",
      "Epoch [5565/50000], Train Loss: 130.3499, Test Loss: 75.9272\n",
      "Epoch [5570/50000], Train Loss: 106.4589, Test Loss: 74.2285\n",
      "Epoch [5575/50000], Train Loss: 151.9447, Test Loss: 81.7153\n",
      "Epoch [5580/50000], Train Loss: 135.0596, Test Loss: 76.7421\n",
      "Epoch [5585/50000], Train Loss: 101.1591, Test Loss: 74.5301\n",
      "Epoch [5590/50000], Train Loss: 108.5790, Test Loss: 70.7335\n",
      "Epoch [5595/50000], Train Loss: 277.8162, Test Loss: 66.6597\n",
      "Epoch [5600/50000], Train Loss: 128.4959, Test Loss: 78.7526\n",
      "Epoch [5605/50000], Train Loss: 126.2349, Test Loss: 73.8703\n",
      "Epoch [5610/50000], Train Loss: 112.1380, Test Loss: 70.0367\n",
      "Epoch [5615/50000], Train Loss: 97.5319, Test Loss: 65.8724\n",
      "Epoch [5620/50000], Train Loss: 114.4977, Test Loss: 73.0300\n",
      "Epoch [5625/50000], Train Loss: 193.3866, Test Loss: 99.3417\n",
      "Epoch [5630/50000], Train Loss: 113.8003, Test Loss: 80.3121\n",
      "Epoch [5635/50000], Train Loss: 93.6331, Test Loss: 70.5737\n",
      "Epoch [5640/50000], Train Loss: 116.1153, Test Loss: 106.4839\n",
      "Epoch [5645/50000], Train Loss: 119.4124, Test Loss: 75.8533\n",
      "Epoch [5650/50000], Train Loss: 107.7705, Test Loss: 72.3870\n",
      "Epoch [5655/50000], Train Loss: 118.0319, Test Loss: 70.3479\n",
      "Epoch [5660/50000], Train Loss: 105.3209, Test Loss: 79.9540\n",
      "Epoch [5665/50000], Train Loss: 126.3733, Test Loss: 65.2070\n",
      "Epoch [5670/50000], Train Loss: 120.4728, Test Loss: 74.4583\n",
      "Epoch [5675/50000], Train Loss: 141.1898, Test Loss: 71.9666\n",
      "Epoch [5680/50000], Train Loss: 138.5509, Test Loss: 126.5489\n",
      "Epoch [5685/50000], Train Loss: 132.6578, Test Loss: 72.8915\n",
      "Epoch [5690/50000], Train Loss: 113.2408, Test Loss: 75.3697\n",
      "Epoch [5695/50000], Train Loss: 112.5163, Test Loss: 82.0613\n",
      "Epoch [5700/50000], Train Loss: 107.7762, Test Loss: 74.0559\n",
      "Epoch [5705/50000], Train Loss: 104.2327, Test Loss: 72.6998\n",
      "Epoch [5710/50000], Train Loss: 209.3477, Test Loss: 79.8286\n",
      "Epoch [5715/50000], Train Loss: 107.6083, Test Loss: 88.1260\n",
      "Epoch [5720/50000], Train Loss: 123.9420, Test Loss: 87.4934\n",
      "Epoch [5725/50000], Train Loss: 105.8545, Test Loss: 74.4197\n",
      "Epoch [5730/50000], Train Loss: 128.4827, Test Loss: 67.3033\n",
      "Epoch [5735/50000], Train Loss: 106.3404, Test Loss: 74.1486\n",
      "Epoch [5740/50000], Train Loss: 93.6321, Test Loss: 80.5788\n",
      "Epoch [5745/50000], Train Loss: 122.0359, Test Loss: 78.5628\n",
      "Epoch [5750/50000], Train Loss: 103.4703, Test Loss: 65.5554\n",
      "Epoch [5755/50000], Train Loss: 112.7788, Test Loss: 71.8595\n",
      "Epoch [5760/50000], Train Loss: 105.1044, Test Loss: 78.2671\n",
      "Epoch [5765/50000], Train Loss: 184.1815, Test Loss: 74.3774\n",
      "Epoch [5770/50000], Train Loss: 121.0397, Test Loss: 68.8162\n",
      "Epoch [5775/50000], Train Loss: 121.9502, Test Loss: 73.7372\n",
      "Epoch [5780/50000], Train Loss: 111.2500, Test Loss: 67.1868\n",
      "Epoch [5785/50000], Train Loss: 159.4071, Test Loss: 68.6439\n",
      "Epoch [5790/50000], Train Loss: 98.4802, Test Loss: 71.4865\n",
      "Epoch [5795/50000], Train Loss: 110.7513, Test Loss: 69.2697\n",
      "Epoch [5800/50000], Train Loss: 92.6357, Test Loss: 67.1018\n",
      "Epoch [5805/50000], Train Loss: 107.6902, Test Loss: 91.9540\n",
      "Epoch [5810/50000], Train Loss: 90.8867, Test Loss: 65.6735\n",
      "Epoch [5815/50000], Train Loss: 114.3961, Test Loss: 76.0549\n",
      "Epoch [5820/50000], Train Loss: 141.0288, Test Loss: 69.1741\n",
      "Epoch [5825/50000], Train Loss: 111.1273, Test Loss: 86.2432\n",
      "Epoch [5830/50000], Train Loss: 118.1804, Test Loss: 71.8276\n",
      "Epoch [5835/50000], Train Loss: 89.4627, Test Loss: 68.6818\n",
      "Epoch [5840/50000], Train Loss: 182.4445, Test Loss: 84.6586\n",
      "Epoch [5845/50000], Train Loss: 98.1125, Test Loss: 74.9470\n",
      "Epoch [5850/50000], Train Loss: 117.3523, Test Loss: 77.7820\n",
      "Epoch [5855/50000], Train Loss: 111.7682, Test Loss: 100.8143\n",
      "Epoch [5860/50000], Train Loss: 96.3687, Test Loss: 70.0974\n",
      "Epoch [5865/50000], Train Loss: 99.6051, Test Loss: 80.8804\n",
      "Epoch [5870/50000], Train Loss: 101.3115, Test Loss: 83.9026\n",
      "Epoch [5875/50000], Train Loss: 119.9750, Test Loss: 67.5328\n",
      "Epoch [5880/50000], Train Loss: 106.3951, Test Loss: 74.8175\n",
      "Epoch [5885/50000], Train Loss: 113.5236, Test Loss: 67.0867\n",
      "Epoch [5890/50000], Train Loss: 101.5887, Test Loss: 75.2465\n",
      "Epoch [5895/50000], Train Loss: 98.6833, Test Loss: 99.7293\n",
      "Epoch [5900/50000], Train Loss: 130.5402, Test Loss: 67.2120\n",
      "Epoch [5905/50000], Train Loss: 182.2306, Test Loss: 85.6286\n",
      "Epoch [5910/50000], Train Loss: 98.4307, Test Loss: 69.2273\n",
      "Epoch [5915/50000], Train Loss: 92.8485, Test Loss: 66.2301\n",
      "Epoch [5920/50000], Train Loss: 100.8471, Test Loss: 68.7998\n",
      "Epoch [5925/50000], Train Loss: 93.0169, Test Loss: 76.3908\n",
      "Epoch [5930/50000], Train Loss: 95.7861, Test Loss: 64.3034\n",
      "Epoch [5935/50000], Train Loss: 131.0518, Test Loss: 75.0849\n",
      "Epoch [5940/50000], Train Loss: 100.8661, Test Loss: 72.2205\n",
      "Epoch [5945/50000], Train Loss: 91.0018, Test Loss: 68.5172\n",
      "Epoch [5950/50000], Train Loss: 108.0089, Test Loss: 70.4562\n",
      "Epoch [5955/50000], Train Loss: 84.8519, Test Loss: 78.3911\n",
      "Epoch [5960/50000], Train Loss: 106.4040, Test Loss: 70.9553\n",
      "Epoch [5965/50000], Train Loss: 149.4312, Test Loss: 65.1642\n",
      "Epoch [5970/50000], Train Loss: 113.5326, Test Loss: 67.0123\n",
      "Epoch [5975/50000], Train Loss: 115.2901, Test Loss: 69.9735\n",
      "Epoch [5980/50000], Train Loss: 106.9568, Test Loss: 68.3793\n",
      "Epoch [5985/50000], Train Loss: 113.3414, Test Loss: 65.1709\n",
      "Epoch [5990/50000], Train Loss: 86.5789, Test Loss: 74.6583\n",
      "Epoch [5995/50000], Train Loss: 152.2471, Test Loss: 64.3046\n",
      "Epoch [6000/50000], Train Loss: 147.7531, Test Loss: 121.1387\n",
      "Epoch [6005/50000], Train Loss: 113.4722, Test Loss: 77.0295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6010/50000], Train Loss: 112.3950, Test Loss: 69.0019\n",
      "Epoch [6015/50000], Train Loss: 111.5951, Test Loss: 69.7880\n",
      "Epoch [6020/50000], Train Loss: 106.2789, Test Loss: 70.3139\n",
      "Epoch [6025/50000], Train Loss: 163.7331, Test Loss: 71.1928\n",
      "Epoch [6030/50000], Train Loss: 99.7605, Test Loss: 76.9652\n",
      "Epoch [6035/50000], Train Loss: 105.5994, Test Loss: 68.2135\n",
      "Epoch [6040/50000], Train Loss: 100.4494, Test Loss: 75.2899\n",
      "Epoch [6045/50000], Train Loss: 102.2360, Test Loss: 68.1097\n",
      "Epoch [6050/50000], Train Loss: 90.2704, Test Loss: 71.5773\n",
      "Epoch [6055/50000], Train Loss: 102.6050, Test Loss: 67.4554\n",
      "Epoch [6060/50000], Train Loss: 103.4108, Test Loss: 65.0722\n",
      "Epoch [6065/50000], Train Loss: 106.2096, Test Loss: 70.6293\n",
      "Epoch [6070/50000], Train Loss: 100.9013, Test Loss: 108.1345\n",
      "Epoch [6075/50000], Train Loss: 100.3580, Test Loss: 64.4760\n",
      "Epoch [6080/50000], Train Loss: 102.9745, Test Loss: 66.4196\n",
      "Epoch [6085/50000], Train Loss: 85.4191, Test Loss: 64.3296\n",
      "Epoch [6090/50000], Train Loss: 112.5835, Test Loss: 67.8849\n",
      "Epoch [6095/50000], Train Loss: 147.0494, Test Loss: 196.8294\n",
      "Epoch [6100/50000], Train Loss: 98.7966, Test Loss: 75.8051\n",
      "Epoch [6105/50000], Train Loss: 112.1639, Test Loss: 74.2306\n",
      "Epoch [6110/50000], Train Loss: 88.9088, Test Loss: 84.1590\n",
      "Epoch [6115/50000], Train Loss: 116.3626, Test Loss: 77.2845\n",
      "Epoch [6120/50000], Train Loss: 118.2412, Test Loss: 66.0950\n",
      "Epoch [6125/50000], Train Loss: 103.2188, Test Loss: 82.5081\n",
      "Epoch [6130/50000], Train Loss: 119.4648, Test Loss: 179.6158\n",
      "Epoch [6135/50000], Train Loss: 99.8973, Test Loss: 64.0819\n",
      "Epoch [6140/50000], Train Loss: 90.5227, Test Loss: 83.3097\n",
      "Epoch [6145/50000], Train Loss: 100.5195, Test Loss: 84.3506\n",
      "Epoch [6150/50000], Train Loss: 89.7373, Test Loss: 61.8694\n",
      "Epoch [6155/50000], Train Loss: 105.9128, Test Loss: 68.9588\n",
      "Epoch [6160/50000], Train Loss: 83.5571, Test Loss: 64.7690\n",
      "Epoch [6165/50000], Train Loss: 115.6133, Test Loss: 88.1272\n",
      "Epoch [6170/50000], Train Loss: 154.0015, Test Loss: 360.2275\n",
      "Epoch [6175/50000], Train Loss: 110.9884, Test Loss: 66.1237\n",
      "Epoch [6180/50000], Train Loss: 139.0638, Test Loss: 71.3119\n",
      "Epoch [6185/50000], Train Loss: 83.5648, Test Loss: 72.0647\n",
      "Epoch [6190/50000], Train Loss: 105.6699, Test Loss: 67.7802\n",
      "Epoch [6195/50000], Train Loss: 147.8942, Test Loss: 69.3789\n",
      "Epoch [6200/50000], Train Loss: 98.0969, Test Loss: 91.2611\n",
      "Epoch [6205/50000], Train Loss: 102.6752, Test Loss: 64.4412\n",
      "Epoch [6210/50000], Train Loss: 143.7410, Test Loss: 83.2509\n",
      "Epoch [6215/50000], Train Loss: 121.8446, Test Loss: 68.0563\n",
      "Epoch [6220/50000], Train Loss: 86.7115, Test Loss: 68.9035\n",
      "Epoch [6225/50000], Train Loss: 109.1669, Test Loss: 65.9410\n",
      "Epoch [6230/50000], Train Loss: 110.6894, Test Loss: 68.4926\n",
      "Epoch [6235/50000], Train Loss: 108.6100, Test Loss: 71.7037\n",
      "Epoch [6240/50000], Train Loss: 143.8627, Test Loss: 68.7957\n",
      "Epoch [6245/50000], Train Loss: 96.3009, Test Loss: 64.0015\n",
      "Epoch [6250/50000], Train Loss: 143.8230, Test Loss: 67.7263\n",
      "Epoch [6255/50000], Train Loss: 96.9506, Test Loss: 68.4098\n",
      "Epoch [6260/50000], Train Loss: 379.4700, Test Loss: 105.5492\n",
      "Epoch [6265/50000], Train Loss: 101.3023, Test Loss: 85.8515\n",
      "Epoch [6270/50000], Train Loss: 89.2107, Test Loss: 79.2233\n",
      "Epoch [6275/50000], Train Loss: 89.8159, Test Loss: 65.1116\n",
      "Epoch [6280/50000], Train Loss: 87.4838, Test Loss: 65.9927\n",
      "Epoch [6285/50000], Train Loss: 140.8755, Test Loss: 68.0292\n",
      "Epoch [6290/50000], Train Loss: 123.4055, Test Loss: 72.2253\n",
      "Epoch [6295/50000], Train Loss: 108.1326, Test Loss: 66.9214\n",
      "Epoch [6300/50000], Train Loss: 104.5223, Test Loss: 63.1268\n",
      "Epoch [6305/50000], Train Loss: 168.1659, Test Loss: 84.7520\n",
      "Epoch [6310/50000], Train Loss: 107.9710, Test Loss: 69.3197\n",
      "Epoch [6315/50000], Train Loss: 157.9460, Test Loss: 71.4684\n",
      "Epoch [6320/50000], Train Loss: 86.6888, Test Loss: 63.6892\n",
      "Epoch [6325/50000], Train Loss: 181.3168, Test Loss: 73.3985\n",
      "Epoch [6330/50000], Train Loss: 160.6245, Test Loss: 69.1187\n",
      "Epoch [6335/50000], Train Loss: 110.9904, Test Loss: 255.1306\n",
      "Epoch [6340/50000], Train Loss: 102.1407, Test Loss: 64.3916\n",
      "Epoch [6345/50000], Train Loss: 80.3285, Test Loss: 76.5661\n",
      "Epoch [6350/50000], Train Loss: 78.8562, Test Loss: 66.8111\n",
      "Epoch [6355/50000], Train Loss: 102.5304, Test Loss: 63.8258\n",
      "Epoch [6360/50000], Train Loss: 96.0738, Test Loss: 71.4927\n",
      "Epoch [6365/50000], Train Loss: 114.8510, Test Loss: 133.1024\n",
      "Epoch [6370/50000], Train Loss: 107.7591, Test Loss: 72.2096\n",
      "Epoch [6375/50000], Train Loss: 112.2461, Test Loss: 65.6000\n",
      "Epoch [6380/50000], Train Loss: 88.8267, Test Loss: 63.9557\n",
      "Epoch [6385/50000], Train Loss: 91.1133, Test Loss: 68.2404\n",
      "Epoch [6390/50000], Train Loss: 100.9902, Test Loss: 69.9738\n",
      "Epoch [6395/50000], Train Loss: 97.6023, Test Loss: 77.0206\n",
      "Epoch [6400/50000], Train Loss: 93.0951, Test Loss: 65.6930\n",
      "Epoch [6405/50000], Train Loss: 95.1493, Test Loss: 66.8579\n",
      "Epoch [6410/50000], Train Loss: 73.6762, Test Loss: 70.6894\n",
      "Epoch [6415/50000], Train Loss: 137.5005, Test Loss: 66.8258\n",
      "Epoch [6420/50000], Train Loss: 101.4776, Test Loss: 85.9908\n",
      "Epoch [6425/50000], Train Loss: 100.5494, Test Loss: 79.8265\n",
      "Epoch [6430/50000], Train Loss: 95.4509, Test Loss: 68.8220\n",
      "Epoch [6435/50000], Train Loss: 85.7560, Test Loss: 65.6160\n",
      "Epoch [6440/50000], Train Loss: 92.8338, Test Loss: 63.0840\n",
      "Epoch [6445/50000], Train Loss: 87.9996, Test Loss: 64.5676\n",
      "Epoch [6450/50000], Train Loss: 97.0481, Test Loss: 75.7548\n",
      "Epoch [6455/50000], Train Loss: 542.4618, Test Loss: 67.9979\n",
      "Epoch [6460/50000], Train Loss: 93.7082, Test Loss: 72.5617\n",
      "Epoch [6465/50000], Train Loss: 105.6497, Test Loss: 62.6549\n",
      "Epoch [6470/50000], Train Loss: 86.5447, Test Loss: 64.5147\n",
      "Epoch [6475/50000], Train Loss: 108.6865, Test Loss: 64.9796\n",
      "Epoch [6480/50000], Train Loss: 92.5446, Test Loss: 66.3416\n",
      "Epoch [6485/50000], Train Loss: 323.4858, Test Loss: 62.4718\n",
      "Epoch [6490/50000], Train Loss: 98.5733, Test Loss: 68.0053\n",
      "Epoch [6495/50000], Train Loss: 122.3533, Test Loss: 65.2943\n",
      "Epoch [6500/50000], Train Loss: 168.0891, Test Loss: 71.8370\n",
      "Epoch [6505/50000], Train Loss: 87.9553, Test Loss: 80.2201\n",
      "Epoch [6510/50000], Train Loss: 83.5962, Test Loss: 72.1006\n",
      "Epoch [6515/50000], Train Loss: 83.1578, Test Loss: 89.1064\n",
      "Epoch [6520/50000], Train Loss: 106.2787, Test Loss: 76.9791\n",
      "Epoch [6525/50000], Train Loss: 99.8705, Test Loss: 78.3280\n",
      "Epoch [6530/50000], Train Loss: 96.8220, Test Loss: 73.5441\n",
      "Epoch [6535/50000], Train Loss: 80.6436, Test Loss: 82.2324\n",
      "Epoch [6540/50000], Train Loss: 88.6613, Test Loss: 87.1472\n",
      "Epoch [6545/50000], Train Loss: 102.4112, Test Loss: 79.5373\n",
      "Epoch [6550/50000], Train Loss: 121.4034, Test Loss: 76.4450\n",
      "Epoch [6555/50000], Train Loss: 70.5506, Test Loss: 63.3981\n",
      "Epoch [6560/50000], Train Loss: 98.4221, Test Loss: 68.4268\n",
      "Epoch [6565/50000], Train Loss: 91.0060, Test Loss: 68.4528\n",
      "Epoch [6570/50000], Train Loss: 94.9729, Test Loss: 63.5238\n",
      "Epoch [6575/50000], Train Loss: 104.8660, Test Loss: 65.2137\n",
      "Epoch [6580/50000], Train Loss: 113.8451, Test Loss: 72.0367\n",
      "Epoch [6585/50000], Train Loss: 119.0288, Test Loss: 63.8599\n",
      "Epoch [6590/50000], Train Loss: 86.6213, Test Loss: 76.5584\n",
      "Epoch [6595/50000], Train Loss: 163.9416, Test Loss: 85.7753\n",
      "Epoch [6600/50000], Train Loss: 155.5563, Test Loss: 64.3233\n",
      "Epoch [6605/50000], Train Loss: 83.5529, Test Loss: 63.1143\n",
      "Epoch [6610/50000], Train Loss: 97.3953, Test Loss: 63.4339\n",
      "Epoch [6615/50000], Train Loss: 82.2270, Test Loss: 63.8274\n",
      "Epoch [6620/50000], Train Loss: 391.3303, Test Loss: 71.3769\n",
      "Epoch [6625/50000], Train Loss: 97.0589, Test Loss: 74.5504\n",
      "Epoch [6630/50000], Train Loss: 90.8274, Test Loss: 63.0010\n",
      "Epoch [6635/50000], Train Loss: 92.7951, Test Loss: 70.5889\n",
      "Epoch [6640/50000], Train Loss: 104.9372, Test Loss: 204.5436\n",
      "Epoch [6645/50000], Train Loss: 103.0150, Test Loss: 64.3137\n",
      "Epoch [6650/50000], Train Loss: 103.2351, Test Loss: 103.4426\n",
      "Epoch [6655/50000], Train Loss: 88.3635, Test Loss: 67.4241\n",
      "Epoch [6660/50000], Train Loss: 92.6362, Test Loss: 77.9039\n",
      "Epoch [6665/50000], Train Loss: 97.0973, Test Loss: 67.3391\n",
      "Epoch [6670/50000], Train Loss: 90.5659, Test Loss: 62.5463\n",
      "Epoch [6675/50000], Train Loss: 75.5105, Test Loss: 62.0555\n",
      "Epoch [6680/50000], Train Loss: 95.7889, Test Loss: 62.8971\n",
      "Epoch [6685/50000], Train Loss: 79.0692, Test Loss: 66.9503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6690/50000], Train Loss: 113.6498, Test Loss: 79.0343\n",
      "Epoch [6695/50000], Train Loss: 99.8393, Test Loss: 101.3737\n",
      "Epoch [6700/50000], Train Loss: 82.8635, Test Loss: 63.8966\n",
      "Epoch [6705/50000], Train Loss: 105.5837, Test Loss: 149.5853\n",
      "Epoch [6710/50000], Train Loss: 109.4891, Test Loss: 68.2767\n",
      "Epoch [6715/50000], Train Loss: 78.7155, Test Loss: 70.3888\n",
      "Epoch [6720/50000], Train Loss: 70.4486, Test Loss: 75.0806\n",
      "Epoch [6725/50000], Train Loss: 73.9031, Test Loss: 65.9819\n",
      "Epoch [6730/50000], Train Loss: 101.2134, Test Loss: 65.9985\n",
      "Epoch [6735/50000], Train Loss: 86.8050, Test Loss: 66.0852\n",
      "Epoch [6740/50000], Train Loss: 93.4152, Test Loss: 68.0802\n",
      "Epoch [6745/50000], Train Loss: 83.0773, Test Loss: 62.6228\n",
      "Epoch [6750/50000], Train Loss: 185.3471, Test Loss: 59.7122\n",
      "Epoch [6755/50000], Train Loss: 85.7074, Test Loss: 64.3261\n",
      "Epoch [6760/50000], Train Loss: 71.4139, Test Loss: 61.5153\n",
      "Epoch [6765/50000], Train Loss: 98.8356, Test Loss: 64.3989\n",
      "Epoch [6770/50000], Train Loss: 75.4201, Test Loss: 85.3942\n",
      "Epoch [6775/50000], Train Loss: 131.5673, Test Loss: 63.2099\n",
      "Epoch [6780/50000], Train Loss: 82.2004, Test Loss: 69.1793\n",
      "Epoch [6785/50000], Train Loss: 79.1378, Test Loss: 75.7890\n",
      "Epoch [6790/50000], Train Loss: 196.4191, Test Loss: 65.2676\n",
      "Epoch [6795/50000], Train Loss: 114.1288, Test Loss: 65.0905\n",
      "Epoch [6800/50000], Train Loss: 102.7214, Test Loss: 78.6147\n",
      "Epoch [6805/50000], Train Loss: 85.7232, Test Loss: 68.5566\n",
      "Epoch [6810/50000], Train Loss: 75.2431, Test Loss: 60.3159\n",
      "Epoch [6815/50000], Train Loss: 114.5584, Test Loss: 73.6758\n",
      "Epoch [6820/50000], Train Loss: 82.0294, Test Loss: 67.7680\n",
      "Epoch [6825/50000], Train Loss: 111.0185, Test Loss: 69.8546\n",
      "Epoch [6830/50000], Train Loss: 98.5782, Test Loss: 64.9049\n",
      "Epoch [6835/50000], Train Loss: 87.9982, Test Loss: 61.3013\n",
      "Epoch [6840/50000], Train Loss: 92.7087, Test Loss: 61.0572\n",
      "Epoch [6845/50000], Train Loss: 104.8355, Test Loss: 66.9314\n",
      "Epoch [6850/50000], Train Loss: 99.1462, Test Loss: 60.6523\n",
      "Epoch [6855/50000], Train Loss: 113.7376, Test Loss: 61.4388\n",
      "Epoch [6860/50000], Train Loss: 65.0750, Test Loss: 66.2371\n",
      "Epoch [6865/50000], Train Loss: 78.8562, Test Loss: 69.6442\n",
      "Epoch [6870/50000], Train Loss: 100.5626, Test Loss: 70.8106\n",
      "Epoch [6875/50000], Train Loss: 216.3886, Test Loss: 86.5204\n",
      "Epoch [6880/50000], Train Loss: 91.6315, Test Loss: 68.2972\n",
      "Epoch [6885/50000], Train Loss: 81.1572, Test Loss: 70.6280\n",
      "Epoch [6890/50000], Train Loss: 72.6336, Test Loss: 65.7954\n",
      "Epoch [6895/50000], Train Loss: 93.7462, Test Loss: 71.3798\n",
      "Epoch [6900/50000], Train Loss: 70.2994, Test Loss: 60.6627\n",
      "Epoch [6905/50000], Train Loss: 102.1425, Test Loss: 64.3990\n",
      "Epoch [6910/50000], Train Loss: 121.1777, Test Loss: 63.5377\n",
      "Epoch [6915/50000], Train Loss: 83.1095, Test Loss: 63.5067\n",
      "Epoch [6920/50000], Train Loss: 108.1124, Test Loss: 69.4678\n",
      "Epoch [6925/50000], Train Loss: 119.6731, Test Loss: 63.5635\n",
      "Epoch [6930/50000], Train Loss: 196.4402, Test Loss: 67.9648\n",
      "Epoch [6935/50000], Train Loss: 88.6815, Test Loss: 72.1010\n",
      "Epoch [6940/50000], Train Loss: 96.5706, Test Loss: 64.9510\n",
      "Epoch [6945/50000], Train Loss: 64.6893, Test Loss: 68.7519\n",
      "Epoch [6950/50000], Train Loss: 92.0274, Test Loss: 67.3084\n",
      "Epoch [6955/50000], Train Loss: 81.7932, Test Loss: 62.5548\n",
      "Epoch [6960/50000], Train Loss: 85.8088, Test Loss: 68.2283\n",
      "Epoch [6965/50000], Train Loss: 85.7966, Test Loss: 61.9980\n",
      "Epoch [6970/50000], Train Loss: 189.7544, Test Loss: 61.2502\n",
      "Epoch [6975/50000], Train Loss: 65.1810, Test Loss: 71.1233\n",
      "Epoch [6980/50000], Train Loss: 93.3993, Test Loss: 79.5177\n",
      "Epoch [6985/50000], Train Loss: 113.6625, Test Loss: 60.0713\n",
      "Epoch [6990/50000], Train Loss: 178.5897, Test Loss: 69.3126\n",
      "Epoch [6995/50000], Train Loss: 129.7651, Test Loss: 68.4446\n",
      "Epoch [7000/50000], Train Loss: 82.3976, Test Loss: 60.5566\n",
      "Epoch [7005/50000], Train Loss: 77.9790, Test Loss: 65.1670\n",
      "Epoch [7010/50000], Train Loss: 106.7946, Test Loss: 70.0094\n",
      "Epoch [7015/50000], Train Loss: 86.1823, Test Loss: 64.2534\n",
      "Epoch [7020/50000], Train Loss: 93.9344, Test Loss: 89.2895\n",
      "Epoch [7025/50000], Train Loss: 67.0417, Test Loss: 58.9250\n",
      "Epoch [7030/50000], Train Loss: 87.3452, Test Loss: 72.0250\n",
      "Epoch [7035/50000], Train Loss: 91.0178, Test Loss: 63.9743\n",
      "Epoch [7040/50000], Train Loss: 71.2724, Test Loss: 61.5099\n",
      "Epoch [7045/50000], Train Loss: 102.2616, Test Loss: 69.1728\n",
      "Epoch [7050/50000], Train Loss: 72.6545, Test Loss: 67.8811\n",
      "Epoch [7055/50000], Train Loss: 154.5740, Test Loss: 144.9768\n",
      "Epoch [7060/50000], Train Loss: 89.1067, Test Loss: 58.9793\n",
      "Epoch [7065/50000], Train Loss: 112.2027, Test Loss: 153.5013\n",
      "Epoch [7070/50000], Train Loss: 87.0827, Test Loss: 63.9204\n",
      "Epoch [7075/50000], Train Loss: 85.2538, Test Loss: 75.5345\n",
      "Epoch [7080/50000], Train Loss: 87.0539, Test Loss: 76.6496\n",
      "Epoch [7085/50000], Train Loss: 84.8719, Test Loss: 70.3118\n",
      "Epoch [7090/50000], Train Loss: 91.6216, Test Loss: 66.4681\n",
      "Epoch [7095/50000], Train Loss: 188.3838, Test Loss: 77.1667\n",
      "Epoch [7100/50000], Train Loss: 164.5349, Test Loss: 64.3327\n",
      "Epoch [7105/50000], Train Loss: 107.9989, Test Loss: 67.4076\n",
      "Epoch [7110/50000], Train Loss: 93.5243, Test Loss: 61.8675\n",
      "Epoch [7115/50000], Train Loss: 86.2243, Test Loss: 59.2267\n",
      "Epoch [7120/50000], Train Loss: 206.2867, Test Loss: 64.1321\n",
      "Epoch [7125/50000], Train Loss: 95.2376, Test Loss: 60.7480\n",
      "Epoch [7130/50000], Train Loss: 82.1557, Test Loss: 70.7275\n",
      "Epoch [7135/50000], Train Loss: 84.4425, Test Loss: 66.3698\n",
      "Epoch [7140/50000], Train Loss: 101.4368, Test Loss: 71.6162\n",
      "Epoch [7145/50000], Train Loss: 102.1176, Test Loss: 65.7088\n",
      "Epoch [7150/50000], Train Loss: 82.8744, Test Loss: 66.6363\n",
      "Epoch [7155/50000], Train Loss: 88.0052, Test Loss: 65.7814\n",
      "Epoch [7160/50000], Train Loss: 146.5529, Test Loss: 59.8751\n",
      "Epoch [7165/50000], Train Loss: 115.0307, Test Loss: 60.0731\n",
      "Epoch [7170/50000], Train Loss: 120.5545, Test Loss: 68.8268\n",
      "Epoch [7175/50000], Train Loss: 120.2051, Test Loss: 70.4249\n",
      "Epoch [7180/50000], Train Loss: 123.1663, Test Loss: 61.5183\n",
      "Epoch [7185/50000], Train Loss: 158.4975, Test Loss: 70.2538\n",
      "Epoch [7190/50000], Train Loss: 248.8487, Test Loss: 58.9605\n",
      "Epoch [7195/50000], Train Loss: 237.6995, Test Loss: 68.9480\n",
      "Epoch [7200/50000], Train Loss: 95.0291, Test Loss: 63.9391\n",
      "Epoch [7205/50000], Train Loss: 115.4866, Test Loss: 59.7653\n",
      "Epoch [7210/50000], Train Loss: 63.8914, Test Loss: 63.4275\n",
      "Epoch [7215/50000], Train Loss: 89.8049, Test Loss: 62.7993\n",
      "Epoch [7220/50000], Train Loss: 264.7176, Test Loss: 93.2341\n",
      "Epoch [7225/50000], Train Loss: 97.2167, Test Loss: 59.3536\n",
      "Epoch [7230/50000], Train Loss: 95.3404, Test Loss: 63.8278\n",
      "Epoch [7235/50000], Train Loss: 99.5502, Test Loss: 67.1628\n",
      "Epoch [7240/50000], Train Loss: 77.6445, Test Loss: 68.6213\n",
      "Epoch [7245/50000], Train Loss: 85.4963, Test Loss: 64.3421\n",
      "Epoch [7250/50000], Train Loss: 68.0465, Test Loss: 59.3229\n",
      "Epoch [7255/50000], Train Loss: 93.0481, Test Loss: 69.6495\n",
      "Epoch [7260/50000], Train Loss: 82.2652, Test Loss: 67.9650\n",
      "Epoch [7265/50000], Train Loss: 80.1166, Test Loss: 66.1493\n",
      "Epoch [7270/50000], Train Loss: 110.0270, Test Loss: 66.8508\n",
      "Epoch [7275/50000], Train Loss: 79.9582, Test Loss: 100.7859\n",
      "Epoch [7280/50000], Train Loss: 138.3256, Test Loss: 59.3714\n",
      "Epoch [7285/50000], Train Loss: 77.7857, Test Loss: 59.9736\n",
      "Epoch [7290/50000], Train Loss: 84.3814, Test Loss: 69.2856\n",
      "Epoch [7295/50000], Train Loss: 74.2481, Test Loss: 68.2001\n",
      "Epoch [7300/50000], Train Loss: 86.9033, Test Loss: 89.6428\n",
      "Epoch [7305/50000], Train Loss: 160.4674, Test Loss: 205.2257\n",
      "Epoch [7310/50000], Train Loss: 76.8367, Test Loss: 74.2953\n",
      "Epoch [7315/50000], Train Loss: 130.5245, Test Loss: 61.1581\n",
      "Epoch [7320/50000], Train Loss: 208.8172, Test Loss: 59.8791\n",
      "Epoch [7325/50000], Train Loss: 74.8664, Test Loss: 65.3361\n",
      "Epoch [7330/50000], Train Loss: 87.6418, Test Loss: 85.8676\n",
      "Epoch [7335/50000], Train Loss: 82.3261, Test Loss: 70.6585\n",
      "Epoch [7340/50000], Train Loss: 88.4939, Test Loss: 70.6904\n",
      "Epoch [7345/50000], Train Loss: 81.2199, Test Loss: 74.4500\n",
      "Epoch [7350/50000], Train Loss: 60.6195, Test Loss: 61.4285\n",
      "Epoch [7355/50000], Train Loss: 126.0132, Test Loss: 59.5164\n",
      "Epoch [7360/50000], Train Loss: 84.6385, Test Loss: 78.2479\n",
      "Epoch [7365/50000], Train Loss: 86.2190, Test Loss: 137.5253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7370/50000], Train Loss: 88.1890, Test Loss: 65.7655\n",
      "Epoch [7375/50000], Train Loss: 173.0877, Test Loss: 66.1490\n",
      "Epoch [7380/50000], Train Loss: 75.4394, Test Loss: 67.1157\n",
      "Epoch [7385/50000], Train Loss: 65.6634, Test Loss: 60.0273\n",
      "Epoch [7390/50000], Train Loss: 85.4142, Test Loss: 66.0925\n",
      "Epoch [7395/50000], Train Loss: 103.2436, Test Loss: 63.1136\n",
      "Epoch [7400/50000], Train Loss: 84.3929, Test Loss: 66.8092\n",
      "Epoch [7405/50000], Train Loss: 77.0562, Test Loss: 66.2539\n",
      "Epoch [7410/50000], Train Loss: 81.8669, Test Loss: 60.0899\n",
      "Epoch [7415/50000], Train Loss: 106.4000, Test Loss: 94.3566\n",
      "Epoch [7420/50000], Train Loss: 85.6274, Test Loss: 59.5040\n",
      "Epoch [7425/50000], Train Loss: 105.4616, Test Loss: 59.2590\n",
      "Epoch [7430/50000], Train Loss: 61.7560, Test Loss: 61.4520\n",
      "Epoch [7435/50000], Train Loss: 81.7212, Test Loss: 62.3924\n",
      "Epoch [7440/50000], Train Loss: 89.2009, Test Loss: 62.7333\n",
      "Epoch [7445/50000], Train Loss: 72.1126, Test Loss: 74.0218\n",
      "Epoch [7450/50000], Train Loss: 58.8289, Test Loss: 60.0963\n",
      "Epoch [7455/50000], Train Loss: 78.9010, Test Loss: 68.6965\n",
      "Epoch [7460/50000], Train Loss: 179.1870, Test Loss: 60.6775\n",
      "Epoch [7465/50000], Train Loss: 81.5009, Test Loss: 60.7405\n",
      "Epoch [7470/50000], Train Loss: 83.7996, Test Loss: 84.2144\n",
      "Epoch [7475/50000], Train Loss: 119.6276, Test Loss: 205.8371\n",
      "Epoch [7480/50000], Train Loss: 77.7625, Test Loss: 82.0704\n",
      "Epoch [7485/50000], Train Loss: 80.6500, Test Loss: 60.5521\n",
      "Epoch [7490/50000], Train Loss: 74.5683, Test Loss: 64.6264\n",
      "Epoch [7495/50000], Train Loss: 78.9405, Test Loss: 61.8494\n",
      "Epoch [7500/50000], Train Loss: 73.3125, Test Loss: 63.1102\n",
      "Epoch [7505/50000], Train Loss: 61.7265, Test Loss: 60.4719\n",
      "Epoch [7510/50000], Train Loss: 68.8454, Test Loss: 73.3351\n",
      "Epoch [7515/50000], Train Loss: 84.1642, Test Loss: 60.9578\n",
      "Epoch [7520/50000], Train Loss: 93.6100, Test Loss: 63.7710\n",
      "Epoch [7525/50000], Train Loss: 95.4980, Test Loss: 80.9597\n",
      "Epoch [7530/50000], Train Loss: 70.0830, Test Loss: 59.9087\n",
      "Epoch [7535/50000], Train Loss: 90.5002, Test Loss: 68.2917\n",
      "Epoch [7540/50000], Train Loss: 90.9941, Test Loss: 56.8419\n",
      "Epoch [7545/50000], Train Loss: 77.8036, Test Loss: 84.7763\n",
      "Epoch [7550/50000], Train Loss: 90.7871, Test Loss: 64.0636\n",
      "Epoch [7555/50000], Train Loss: 152.5292, Test Loss: 69.2276\n",
      "Epoch [7560/50000], Train Loss: 70.7499, Test Loss: 86.8496\n",
      "Epoch [7565/50000], Train Loss: 89.3886, Test Loss: 61.6396\n",
      "Epoch [7570/50000], Train Loss: 71.8573, Test Loss: 66.0576\n",
      "Epoch [7575/50000], Train Loss: 97.0693, Test Loss: 62.4693\n",
      "Epoch [7580/50000], Train Loss: 66.2019, Test Loss: 67.4145\n",
      "Epoch [7585/50000], Train Loss: 81.2755, Test Loss: 59.5387\n",
      "Epoch [7590/50000], Train Loss: 79.5599, Test Loss: 65.0381\n",
      "Epoch [7595/50000], Train Loss: 86.5588, Test Loss: 70.7894\n",
      "Epoch [7600/50000], Train Loss: 82.7240, Test Loss: 61.7038\n",
      "Epoch [7605/50000], Train Loss: 74.6472, Test Loss: 59.4436\n",
      "Epoch [7610/50000], Train Loss: 109.0750, Test Loss: 69.8723\n",
      "Epoch [7615/50000], Train Loss: 145.3878, Test Loss: 68.8479\n",
      "Epoch [7620/50000], Train Loss: 75.2359, Test Loss: 59.2098\n",
      "Epoch [7625/50000], Train Loss: 91.3809, Test Loss: 61.5155\n",
      "Epoch [7630/50000], Train Loss: 95.3207, Test Loss: 67.8219\n",
      "Epoch [7635/50000], Train Loss: 97.3879, Test Loss: 69.3205\n",
      "Epoch [7640/50000], Train Loss: 94.7189, Test Loss: 98.3143\n",
      "Epoch [7645/50000], Train Loss: 85.6263, Test Loss: 64.3304\n",
      "Epoch [7650/50000], Train Loss: 64.0497, Test Loss: 59.9989\n",
      "Epoch [7655/50000], Train Loss: 172.1029, Test Loss: 63.7695\n",
      "Epoch [7660/50000], Train Loss: 165.4670, Test Loss: 64.3288\n",
      "Epoch [7665/50000], Train Loss: 70.2740, Test Loss: 70.3021\n",
      "Epoch [7670/50000], Train Loss: 86.8552, Test Loss: 64.1007\n",
      "Epoch [7675/50000], Train Loss: 76.3142, Test Loss: 61.9572\n",
      "Epoch [7680/50000], Train Loss: 68.4942, Test Loss: 60.2853\n",
      "Epoch [7685/50000], Train Loss: 87.2873, Test Loss: 66.1788\n",
      "Epoch [7690/50000], Train Loss: 77.0149, Test Loss: 60.1233\n",
      "Epoch [7695/50000], Train Loss: 78.6155, Test Loss: 59.2492\n",
      "Epoch [7700/50000], Train Loss: 65.1521, Test Loss: 60.9706\n",
      "Epoch [7705/50000], Train Loss: 72.3734, Test Loss: 58.6375\n",
      "Epoch [7710/50000], Train Loss: 119.3657, Test Loss: 70.5742\n",
      "Epoch [7715/50000], Train Loss: 93.1499, Test Loss: 68.0767\n",
      "Epoch [7720/50000], Train Loss: 81.2473, Test Loss: 60.6944\n",
      "Epoch [7725/50000], Train Loss: 130.0165, Test Loss: 60.2780\n",
      "Epoch [7730/50000], Train Loss: 78.4691, Test Loss: 79.5773\n",
      "Epoch [7735/50000], Train Loss: 133.7692, Test Loss: 61.8565\n",
      "Epoch [7740/50000], Train Loss: 151.4735, Test Loss: 66.1466\n",
      "Epoch [7745/50000], Train Loss: 64.4987, Test Loss: 57.0789\n",
      "Epoch [7750/50000], Train Loss: 187.4488, Test Loss: 99.8775\n",
      "Epoch [7755/50000], Train Loss: 80.3983, Test Loss: 64.2146\n",
      "Epoch [7760/50000], Train Loss: 185.1184, Test Loss: 56.8419\n",
      "Epoch [7765/50000], Train Loss: 81.4863, Test Loss: 73.5017\n",
      "Epoch [7770/50000], Train Loss: 527.1421, Test Loss: 60.3601\n",
      "Epoch [7775/50000], Train Loss: 78.9090, Test Loss: 71.4342\n",
      "Epoch [7780/50000], Train Loss: 75.8617, Test Loss: 67.8002\n",
      "Epoch [7785/50000], Train Loss: 67.0535, Test Loss: 60.0297\n",
      "Epoch [7790/50000], Train Loss: 73.5304, Test Loss: 62.5937\n",
      "Epoch [7795/50000], Train Loss: 83.7434, Test Loss: 62.0908\n",
      "Epoch [7800/50000], Train Loss: 80.9590, Test Loss: 69.1750\n",
      "Epoch [7805/50000], Train Loss: 109.7221, Test Loss: 62.4662\n",
      "Epoch [7810/50000], Train Loss: 60.3026, Test Loss: 59.8116\n",
      "Epoch [7815/50000], Train Loss: 79.9773, Test Loss: 66.4156\n",
      "Epoch [7820/50000], Train Loss: 71.4768, Test Loss: 62.7227\n",
      "Epoch [7825/50000], Train Loss: 76.1630, Test Loss: 61.5606\n",
      "Epoch [7830/50000], Train Loss: 81.5441, Test Loss: 72.9113\n",
      "Epoch [7835/50000], Train Loss: 199.4243, Test Loss: 61.7919\n",
      "Epoch [7840/50000], Train Loss: 85.7764, Test Loss: 67.7318\n",
      "Epoch [7845/50000], Train Loss: 81.2957, Test Loss: 68.9984\n",
      "Epoch [7850/50000], Train Loss: 71.7791, Test Loss: 61.7225\n",
      "Epoch [7855/50000], Train Loss: 72.9886, Test Loss: 60.9224\n",
      "Epoch [7860/50000], Train Loss: 257.9085, Test Loss: 59.2884\n",
      "Epoch [7865/50000], Train Loss: 72.6243, Test Loss: 58.5198\n",
      "Epoch [7870/50000], Train Loss: 108.9523, Test Loss: 66.0121\n",
      "Epoch [7875/50000], Train Loss: 82.7568, Test Loss: 64.3978\n",
      "Epoch [7880/50000], Train Loss: 131.5862, Test Loss: 62.3664\n",
      "Epoch [7885/50000], Train Loss: 81.1604, Test Loss: 60.7926\n",
      "Epoch [7890/50000], Train Loss: 70.7251, Test Loss: 60.1823\n",
      "Epoch [7895/50000], Train Loss: 82.4122, Test Loss: 62.2566\n",
      "Epoch [7900/50000], Train Loss: 68.1442, Test Loss: 60.5924\n",
      "Epoch [7905/50000], Train Loss: 71.6971, Test Loss: 58.4573\n",
      "Epoch [7910/50000], Train Loss: 189.5062, Test Loss: 60.5708\n",
      "Epoch [7915/50000], Train Loss: 76.7403, Test Loss: 58.9660\n",
      "Epoch [7920/50000], Train Loss: 83.6746, Test Loss: 64.4863\n",
      "Epoch [7925/50000], Train Loss: 75.0272, Test Loss: 64.9365\n",
      "Epoch [7930/50000], Train Loss: 76.7875, Test Loss: 64.5314\n",
      "Epoch [7935/50000], Train Loss: 77.5544, Test Loss: 63.1059\n",
      "Epoch [7940/50000], Train Loss: 77.3202, Test Loss: 67.9658\n",
      "Epoch [7945/50000], Train Loss: 73.3390, Test Loss: 62.1434\n",
      "Epoch [7950/50000], Train Loss: 85.3125, Test Loss: 70.8907\n",
      "Epoch [7955/50000], Train Loss: 100.4420, Test Loss: 74.9900\n",
      "Epoch [7960/50000], Train Loss: 68.6220, Test Loss: 64.6996\n",
      "Epoch [7965/50000], Train Loss: 56.3755, Test Loss: 56.8192\n",
      "Epoch [7970/50000], Train Loss: 82.2776, Test Loss: 61.4501\n",
      "Epoch [7975/50000], Train Loss: 111.8575, Test Loss: 58.3666\n",
      "Epoch [7980/50000], Train Loss: 71.5784, Test Loss: 60.4671\n",
      "Epoch [7985/50000], Train Loss: 138.7302, Test Loss: 59.7233\n",
      "Epoch [7990/50000], Train Loss: 64.6489, Test Loss: 69.6105\n",
      "Epoch [7995/50000], Train Loss: 84.5701, Test Loss: 60.4544\n",
      "Epoch [8000/50000], Train Loss: 74.9976, Test Loss: 63.7413\n",
      "Epoch [8005/50000], Train Loss: 74.0873, Test Loss: 66.5093\n",
      "Epoch [8010/50000], Train Loss: 69.6467, Test Loss: 62.7465\n",
      "Epoch [8015/50000], Train Loss: 78.7949, Test Loss: 61.7695\n",
      "Epoch [8020/50000], Train Loss: 72.3081, Test Loss: 60.4625\n",
      "Epoch [8025/50000], Train Loss: 74.8779, Test Loss: 79.4137\n",
      "Epoch [8030/50000], Train Loss: 82.0012, Test Loss: 69.8967\n",
      "Epoch [8035/50000], Train Loss: 142.4182, Test Loss: 57.9674\n",
      "Epoch [8040/50000], Train Loss: 66.6047, Test Loss: 58.8077\n",
      "Epoch [8045/50000], Train Loss: 81.9257, Test Loss: 62.8775\n",
      "Epoch [8050/50000], Train Loss: 75.8738, Test Loss: 57.8053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8055/50000], Train Loss: 65.8318, Test Loss: 59.0043\n",
      "Epoch [8060/50000], Train Loss: 78.0658, Test Loss: 64.4022\n",
      "Epoch [8065/50000], Train Loss: 84.4366, Test Loss: 81.4004\n",
      "Epoch [8070/50000], Train Loss: 62.7019, Test Loss: 59.5642\n",
      "Epoch [8075/50000], Train Loss: 75.1562, Test Loss: 71.5794\n",
      "Epoch [8080/50000], Train Loss: 87.2909, Test Loss: 57.1446\n",
      "Epoch [8085/50000], Train Loss: 79.8533, Test Loss: 72.2947\n",
      "Epoch [8090/50000], Train Loss: 54.0542, Test Loss: 61.1651\n",
      "Epoch [8095/50000], Train Loss: 75.0868, Test Loss: 68.6554\n",
      "Epoch [8100/50000], Train Loss: 73.5059, Test Loss: 71.7127\n",
      "Epoch [8105/50000], Train Loss: 70.1497, Test Loss: 69.4399\n",
      "Epoch [8110/50000], Train Loss: 91.2580, Test Loss: 55.6308\n",
      "Epoch [8115/50000], Train Loss: 75.1660, Test Loss: 62.2551\n",
      "Epoch [8120/50000], Train Loss: 73.3656, Test Loss: 66.4996\n",
      "Epoch [8125/50000], Train Loss: 71.2439, Test Loss: 61.4345\n",
      "Epoch [8130/50000], Train Loss: 76.4711, Test Loss: 61.7118\n",
      "Epoch [8135/50000], Train Loss: 71.7085, Test Loss: 65.6032\n",
      "Epoch [8140/50000], Train Loss: 77.8743, Test Loss: 73.6601\n",
      "Epoch [8145/50000], Train Loss: 81.2733, Test Loss: 96.4689\n",
      "Epoch [8150/50000], Train Loss: 57.9617, Test Loss: 68.5772\n",
      "Epoch [8155/50000], Train Loss: 61.6805, Test Loss: 58.9535\n",
      "Epoch [8160/50000], Train Loss: 76.9912, Test Loss: 64.7713\n",
      "Epoch [8165/50000], Train Loss: 53.5929, Test Loss: 62.7013\n",
      "Epoch [8170/50000], Train Loss: 65.6488, Test Loss: 62.3052\n",
      "Epoch [8175/50000], Train Loss: 66.9238, Test Loss: 61.0107\n",
      "Epoch [8180/50000], Train Loss: 85.7137, Test Loss: 57.6266\n",
      "Epoch [8185/50000], Train Loss: 88.1258, Test Loss: 61.6592\n",
      "Epoch [8190/50000], Train Loss: 74.9003, Test Loss: 65.3383\n",
      "Epoch [8195/50000], Train Loss: 71.8805, Test Loss: 62.6602\n",
      "Epoch [8200/50000], Train Loss: 110.9218, Test Loss: 150.3404\n",
      "Epoch [8205/50000], Train Loss: 156.3771, Test Loss: 62.1291\n",
      "Epoch [8210/50000], Train Loss: 70.7007, Test Loss: 65.4359\n",
      "Epoch [8215/50000], Train Loss: 93.4044, Test Loss: 57.4514\n",
      "Epoch [8220/50000], Train Loss: 265.3517, Test Loss: 60.7835\n",
      "Epoch [8225/50000], Train Loss: 207.5104, Test Loss: 162.9956\n",
      "Epoch [8230/50000], Train Loss: 114.7038, Test Loss: 57.3158\n",
      "Epoch [8235/50000], Train Loss: 105.3682, Test Loss: 57.7472\n",
      "Epoch [8240/50000], Train Loss: 74.5196, Test Loss: 71.5704\n",
      "Epoch [8245/50000], Train Loss: 84.4844, Test Loss: 68.4975\n",
      "Epoch [8250/50000], Train Loss: 78.5424, Test Loss: 67.5393\n",
      "Epoch [8255/50000], Train Loss: 149.6212, Test Loss: 74.5189\n",
      "Epoch [8260/50000], Train Loss: 75.1605, Test Loss: 63.6862\n",
      "Epoch [8265/50000], Train Loss: 71.9184, Test Loss: 60.4830\n",
      "Epoch [8270/50000], Train Loss: 104.6961, Test Loss: 59.9291\n",
      "Epoch [8275/50000], Train Loss: 63.3956, Test Loss: 64.6172\n",
      "Epoch [8280/50000], Train Loss: 111.1338, Test Loss: 65.1750\n",
      "Epoch [8285/50000], Train Loss: 73.6054, Test Loss: 72.3618\n",
      "Epoch [8290/50000], Train Loss: 78.2715, Test Loss: 69.6712\n",
      "Epoch [8295/50000], Train Loss: 69.1217, Test Loss: 79.8003\n",
      "Epoch [8300/50000], Train Loss: 99.8900, Test Loss: 398.5133\n",
      "Epoch [8305/50000], Train Loss: 64.8056, Test Loss: 61.4479\n",
      "Epoch [8310/50000], Train Loss: 75.3450, Test Loss: 61.7600\n",
      "Epoch [8315/50000], Train Loss: 74.6423, Test Loss: 68.4979\n",
      "Epoch [8320/50000], Train Loss: 72.9771, Test Loss: 63.4565\n",
      "Epoch [8325/50000], Train Loss: 80.4617, Test Loss: 71.7694\n",
      "Epoch [8330/50000], Train Loss: 73.9053, Test Loss: 63.9985\n",
      "Epoch [8335/50000], Train Loss: 87.2285, Test Loss: 65.0142\n",
      "Epoch [8340/50000], Train Loss: 126.9499, Test Loss: 61.6581\n",
      "Epoch [8345/50000], Train Loss: 73.0358, Test Loss: 66.6848\n",
      "Epoch [8350/50000], Train Loss: 75.0947, Test Loss: 64.1875\n",
      "Epoch [8355/50000], Train Loss: 233.6812, Test Loss: 58.0932\n",
      "Epoch [8360/50000], Train Loss: 59.3779, Test Loss: 60.1721\n",
      "Epoch [8365/50000], Train Loss: 66.0928, Test Loss: 58.6147\n",
      "Epoch [8370/50000], Train Loss: 71.7264, Test Loss: 59.6428\n",
      "Epoch [8375/50000], Train Loss: 75.1782, Test Loss: 62.1775\n",
      "Epoch [8380/50000], Train Loss: 76.4715, Test Loss: 60.0674\n",
      "Epoch [8385/50000], Train Loss: 62.9084, Test Loss: 57.7123\n",
      "Epoch [8390/50000], Train Loss: 78.9958, Test Loss: 77.4091\n",
      "Epoch [8395/50000], Train Loss: 76.1999, Test Loss: 69.8884\n",
      "Epoch [8400/50000], Train Loss: 89.1090, Test Loss: 60.8496\n",
      "Epoch [8405/50000], Train Loss: 63.7392, Test Loss: 76.4289\n",
      "Epoch [8410/50000], Train Loss: 72.9129, Test Loss: 63.8374\n",
      "Epoch [8415/50000], Train Loss: 73.4495, Test Loss: 58.7307\n",
      "Epoch [8420/50000], Train Loss: 80.8112, Test Loss: 66.4629\n",
      "Epoch [8425/50000], Train Loss: 69.6800, Test Loss: 62.5239\n",
      "Epoch [8430/50000], Train Loss: 91.9349, Test Loss: 64.5127\n",
      "Epoch [8435/50000], Train Loss: 59.9425, Test Loss: 59.9284\n",
      "Epoch [8440/50000], Train Loss: 104.0257, Test Loss: 65.8297\n",
      "Epoch [8445/50000], Train Loss: 116.0970, Test Loss: 62.4900\n",
      "Epoch [8450/50000], Train Loss: 85.6579, Test Loss: 63.3196\n",
      "Epoch [8455/50000], Train Loss: 76.2982, Test Loss: 61.1074\n",
      "Epoch [8460/50000], Train Loss: 83.5249, Test Loss: 64.6234\n",
      "Epoch [8465/50000], Train Loss: 77.9534, Test Loss: 60.8250\n",
      "Epoch [8470/50000], Train Loss: 144.6067, Test Loss: 62.8476\n",
      "Epoch [8475/50000], Train Loss: 65.3528, Test Loss: 69.6006\n",
      "Epoch [8480/50000], Train Loss: 174.2618, Test Loss: 59.5537\n",
      "Epoch [8485/50000], Train Loss: 77.2533, Test Loss: 59.8215\n",
      "Epoch [8490/50000], Train Loss: 56.4525, Test Loss: 58.6987\n",
      "Epoch [8495/50000], Train Loss: 70.7286, Test Loss: 86.4232\n",
      "Epoch [8500/50000], Train Loss: 72.5713, Test Loss: 75.0265\n",
      "Epoch [8505/50000], Train Loss: 70.0560, Test Loss: 61.7908\n",
      "Epoch [8510/50000], Train Loss: 70.8487, Test Loss: 59.9799\n",
      "Epoch [8515/50000], Train Loss: 69.0745, Test Loss: 62.7925\n",
      "Epoch [8520/50000], Train Loss: 87.3987, Test Loss: 61.9534\n",
      "Epoch [8525/50000], Train Loss: 81.4857, Test Loss: 62.3945\n",
      "Epoch [8530/50000], Train Loss: 68.4003, Test Loss: 60.7284\n",
      "Epoch [8535/50000], Train Loss: 62.8167, Test Loss: 56.8627\n",
      "Epoch [8540/50000], Train Loss: 61.6675, Test Loss: 65.5319\n",
      "Epoch [8545/50000], Train Loss: 66.0816, Test Loss: 60.0719\n",
      "Epoch [8550/50000], Train Loss: 87.1602, Test Loss: 63.0280\n",
      "Epoch [8555/50000], Train Loss: 69.6250, Test Loss: 58.3441\n",
      "Epoch [8560/50000], Train Loss: 52.8447, Test Loss: 61.7816\n",
      "Epoch [8565/50000], Train Loss: 73.2204, Test Loss: 60.7028\n",
      "Epoch [8570/50000], Train Loss: 105.3601, Test Loss: 65.4540\n",
      "Epoch [8575/50000], Train Loss: 78.5007, Test Loss: 59.0704\n",
      "Epoch [8580/50000], Train Loss: 67.8815, Test Loss: 61.8035\n",
      "Epoch [8585/50000], Train Loss: 59.9001, Test Loss: 57.4149\n",
      "Epoch [8590/50000], Train Loss: 83.4826, Test Loss: 65.0000\n",
      "Epoch [8595/50000], Train Loss: 63.2775, Test Loss: 57.1794\n",
      "Epoch [8600/50000], Train Loss: 59.9440, Test Loss: 67.2045\n",
      "Epoch [8605/50000], Train Loss: 300.9266, Test Loss: 69.5549\n",
      "Epoch [8610/50000], Train Loss: 78.2436, Test Loss: 72.7265\n",
      "Epoch [8615/50000], Train Loss: 110.1083, Test Loss: 75.5962\n",
      "Epoch [8620/50000], Train Loss: 67.2706, Test Loss: 58.9938\n",
      "Epoch [8625/50000], Train Loss: 71.3143, Test Loss: 59.8340\n",
      "Epoch [8630/50000], Train Loss: 60.0786, Test Loss: 56.9155\n",
      "Epoch [8635/50000], Train Loss: 60.1707, Test Loss: 57.6717\n",
      "Epoch [8640/50000], Train Loss: 65.8941, Test Loss: 57.8313\n",
      "Epoch [8645/50000], Train Loss: 71.3632, Test Loss: 64.9971\n",
      "Epoch [8650/50000], Train Loss: 78.7295, Test Loss: 64.2116\n",
      "Epoch [8655/50000], Train Loss: 106.6941, Test Loss: 85.0705\n",
      "Epoch [8660/50000], Train Loss: 103.4924, Test Loss: 59.7762\n",
      "Epoch [8665/50000], Train Loss: 60.0859, Test Loss: 57.1212\n",
      "Epoch [8670/50000], Train Loss: 75.2437, Test Loss: 62.8122\n",
      "Epoch [8675/50000], Train Loss: 56.6652, Test Loss: 65.9590\n",
      "Epoch [8680/50000], Train Loss: 67.5501, Test Loss: 67.6364\n",
      "Epoch [8685/50000], Train Loss: 85.1536, Test Loss: 76.0196\n",
      "Epoch [8690/50000], Train Loss: 69.3966, Test Loss: 65.4941\n",
      "Epoch [8695/50000], Train Loss: 68.4281, Test Loss: 71.3394\n",
      "Epoch [8700/50000], Train Loss: 80.9619, Test Loss: 65.4178\n",
      "Epoch [8705/50000], Train Loss: 68.0848, Test Loss: 65.9385\n",
      "Epoch [8710/50000], Train Loss: 192.5999, Test Loss: 57.1631\n",
      "Epoch [8715/50000], Train Loss: 97.8323, Test Loss: 55.9923\n",
      "Epoch [8720/50000], Train Loss: 69.0438, Test Loss: 58.3835\n",
      "Epoch [8725/50000], Train Loss: 85.9297, Test Loss: 61.2200\n",
      "Epoch [8730/50000], Train Loss: 80.5305, Test Loss: 59.6789\n",
      "Epoch [8735/50000], Train Loss: 87.2631, Test Loss: 84.4284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8740/50000], Train Loss: 70.3710, Test Loss: 56.5663\n",
      "Epoch [8745/50000], Train Loss: 153.1757, Test Loss: 71.9379\n",
      "Epoch [8750/50000], Train Loss: 62.1020, Test Loss: 54.7992\n",
      "Epoch [8755/50000], Train Loss: 79.8578, Test Loss: 64.5356\n",
      "Epoch [8760/50000], Train Loss: 88.6776, Test Loss: 83.1850\n",
      "Epoch [8765/50000], Train Loss: 76.8121, Test Loss: 58.0892\n",
      "Epoch [8770/50000], Train Loss: 54.2589, Test Loss: 57.5938\n",
      "Epoch [8775/50000], Train Loss: 81.3365, Test Loss: 62.8130\n",
      "Epoch [8780/50000], Train Loss: 83.7049, Test Loss: 64.2823\n",
      "Epoch [8785/50000], Train Loss: 66.8513, Test Loss: 61.3845\n",
      "Epoch [8790/50000], Train Loss: 100.8505, Test Loss: 61.0951\n",
      "Epoch [8795/50000], Train Loss: 73.6055, Test Loss: 61.4993\n",
      "Epoch [8800/50000], Train Loss: 53.3273, Test Loss: 57.7991\n",
      "Epoch [8805/50000], Train Loss: 70.9989, Test Loss: 62.7035\n",
      "Epoch [8810/50000], Train Loss: 96.8392, Test Loss: 57.1138\n",
      "Epoch [8815/50000], Train Loss: 61.4586, Test Loss: 63.9877\n",
      "Epoch [8820/50000], Train Loss: 118.3190, Test Loss: 479.7423\n",
      "Epoch [8825/50000], Train Loss: 68.5598, Test Loss: 60.4795\n",
      "Epoch [8830/50000], Train Loss: 75.0895, Test Loss: 62.2650\n",
      "Epoch [8835/50000], Train Loss: 65.9187, Test Loss: 60.6062\n",
      "Epoch [8840/50000], Train Loss: 80.4789, Test Loss: 71.8857\n",
      "Epoch [8845/50000], Train Loss: 62.2398, Test Loss: 67.1729\n",
      "Epoch [8850/50000], Train Loss: 60.8320, Test Loss: 68.9210\n",
      "Epoch [8855/50000], Train Loss: 56.2938, Test Loss: 56.7438\n",
      "Epoch [8860/50000], Train Loss: 55.9046, Test Loss: 56.9029\n",
      "Epoch [8865/50000], Train Loss: 96.6658, Test Loss: 57.8533\n",
      "Epoch [8870/50000], Train Loss: 128.0304, Test Loss: 60.8267\n",
      "Epoch [8875/50000], Train Loss: 76.3220, Test Loss: 91.0200\n",
      "Epoch [8880/50000], Train Loss: 57.3180, Test Loss: 66.1316\n",
      "Epoch [8885/50000], Train Loss: 97.0066, Test Loss: 55.5273\n",
      "Epoch [8890/50000], Train Loss: 71.0174, Test Loss: 76.3836\n",
      "Epoch [8895/50000], Train Loss: 69.0992, Test Loss: 56.3850\n",
      "Epoch [8900/50000], Train Loss: 47.4572, Test Loss: 60.4195\n",
      "Epoch [8905/50000], Train Loss: 63.2801, Test Loss: 64.5356\n",
      "Epoch [8910/50000], Train Loss: 63.2579, Test Loss: 61.9315\n",
      "Epoch [8915/50000], Train Loss: 167.8712, Test Loss: 162.2650\n",
      "Epoch [8920/50000], Train Loss: 83.0353, Test Loss: 58.1663\n",
      "Epoch [8925/50000], Train Loss: 63.3509, Test Loss: 69.8204\n",
      "Epoch [8930/50000], Train Loss: 187.0998, Test Loss: 61.7962\n",
      "Epoch [8935/50000], Train Loss: 58.6957, Test Loss: 68.3978\n",
      "Epoch [8940/50000], Train Loss: 67.9527, Test Loss: 61.8094\n",
      "Epoch [8945/50000], Train Loss: 66.4043, Test Loss: 62.9819\n",
      "Epoch [8950/50000], Train Loss: 68.0366, Test Loss: 63.1924\n",
      "Epoch [8955/50000], Train Loss: 71.1072, Test Loss: 66.0708\n",
      "Epoch [8960/50000], Train Loss: 56.7905, Test Loss: 55.1857\n",
      "Epoch [8965/50000], Train Loss: 71.9237, Test Loss: 82.4498\n",
      "Epoch [8970/50000], Train Loss: 71.6926, Test Loss: 55.8358\n",
      "Epoch [8975/50000], Train Loss: 104.0143, Test Loss: 63.0260\n",
      "Epoch [8980/50000], Train Loss: 68.7164, Test Loss: 63.2650\n",
      "Epoch [8985/50000], Train Loss: 86.9734, Test Loss: 55.1746\n",
      "Epoch [8990/50000], Train Loss: 51.1632, Test Loss: 63.8133\n",
      "Epoch [8995/50000], Train Loss: 80.6399, Test Loss: 101.5939\n",
      "Epoch [9000/50000], Train Loss: 295.8603, Test Loss: 62.3976\n",
      "Epoch [9005/50000], Train Loss: 62.6333, Test Loss: 56.6573\n",
      "Epoch [9010/50000], Train Loss: 121.9651, Test Loss: 57.2401\n",
      "Epoch [9015/50000], Train Loss: 70.5313, Test Loss: 60.2305\n",
      "Epoch [9020/50000], Train Loss: 60.7567, Test Loss: 59.4643\n",
      "Epoch [9025/50000], Train Loss: 56.4288, Test Loss: 65.1054\n",
      "Epoch [9030/50000], Train Loss: 77.7570, Test Loss: 92.0983\n",
      "Epoch [9035/50000], Train Loss: 65.9365, Test Loss: 60.8633\n",
      "Epoch [9040/50000], Train Loss: 81.7760, Test Loss: 65.4799\n",
      "Epoch [9045/50000], Train Loss: 64.4355, Test Loss: 54.8351\n",
      "Epoch [9050/50000], Train Loss: 72.2732, Test Loss: 60.9161\n",
      "Epoch [9055/50000], Train Loss: 56.7119, Test Loss: 59.5421\n",
      "Epoch [9060/50000], Train Loss: 71.0787, Test Loss: 59.4671\n",
      "Epoch [9065/50000], Train Loss: 109.6095, Test Loss: 63.5001\n",
      "Epoch [9070/50000], Train Loss: 181.3780, Test Loss: 70.8327\n",
      "Epoch [9075/50000], Train Loss: 83.6187, Test Loss: 98.6369\n",
      "Epoch [9080/50000], Train Loss: 66.6402, Test Loss: 59.0705\n",
      "Epoch [9085/50000], Train Loss: 153.1507, Test Loss: 106.3194\n",
      "Epoch [9090/50000], Train Loss: 46.0652, Test Loss: 59.1950\n",
      "Epoch [9095/50000], Train Loss: 54.8415, Test Loss: 57.5146\n",
      "Epoch [9100/50000], Train Loss: 80.1519, Test Loss: 56.7396\n",
      "Epoch [9105/50000], Train Loss: 73.1463, Test Loss: 63.2817\n",
      "Epoch [9110/50000], Train Loss: 72.5099, Test Loss: 61.0211\n",
      "Epoch [9115/50000], Train Loss: 63.8793, Test Loss: 66.1819\n",
      "Epoch [9120/50000], Train Loss: 72.9398, Test Loss: 57.8852\n",
      "Epoch [9125/50000], Train Loss: 77.2162, Test Loss: 59.5015\n",
      "Epoch [9130/50000], Train Loss: 69.2908, Test Loss: 73.7761\n",
      "Epoch [9135/50000], Train Loss: 61.3807, Test Loss: 78.2222\n",
      "Epoch [9140/50000], Train Loss: 60.7719, Test Loss: 59.8659\n",
      "Epoch [9145/50000], Train Loss: 74.8000, Test Loss: 61.1165\n",
      "Epoch [9150/50000], Train Loss: 67.5165, Test Loss: 63.0135\n",
      "Epoch [9155/50000], Train Loss: 64.4683, Test Loss: 60.7761\n",
      "Epoch [9160/50000], Train Loss: 62.3329, Test Loss: 56.3975\n",
      "Epoch [9165/50000], Train Loss: 190.5248, Test Loss: 69.4075\n",
      "Epoch [9170/50000], Train Loss: 83.0518, Test Loss: 67.7161\n",
      "Epoch [9175/50000], Train Loss: 77.8702, Test Loss: 67.1296\n",
      "Epoch [9180/50000], Train Loss: 72.8038, Test Loss: 70.0149\n",
      "Epoch [9185/50000], Train Loss: 53.7413, Test Loss: 58.6991\n",
      "Epoch [9190/50000], Train Loss: 87.7891, Test Loss: 62.2432\n",
      "Epoch [9195/50000], Train Loss: 68.7544, Test Loss: 62.9783\n",
      "Epoch [9200/50000], Train Loss: 82.8864, Test Loss: 62.8037\n",
      "Epoch [9205/50000], Train Loss: 112.5117, Test Loss: 55.7394\n",
      "Epoch [9210/50000], Train Loss: 65.5059, Test Loss: 66.7760\n",
      "Epoch [9215/50000], Train Loss: 61.6263, Test Loss: 55.0326\n",
      "Epoch [9220/50000], Train Loss: 81.4195, Test Loss: 58.1481\n",
      "Epoch [9225/50000], Train Loss: 69.6618, Test Loss: 66.4233\n",
      "Epoch [9230/50000], Train Loss: 59.4409, Test Loss: 55.6693\n",
      "Epoch [9235/50000], Train Loss: 58.5509, Test Loss: 55.9201\n",
      "Epoch [9240/50000], Train Loss: 56.2074, Test Loss: 66.5963\n",
      "Epoch [9245/50000], Train Loss: 59.6874, Test Loss: 58.9679\n",
      "Epoch [9250/50000], Train Loss: 114.1296, Test Loss: 54.1699\n",
      "Epoch [9255/50000], Train Loss: 55.6944, Test Loss: 58.7384\n",
      "Epoch [9260/50000], Train Loss: 70.9425, Test Loss: 63.7720\n",
      "Epoch [9265/50000], Train Loss: 55.4875, Test Loss: 56.3960\n",
      "Epoch [9270/50000], Train Loss: 66.6565, Test Loss: 66.1173\n",
      "Epoch [9275/50000], Train Loss: 56.4859, Test Loss: 56.5660\n",
      "Epoch [9280/50000], Train Loss: 55.7785, Test Loss: 55.9362\n",
      "Epoch [9285/50000], Train Loss: 66.4279, Test Loss: 68.2354\n",
      "Epoch [9290/50000], Train Loss: 70.2738, Test Loss: 63.9071\n",
      "Epoch [9295/50000], Train Loss: 61.4975, Test Loss: 62.7222\n",
      "Epoch [9300/50000], Train Loss: 61.3382, Test Loss: 60.7314\n",
      "Epoch [9305/50000], Train Loss: 67.4174, Test Loss: 61.1632\n",
      "Epoch [9310/50000], Train Loss: 60.1949, Test Loss: 59.1920\n",
      "Epoch [9315/50000], Train Loss: 61.6973, Test Loss: 83.5943\n",
      "Epoch [9320/50000], Train Loss: 131.5808, Test Loss: 54.3822\n",
      "Epoch [9325/50000], Train Loss: 64.2516, Test Loss: 80.8436\n",
      "Epoch [9330/50000], Train Loss: 127.6351, Test Loss: 102.8054\n",
      "Epoch [9335/50000], Train Loss: 49.8155, Test Loss: 58.6542\n",
      "Epoch [9340/50000], Train Loss: 95.0738, Test Loss: 61.6107\n",
      "Epoch [9345/50000], Train Loss: 70.0316, Test Loss: 63.0375\n",
      "Epoch [9350/50000], Train Loss: 66.1031, Test Loss: 56.4112\n",
      "Epoch [9355/50000], Train Loss: 50.8949, Test Loss: 56.6095\n",
      "Epoch [9360/50000], Train Loss: 64.4264, Test Loss: 66.1911\n",
      "Epoch [9365/50000], Train Loss: 48.1803, Test Loss: 57.1963\n",
      "Epoch [9370/50000], Train Loss: 62.8532, Test Loss: 69.8085\n",
      "Epoch [9375/50000], Train Loss: 67.9251, Test Loss: 56.8103\n",
      "Epoch [9380/50000], Train Loss: 350.0879, Test Loss: 57.1140\n",
      "Epoch [9385/50000], Train Loss: 73.3000, Test Loss: 56.3147\n",
      "Epoch [9390/50000], Train Loss: 80.9912, Test Loss: 63.2386\n",
      "Epoch [9395/50000], Train Loss: 94.0660, Test Loss: 59.1531\n",
      "Epoch [9400/50000], Train Loss: 55.6810, Test Loss: 60.6982\n",
      "Epoch [9405/50000], Train Loss: 50.8950, Test Loss: 60.6477\n",
      "Epoch [9410/50000], Train Loss: 59.3258, Test Loss: 55.4453\n",
      "Epoch [9415/50000], Train Loss: 205.9757, Test Loss: 56.4926\n",
      "Epoch [9420/50000], Train Loss: 61.7906, Test Loss: 61.9547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9425/50000], Train Loss: 75.5531, Test Loss: 59.8178\n",
      "Epoch [9430/50000], Train Loss: 67.3053, Test Loss: 57.0803\n",
      "Epoch [9435/50000], Train Loss: 71.4827, Test Loss: 59.7484\n",
      "Epoch [9440/50000], Train Loss: 68.0051, Test Loss: 59.6566\n",
      "Epoch [9445/50000], Train Loss: 64.6594, Test Loss: 57.6675\n",
      "Epoch [9450/50000], Train Loss: 78.1142, Test Loss: 57.6730\n",
      "Epoch [9455/50000], Train Loss: 70.9026, Test Loss: 100.0386\n",
      "Epoch [9460/50000], Train Loss: 166.5152, Test Loss: 56.5657\n",
      "Epoch [9465/50000], Train Loss: 62.2509, Test Loss: 64.6132\n",
      "Epoch [9470/50000], Train Loss: 101.7911, Test Loss: 55.6053\n",
      "Epoch [9475/50000], Train Loss: 72.2331, Test Loss: 64.9628\n",
      "Epoch [9480/50000], Train Loss: 71.1450, Test Loss: 58.3290\n",
      "Epoch [9485/50000], Train Loss: 62.1171, Test Loss: 72.3532\n",
      "Epoch [9490/50000], Train Loss: 59.2362, Test Loss: 58.3205\n",
      "Epoch [9495/50000], Train Loss: 69.7540, Test Loss: 72.4735\n",
      "Epoch [9500/50000], Train Loss: 77.3013, Test Loss: 62.6855\n",
      "Epoch [9505/50000], Train Loss: 62.7892, Test Loss: 54.5704\n",
      "Epoch [9510/50000], Train Loss: 71.1381, Test Loss: 56.1511\n",
      "Epoch [9515/50000], Train Loss: 69.4244, Test Loss: 57.1719\n",
      "Epoch [9520/50000], Train Loss: 57.2251, Test Loss: 58.4269\n",
      "Epoch [9525/50000], Train Loss: 69.9228, Test Loss: 68.0845\n",
      "Epoch [9530/50000], Train Loss: 65.2112, Test Loss: 118.6062\n",
      "Epoch [9535/50000], Train Loss: 84.9350, Test Loss: 56.0465\n",
      "Epoch [9540/50000], Train Loss: 63.5827, Test Loss: 85.6068\n",
      "Epoch [9545/50000], Train Loss: 57.2711, Test Loss: 56.4046\n",
      "Epoch [9550/50000], Train Loss: 58.8236, Test Loss: 58.1282\n",
      "Epoch [9555/50000], Train Loss: 74.6627, Test Loss: 55.8644\n",
      "Epoch [9560/50000], Train Loss: 68.1370, Test Loss: 64.1851\n",
      "Epoch [9565/50000], Train Loss: 158.3028, Test Loss: 57.5888\n",
      "Epoch [9570/50000], Train Loss: 62.1576, Test Loss: 66.5719\n",
      "Epoch [9575/50000], Train Loss: 108.7012, Test Loss: 62.9225\n",
      "Epoch [9580/50000], Train Loss: 62.6442, Test Loss: 59.8302\n",
      "Epoch [9585/50000], Train Loss: 84.0545, Test Loss: 65.5239\n",
      "Epoch [9590/50000], Train Loss: 80.9964, Test Loss: 71.8303\n",
      "Epoch [9595/50000], Train Loss: 61.6645, Test Loss: 57.5789\n",
      "Epoch [9600/50000], Train Loss: 67.9939, Test Loss: 57.0069\n",
      "Epoch [9605/50000], Train Loss: 56.4783, Test Loss: 63.6701\n",
      "Epoch [9610/50000], Train Loss: 74.0899, Test Loss: 60.3068\n",
      "Epoch [9615/50000], Train Loss: 163.6228, Test Loss: 54.3682\n",
      "Epoch [9620/50000], Train Loss: 69.5069, Test Loss: 56.2436\n",
      "Epoch [9625/50000], Train Loss: 67.4627, Test Loss: 63.6186\n",
      "Epoch [9630/50000], Train Loss: 66.3165, Test Loss: 57.0108\n",
      "Epoch [9635/50000], Train Loss: 77.0660, Test Loss: 79.7623\n",
      "Epoch [9640/50000], Train Loss: 247.3638, Test Loss: 56.5924\n",
      "Epoch [9645/50000], Train Loss: 74.2193, Test Loss: 70.3397\n",
      "Epoch [9650/50000], Train Loss: 61.0310, Test Loss: 61.8095\n",
      "Epoch [9655/50000], Train Loss: 75.2186, Test Loss: 75.4475\n",
      "Epoch [9660/50000], Train Loss: 66.5508, Test Loss: 66.5231\n",
      "Epoch [9665/50000], Train Loss: 64.3667, Test Loss: 60.6352\n",
      "Epoch [9670/50000], Train Loss: 61.3196, Test Loss: 57.7021\n",
      "Epoch [9675/50000], Train Loss: 62.7630, Test Loss: 62.0053\n",
      "Epoch [9680/50000], Train Loss: 68.2155, Test Loss: 66.7356\n",
      "Epoch [9685/50000], Train Loss: 70.4311, Test Loss: 60.7087\n",
      "Epoch [9690/50000], Train Loss: 56.2676, Test Loss: 56.0074\n",
      "Epoch [9695/50000], Train Loss: 70.2104, Test Loss: 56.5068\n",
      "Epoch [9700/50000], Train Loss: 61.8503, Test Loss: 56.9140\n",
      "Epoch [9705/50000], Train Loss: 43.0911, Test Loss: 55.9999\n",
      "Epoch [9710/50000], Train Loss: 61.7652, Test Loss: 63.8190\n",
      "Epoch [9715/50000], Train Loss: 53.6930, Test Loss: 58.9232\n",
      "Epoch [9720/50000], Train Loss: 70.2323, Test Loss: 55.4404\n",
      "Epoch [9725/50000], Train Loss: 64.4817, Test Loss: 68.6605\n",
      "Epoch [9730/50000], Train Loss: 57.7574, Test Loss: 60.4279\n",
      "Epoch [9735/50000], Train Loss: 64.6733, Test Loss: 58.9825\n",
      "Epoch [9740/50000], Train Loss: 122.5711, Test Loss: 57.7638\n",
      "Epoch [9745/50000], Train Loss: 61.7082, Test Loss: 64.6767\n",
      "Epoch [9750/50000], Train Loss: 63.2704, Test Loss: 56.1802\n",
      "Epoch [9755/50000], Train Loss: 58.2147, Test Loss: 56.1447\n",
      "Epoch [9760/50000], Train Loss: 59.0373, Test Loss: 55.5389\n",
      "Epoch [9765/50000], Train Loss: 72.3705, Test Loss: 59.5608\n",
      "Epoch [9770/50000], Train Loss: 73.2824, Test Loss: 62.5031\n",
      "Epoch [9775/50000], Train Loss: 64.4550, Test Loss: 59.0960\n",
      "Epoch [9780/50000], Train Loss: 73.5046, Test Loss: 59.2273\n",
      "Epoch [9785/50000], Train Loss: 60.0613, Test Loss: 71.7825\n",
      "Epoch [9790/50000], Train Loss: 133.6497, Test Loss: 54.4368\n",
      "Epoch [9795/50000], Train Loss: 53.0247, Test Loss: 57.1248\n",
      "Epoch [9800/50000], Train Loss: 66.7359, Test Loss: 59.6240\n",
      "Epoch [9805/50000], Train Loss: 53.7401, Test Loss: 67.3137\n",
      "Epoch [9810/50000], Train Loss: 58.3985, Test Loss: 64.5182\n",
      "Epoch [9815/50000], Train Loss: 67.6537, Test Loss: 63.1130\n",
      "Epoch [9820/50000], Train Loss: 61.6507, Test Loss: 57.7513\n",
      "Epoch [9825/50000], Train Loss: 63.2437, Test Loss: 69.8089\n",
      "Epoch [9830/50000], Train Loss: 63.1508, Test Loss: 67.9045\n",
      "Epoch [9835/50000], Train Loss: 63.8403, Test Loss: 60.8155\n",
      "Epoch [9840/50000], Train Loss: 59.7677, Test Loss: 60.6229\n",
      "Epoch [9845/50000], Train Loss: 51.2316, Test Loss: 70.7716\n",
      "Epoch [9850/50000], Train Loss: 80.4251, Test Loss: 56.4839\n",
      "Epoch [9855/50000], Train Loss: 49.9542, Test Loss: 56.1698\n",
      "Epoch [9860/50000], Train Loss: 66.5250, Test Loss: 62.0640\n",
      "Epoch [9865/50000], Train Loss: 61.1377, Test Loss: 60.2988\n",
      "Epoch [9870/50000], Train Loss: 59.1343, Test Loss: 55.9213\n",
      "Epoch [9875/50000], Train Loss: 72.0433, Test Loss: 59.2778\n",
      "Epoch [9880/50000], Train Loss: 68.8620, Test Loss: 59.0807\n",
      "Epoch [9885/50000], Train Loss: 55.2152, Test Loss: 56.9904\n",
      "Epoch [9890/50000], Train Loss: 61.8041, Test Loss: 80.2111\n",
      "Epoch [9895/50000], Train Loss: 55.7983, Test Loss: 56.0691\n",
      "Epoch [9900/50000], Train Loss: 51.5174, Test Loss: 57.7039\n",
      "Epoch [9905/50000], Train Loss: 118.6277, Test Loss: 57.1680\n",
      "Epoch [9910/50000], Train Loss: 66.2600, Test Loss: 55.4013\n",
      "Epoch [9915/50000], Train Loss: 69.1695, Test Loss: 74.0817\n",
      "Epoch [9920/50000], Train Loss: 65.8743, Test Loss: 54.7167\n",
      "Epoch [9925/50000], Train Loss: 58.0268, Test Loss: 64.2111\n",
      "Epoch [9930/50000], Train Loss: 64.2043, Test Loss: 57.1230\n",
      "Epoch [9935/50000], Train Loss: 62.8187, Test Loss: 54.8627\n",
      "Epoch [9940/50000], Train Loss: 60.6591, Test Loss: 56.0932\n",
      "Epoch [9945/50000], Train Loss: 61.0521, Test Loss: 58.3878\n",
      "Epoch [9950/50000], Train Loss: 64.3296, Test Loss: 66.6039\n",
      "Epoch [9955/50000], Train Loss: 56.8500, Test Loss: 60.6742\n",
      "Epoch [9960/50000], Train Loss: 51.9205, Test Loss: 59.5979\n",
      "Epoch [9965/50000], Train Loss: 65.1471, Test Loss: 61.0413\n",
      "Epoch [9970/50000], Train Loss: 64.0695, Test Loss: 64.5259\n",
      "Epoch [9975/50000], Train Loss: 54.0929, Test Loss: 57.3451\n",
      "Epoch [9980/50000], Train Loss: 55.1402, Test Loss: 64.9825\n",
      "Epoch [9985/50000], Train Loss: 83.5668, Test Loss: 116.6912\n",
      "Epoch [9990/50000], Train Loss: 333.7311, Test Loss: 58.7711\n",
      "Epoch [9995/50000], Train Loss: 60.0419, Test Loss: 55.9565\n",
      "Epoch [10000/50000], Train Loss: 47.2399, Test Loss: 59.1254\n",
      "Epoch [10005/50000], Train Loss: 60.8479, Test Loss: 60.0948\n",
      "Epoch [10010/50000], Train Loss: 148.8128, Test Loss: 55.0162\n",
      "Epoch [10015/50000], Train Loss: 62.5683, Test Loss: 58.0504\n",
      "Epoch [10020/50000], Train Loss: 65.3032, Test Loss: 56.2646\n",
      "Epoch [10025/50000], Train Loss: 63.8266, Test Loss: 55.8133\n",
      "Epoch [10030/50000], Train Loss: 54.9837, Test Loss: 65.3898\n",
      "Epoch [10035/50000], Train Loss: 55.9586, Test Loss: 76.3917\n",
      "Epoch [10040/50000], Train Loss: 56.6147, Test Loss: 55.4438\n",
      "Epoch [10045/50000], Train Loss: 60.0441, Test Loss: 61.1065\n",
      "Epoch [10050/50000], Train Loss: 63.0331, Test Loss: 58.1383\n",
      "Epoch [10055/50000], Train Loss: 62.9828, Test Loss: 98.2952\n",
      "Epoch [10060/50000], Train Loss: 64.1726, Test Loss: 54.4328\n",
      "Epoch [10065/50000], Train Loss: 63.0001, Test Loss: 66.5315\n",
      "Epoch [10070/50000], Train Loss: 107.6708, Test Loss: 64.2935\n",
      "Epoch [10075/50000], Train Loss: 65.8311, Test Loss: 60.2377\n",
      "Epoch [10080/50000], Train Loss: 66.3669, Test Loss: 77.8830\n",
      "Epoch [10085/50000], Train Loss: 53.7357, Test Loss: 61.4032\n",
      "Epoch [10090/50000], Train Loss: 57.9504, Test Loss: 61.0165\n",
      "Epoch [10095/50000], Train Loss: 81.1174, Test Loss: 62.7853\n",
      "Epoch [10100/50000], Train Loss: 75.8184, Test Loss: 53.0549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10105/50000], Train Loss: 70.0936, Test Loss: 56.7532\n",
      "Epoch [10110/50000], Train Loss: 246.2252, Test Loss: 397.8907\n",
      "Epoch [10115/50000], Train Loss: 61.3557, Test Loss: 55.2417\n",
      "Epoch [10120/50000], Train Loss: 64.5655, Test Loss: 56.6496\n",
      "Epoch [10125/50000], Train Loss: 55.2970, Test Loss: 69.6245\n",
      "Epoch [10130/50000], Train Loss: 110.7833, Test Loss: 55.4596\n",
      "Epoch [10135/50000], Train Loss: 61.5754, Test Loss: 54.9042\n",
      "Epoch [10140/50000], Train Loss: 121.2712, Test Loss: 55.7965\n",
      "Epoch [10145/50000], Train Loss: 60.6878, Test Loss: 61.3887\n",
      "Epoch [10150/50000], Train Loss: 64.6475, Test Loss: 61.0944\n",
      "Epoch [10155/50000], Train Loss: 84.9622, Test Loss: 57.3875\n",
      "Epoch [10160/50000], Train Loss: 59.3084, Test Loss: 57.6771\n",
      "Epoch [10165/50000], Train Loss: 55.9754, Test Loss: 63.6024\n",
      "Epoch [10170/50000], Train Loss: 65.0967, Test Loss: 58.1736\n",
      "Epoch [10175/50000], Train Loss: 50.2273, Test Loss: 56.1261\n",
      "Epoch [10180/50000], Train Loss: 63.6277, Test Loss: 55.6537\n",
      "Epoch [10185/50000], Train Loss: 184.2618, Test Loss: 59.6490\n",
      "Epoch [10190/50000], Train Loss: 56.2500, Test Loss: 60.1841\n",
      "Epoch [10195/50000], Train Loss: 56.9409, Test Loss: 55.2102\n",
      "Epoch [10200/50000], Train Loss: 159.1880, Test Loss: 55.9086\n",
      "Epoch [10205/50000], Train Loss: 61.6171, Test Loss: 58.2237\n",
      "Epoch [10210/50000], Train Loss: 52.8498, Test Loss: 53.3682\n",
      "Epoch [10215/50000], Train Loss: 45.0564, Test Loss: 53.9927\n",
      "Epoch [10220/50000], Train Loss: 64.2025, Test Loss: 64.3731\n",
      "Epoch [10225/50000], Train Loss: 76.8217, Test Loss: 57.2983\n",
      "Epoch [10230/50000], Train Loss: 64.9635, Test Loss: 55.5117\n",
      "Epoch [10235/50000], Train Loss: 71.6741, Test Loss: 58.1951\n",
      "Epoch [10240/50000], Train Loss: 72.0486, Test Loss: 57.6449\n",
      "Epoch [10245/50000], Train Loss: 84.4160, Test Loss: 63.6185\n",
      "Epoch [10250/50000], Train Loss: 45.8310, Test Loss: 61.4482\n",
      "Epoch [10255/50000], Train Loss: 62.9344, Test Loss: 60.3252\n",
      "Epoch [10260/50000], Train Loss: 54.9515, Test Loss: 57.7354\n",
      "Epoch [10265/50000], Train Loss: 61.5195, Test Loss: 61.6252\n",
      "Epoch [10270/50000], Train Loss: 48.2621, Test Loss: 55.7339\n",
      "Epoch [10275/50000], Train Loss: 60.2276, Test Loss: 57.2476\n",
      "Epoch [10280/50000], Train Loss: 66.1607, Test Loss: 55.8560\n",
      "Epoch [10285/50000], Train Loss: 66.7121, Test Loss: 74.1225\n",
      "Epoch [10290/50000], Train Loss: 50.9771, Test Loss: 62.1947\n",
      "Epoch [10295/50000], Train Loss: 80.8288, Test Loss: 54.8757\n",
      "Epoch [10300/50000], Train Loss: 53.6637, Test Loss: 63.1557\n",
      "Epoch [10305/50000], Train Loss: 60.8185, Test Loss: 58.1049\n",
      "Epoch [10310/50000], Train Loss: 130.4139, Test Loss: 55.0627\n",
      "Epoch [10315/50000], Train Loss: 70.1808, Test Loss: 64.0897\n",
      "Epoch [10320/50000], Train Loss: 60.5100, Test Loss: 57.6781\n",
      "Epoch [10325/50000], Train Loss: 58.3162, Test Loss: 58.0381\n",
      "Epoch [10330/50000], Train Loss: 55.7384, Test Loss: 54.9654\n",
      "Epoch [10335/50000], Train Loss: 61.2487, Test Loss: 55.4939\n",
      "Epoch [10340/50000], Train Loss: 76.1690, Test Loss: 59.6281\n",
      "Epoch [10345/50000], Train Loss: 256.6289, Test Loss: 61.1067\n",
      "Epoch [10350/50000], Train Loss: 66.8450, Test Loss: 55.8266\n",
      "Epoch [10355/50000], Train Loss: 65.9255, Test Loss: 74.8074\n",
      "Epoch [10360/50000], Train Loss: 58.9233, Test Loss: 59.7721\n",
      "Epoch [10365/50000], Train Loss: 67.8866, Test Loss: 56.0057\n",
      "Epoch [10370/50000], Train Loss: 61.6268, Test Loss: 56.8811\n",
      "Epoch [10375/50000], Train Loss: 54.0812, Test Loss: 58.2967\n",
      "Epoch [10380/50000], Train Loss: 61.4123, Test Loss: 55.3959\n",
      "Epoch [10385/50000], Train Loss: 56.3430, Test Loss: 59.8749\n",
      "Epoch [10390/50000], Train Loss: 62.0224, Test Loss: 60.2103\n",
      "Epoch [10395/50000], Train Loss: 60.8915, Test Loss: 54.1198\n",
      "Epoch [10400/50000], Train Loss: 122.7238, Test Loss: 74.3004\n",
      "Epoch [10405/50000], Train Loss: 76.1638, Test Loss: 55.3988\n",
      "Epoch [10410/50000], Train Loss: 57.9127, Test Loss: 58.4782\n",
      "Epoch [10415/50000], Train Loss: 54.4140, Test Loss: 56.2474\n",
      "Epoch [10420/50000], Train Loss: 64.4994, Test Loss: 63.9754\n",
      "Epoch [10425/50000], Train Loss: 60.4433, Test Loss: 55.4438\n",
      "Epoch [10430/50000], Train Loss: 59.8444, Test Loss: 61.8652\n",
      "Epoch [10435/50000], Train Loss: 56.3916, Test Loss: 63.1208\n",
      "Epoch [10440/50000], Train Loss: 57.1984, Test Loss: 57.9506\n",
      "Epoch [10445/50000], Train Loss: 43.6008, Test Loss: 55.7377\n",
      "Epoch [10450/50000], Train Loss: 61.4418, Test Loss: 62.8012\n",
      "Epoch [10455/50000], Train Loss: 61.7630, Test Loss: 56.0040\n",
      "Epoch [10460/50000], Train Loss: 52.2879, Test Loss: 58.3610\n",
      "Epoch [10465/50000], Train Loss: 52.0848, Test Loss: 55.3408\n",
      "Epoch [10470/50000], Train Loss: 62.5562, Test Loss: 56.6051\n",
      "Epoch [10475/50000], Train Loss: 63.6930, Test Loss: 62.4717\n",
      "Epoch [10480/50000], Train Loss: 52.5720, Test Loss: 53.7225\n",
      "Epoch [10485/50000], Train Loss: 53.1862, Test Loss: 77.6844\n",
      "Epoch [10490/50000], Train Loss: 65.1858, Test Loss: 60.6402\n",
      "Epoch [10495/50000], Train Loss: 62.2313, Test Loss: 60.4994\n",
      "Epoch [10500/50000], Train Loss: 58.8314, Test Loss: 57.3139\n",
      "Epoch [10505/50000], Train Loss: 149.5768, Test Loss: 56.8798\n",
      "Epoch [10510/50000], Train Loss: 56.5389, Test Loss: 59.6762\n",
      "Epoch [10515/50000], Train Loss: 68.2671, Test Loss: 55.5975\n",
      "Epoch [10520/50000], Train Loss: 58.6674, Test Loss: 64.6544\n",
      "Epoch [10525/50000], Train Loss: 63.2061, Test Loss: 71.6168\n",
      "Epoch [10530/50000], Train Loss: 149.8396, Test Loss: 121.9781\n",
      "Epoch [10535/50000], Train Loss: 55.8808, Test Loss: 57.0841\n",
      "Epoch [10540/50000], Train Loss: 66.2837, Test Loss: 58.0809\n",
      "Epoch [10545/50000], Train Loss: 40.4044, Test Loss: 57.3469\n",
      "Epoch [10550/50000], Train Loss: 65.5114, Test Loss: 55.1906\n",
      "Epoch [10555/50000], Train Loss: 67.5798, Test Loss: 62.5972\n",
      "Epoch [10560/50000], Train Loss: 45.4449, Test Loss: 58.2348\n",
      "Epoch [10565/50000], Train Loss: 56.8371, Test Loss: 61.8854\n",
      "Epoch [10570/50000], Train Loss: 58.7900, Test Loss: 55.2378\n",
      "Epoch [10575/50000], Train Loss: 70.9149, Test Loss: 53.5291\n",
      "Epoch [10580/50000], Train Loss: 49.5366, Test Loss: 55.0960\n",
      "Epoch [10585/50000], Train Loss: 56.7471, Test Loss: 67.8745\n",
      "Epoch [10590/50000], Train Loss: 102.9600, Test Loss: 103.2950\n",
      "Epoch [10595/50000], Train Loss: 80.9773, Test Loss: 63.4957\n",
      "Epoch [10600/50000], Train Loss: 76.7627, Test Loss: 59.9354\n",
      "Epoch [10605/50000], Train Loss: 71.2728, Test Loss: 101.5766\n",
      "Epoch [10610/50000], Train Loss: 66.3473, Test Loss: 58.2257\n",
      "Epoch [10615/50000], Train Loss: 43.8597, Test Loss: 55.7160\n",
      "Epoch [10620/50000], Train Loss: 61.7806, Test Loss: 62.3929\n",
      "Epoch [10625/50000], Train Loss: 57.7996, Test Loss: 65.8254\n",
      "Epoch [10630/50000], Train Loss: 59.3277, Test Loss: 55.7062\n",
      "Epoch [10635/50000], Train Loss: 76.3683, Test Loss: 70.5263\n",
      "Epoch [10640/50000], Train Loss: 66.6836, Test Loss: 61.1664\n",
      "Epoch [10645/50000], Train Loss: 47.4394, Test Loss: 57.2331\n",
      "Epoch [10650/50000], Train Loss: 79.6220, Test Loss: 55.0531\n",
      "Epoch [10655/50000], Train Loss: 95.4055, Test Loss: 57.5035\n",
      "Epoch [10660/50000], Train Loss: 53.4513, Test Loss: 61.6361\n",
      "Epoch [10665/50000], Train Loss: 55.8781, Test Loss: 60.9290\n",
      "Epoch [10670/50000], Train Loss: 64.7606, Test Loss: 54.1172\n",
      "Epoch [10675/50000], Train Loss: 128.9111, Test Loss: 62.6343\n",
      "Epoch [10680/50000], Train Loss: 60.0806, Test Loss: 57.7477\n",
      "Epoch [10685/50000], Train Loss: 65.6367, Test Loss: 70.8319\n",
      "Epoch [10690/50000], Train Loss: 70.5489, Test Loss: 59.8809\n",
      "Epoch [10695/50000], Train Loss: 54.5647, Test Loss: 62.4116\n",
      "Epoch [10700/50000], Train Loss: 54.4101, Test Loss: 56.6881\n",
      "Epoch [10705/50000], Train Loss: 111.1691, Test Loss: 56.5277\n",
      "Epoch [10710/50000], Train Loss: 63.6189, Test Loss: 57.9744\n",
      "Epoch [10715/50000], Train Loss: 56.6391, Test Loss: 58.1412\n",
      "Epoch [10720/50000], Train Loss: 59.1279, Test Loss: 54.2698\n",
      "Epoch [10725/50000], Train Loss: 67.8545, Test Loss: 67.0765\n",
      "Epoch [10730/50000], Train Loss: 47.5227, Test Loss: 54.7830\n",
      "Epoch [10735/50000], Train Loss: 57.1022, Test Loss: 65.2077\n",
      "Epoch [10740/50000], Train Loss: 63.3770, Test Loss: 55.8657\n",
      "Epoch [10745/50000], Train Loss: 60.3887, Test Loss: 56.3201\n",
      "Epoch [10750/50000], Train Loss: 67.1076, Test Loss: 56.5159\n",
      "Epoch [10755/50000], Train Loss: 53.2044, Test Loss: 54.6683\n",
      "Epoch [10760/50000], Train Loss: 76.1155, Test Loss: 55.1512\n",
      "Epoch [10765/50000], Train Loss: 61.6528, Test Loss: 64.9106\n",
      "Epoch [10770/50000], Train Loss: 85.1554, Test Loss: 57.1535\n",
      "Epoch [10775/50000], Train Loss: 61.9204, Test Loss: 62.9292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10780/50000], Train Loss: 49.5820, Test Loss: 55.6036\n",
      "Epoch [10785/50000], Train Loss: 77.9396, Test Loss: 65.2489\n",
      "Epoch [10790/50000], Train Loss: 55.7592, Test Loss: 55.9212\n",
      "Epoch [10795/50000], Train Loss: 51.2947, Test Loss: 53.3833\n",
      "Epoch [10800/50000], Train Loss: 53.3181, Test Loss: 56.7177\n",
      "Epoch [10805/50000], Train Loss: 55.9307, Test Loss: 57.8894\n",
      "Epoch [10810/50000], Train Loss: 64.9267, Test Loss: 58.8714\n",
      "Epoch [10815/50000], Train Loss: 88.5740, Test Loss: 67.5739\n",
      "Epoch [10820/50000], Train Loss: 110.2150, Test Loss: 56.0055\n",
      "Epoch [10825/50000], Train Loss: 53.7322, Test Loss: 57.1909\n",
      "Epoch [10830/50000], Train Loss: 76.1633, Test Loss: 73.9139\n",
      "Epoch [10835/50000], Train Loss: 57.5766, Test Loss: 58.0932\n",
      "Epoch [10840/50000], Train Loss: 49.4953, Test Loss: 60.2228\n",
      "Epoch [10845/50000], Train Loss: 66.7139, Test Loss: 53.2748\n",
      "Epoch [10850/50000], Train Loss: 55.4359, Test Loss: 53.7230\n",
      "Epoch [10855/50000], Train Loss: 61.2772, Test Loss: 60.4067\n",
      "Epoch [10860/50000], Train Loss: 57.8530, Test Loss: 58.0763\n",
      "Epoch [10865/50000], Train Loss: 50.2573, Test Loss: 75.6261\n",
      "Epoch [10870/50000], Train Loss: 48.7279, Test Loss: 55.0371\n",
      "Epoch [10875/50000], Train Loss: 57.7240, Test Loss: 63.5060\n",
      "Epoch [10880/50000], Train Loss: 45.6944, Test Loss: 60.9263\n",
      "Epoch [10885/50000], Train Loss: 66.4172, Test Loss: 57.5395\n",
      "Epoch [10890/50000], Train Loss: 201.1542, Test Loss: 55.6790\n",
      "Epoch [10895/50000], Train Loss: 63.6148, Test Loss: 54.7113\n",
      "Epoch [10900/50000], Train Loss: 52.1088, Test Loss: 57.2926\n",
      "Epoch [10905/50000], Train Loss: 89.1543, Test Loss: 66.0548\n",
      "Epoch [10910/50000], Train Loss: 59.3755, Test Loss: 54.0910\n",
      "Epoch [10915/50000], Train Loss: 59.0387, Test Loss: 59.7869\n",
      "Epoch [10920/50000], Train Loss: 54.7793, Test Loss: 55.5679\n",
      "Epoch [10925/50000], Train Loss: 46.6121, Test Loss: 56.3535\n",
      "Epoch [10930/50000], Train Loss: 67.2852, Test Loss: 64.4173\n",
      "Epoch [10935/50000], Train Loss: 58.5938, Test Loss: 56.5373\n",
      "Epoch [10940/50000], Train Loss: 56.3040, Test Loss: 55.7237\n",
      "Epoch [10945/50000], Train Loss: 65.5651, Test Loss: 59.1183\n",
      "Epoch [10950/50000], Train Loss: 52.1263, Test Loss: 58.8294\n",
      "Epoch [10955/50000], Train Loss: 57.9520, Test Loss: 56.8438\n",
      "Epoch [10960/50000], Train Loss: 72.6888, Test Loss: 55.9074\n",
      "Epoch [10965/50000], Train Loss: 91.5598, Test Loss: 178.4422\n",
      "Epoch [10970/50000], Train Loss: 90.3307, Test Loss: 140.4802\n",
      "Epoch [10975/50000], Train Loss: 90.7695, Test Loss: 67.0131\n",
      "Epoch [10980/50000], Train Loss: 55.9185, Test Loss: 54.9606\n",
      "Epoch [10985/50000], Train Loss: 57.6155, Test Loss: 54.6550\n",
      "Epoch [10990/50000], Train Loss: 51.6508, Test Loss: 54.5520\n",
      "Epoch [10995/50000], Train Loss: 42.8353, Test Loss: 59.6792\n",
      "Epoch [11000/50000], Train Loss: 60.4852, Test Loss: 53.4612\n",
      "Epoch [11005/50000], Train Loss: 59.0805, Test Loss: 61.2858\n",
      "Epoch [11010/50000], Train Loss: 79.4872, Test Loss: 56.6496\n",
      "Epoch [11015/50000], Train Loss: 42.0184, Test Loss: 54.0129\n",
      "Epoch [11020/50000], Train Loss: 125.8736, Test Loss: 54.9611\n",
      "Epoch [11025/50000], Train Loss: 54.6716, Test Loss: 64.7770\n",
      "Epoch [11030/50000], Train Loss: 63.9121, Test Loss: 56.8785\n",
      "Epoch [11035/50000], Train Loss: 54.4685, Test Loss: 53.4868\n",
      "Epoch [11040/50000], Train Loss: 53.9054, Test Loss: 77.7457\n",
      "Epoch [11045/50000], Train Loss: 73.3078, Test Loss: 62.0663\n",
      "Epoch [11050/50000], Train Loss: 57.5833, Test Loss: 56.3457\n",
      "Epoch [11055/50000], Train Loss: 41.2311, Test Loss: 53.1997\n",
      "Epoch [11060/50000], Train Loss: 73.8033, Test Loss: 65.1020\n",
      "Epoch [11065/50000], Train Loss: 50.3033, Test Loss: 69.8653\n",
      "Epoch [11070/50000], Train Loss: 59.2872, Test Loss: 56.7253\n",
      "Epoch [11075/50000], Train Loss: 52.3353, Test Loss: 54.2659\n",
      "Epoch [11080/50000], Train Loss: 56.7226, Test Loss: 58.6799\n",
      "Epoch [11085/50000], Train Loss: 49.0108, Test Loss: 59.0891\n",
      "Epoch [11090/50000], Train Loss: 77.7679, Test Loss: 64.2040\n",
      "Epoch [11095/50000], Train Loss: 68.1744, Test Loss: 55.6820\n",
      "Epoch [11100/50000], Train Loss: 49.4620, Test Loss: 52.5124\n",
      "Epoch [11105/50000], Train Loss: 52.9006, Test Loss: 56.0947\n",
      "Epoch [11110/50000], Train Loss: 59.9271, Test Loss: 67.5977\n",
      "Epoch [11115/50000], Train Loss: 50.3522, Test Loss: 53.5929\n",
      "Epoch [11120/50000], Train Loss: 60.9394, Test Loss: 74.5113\n",
      "Epoch [11125/50000], Train Loss: 55.9317, Test Loss: 59.8936\n",
      "Epoch [11130/50000], Train Loss: 58.6849, Test Loss: 54.5906\n",
      "Epoch [11135/50000], Train Loss: 43.4569, Test Loss: 58.2582\n",
      "Epoch [11140/50000], Train Loss: 51.7405, Test Loss: 59.0219\n",
      "Epoch [11145/50000], Train Loss: 53.8336, Test Loss: 63.3535\n",
      "Epoch [11150/50000], Train Loss: 44.6953, Test Loss: 53.1568\n",
      "Epoch [11155/50000], Train Loss: 62.5010, Test Loss: 66.3606\n",
      "Epoch [11160/50000], Train Loss: 41.8393, Test Loss: 54.1259\n",
      "Epoch [11165/50000], Train Loss: 55.9427, Test Loss: 77.0878\n",
      "Epoch [11170/50000], Train Loss: 56.4869, Test Loss: 56.2123\n",
      "Epoch [11175/50000], Train Loss: 50.3779, Test Loss: 56.8025\n",
      "Epoch [11180/50000], Train Loss: 56.2629, Test Loss: 54.6860\n",
      "Epoch [11185/50000], Train Loss: 55.1310, Test Loss: 56.4863\n",
      "Epoch [11190/50000], Train Loss: 51.8558, Test Loss: 60.4235\n",
      "Epoch [11195/50000], Train Loss: 58.8385, Test Loss: 61.0773\n",
      "Epoch [11200/50000], Train Loss: 41.5456, Test Loss: 56.3284\n",
      "Epoch [11205/50000], Train Loss: 113.8263, Test Loss: 57.7751\n",
      "Epoch [11210/50000], Train Loss: 64.7566, Test Loss: 55.1020\n",
      "Epoch [11215/50000], Train Loss: 138.8132, Test Loss: 55.1052\n",
      "Epoch [11220/50000], Train Loss: 60.5196, Test Loss: 61.6769\n",
      "Epoch [11225/50000], Train Loss: 65.7980, Test Loss: 54.3882\n",
      "Epoch [11230/50000], Train Loss: 52.9841, Test Loss: 58.2135\n",
      "Epoch [11235/50000], Train Loss: 63.5611, Test Loss: 59.7701\n",
      "Epoch [11240/50000], Train Loss: 125.8750, Test Loss: 52.3082\n",
      "Epoch [11245/50000], Train Loss: 62.9596, Test Loss: 54.2595\n",
      "Epoch [11250/50000], Train Loss: 52.0763, Test Loss: 54.8646\n",
      "Epoch [11255/50000], Train Loss: 56.1419, Test Loss: 55.2284\n",
      "Epoch [11260/50000], Train Loss: 118.8341, Test Loss: 53.4984\n",
      "Epoch [11265/50000], Train Loss: 56.4727, Test Loss: 80.4447\n",
      "Epoch [11270/50000], Train Loss: 50.7653, Test Loss: 58.5976\n",
      "Epoch [11275/50000], Train Loss: 53.7468, Test Loss: 60.6901\n",
      "Epoch [11280/50000], Train Loss: 57.7353, Test Loss: 57.6455\n",
      "Epoch [11285/50000], Train Loss: 108.5223, Test Loss: 56.8435\n",
      "Epoch [11290/50000], Train Loss: 57.9405, Test Loss: 56.8157\n",
      "Epoch [11295/50000], Train Loss: 49.6952, Test Loss: 55.9130\n",
      "Epoch [11300/50000], Train Loss: 84.4716, Test Loss: 131.2567\n",
      "Epoch [11305/50000], Train Loss: 53.7765, Test Loss: 57.7732\n",
      "Epoch [11310/50000], Train Loss: 52.7146, Test Loss: 54.2139\n",
      "Epoch [11315/50000], Train Loss: 51.3875, Test Loss: 55.1137\n",
      "Epoch [11320/50000], Train Loss: 63.2382, Test Loss: 57.2994\n",
      "Epoch [11325/50000], Train Loss: 59.6087, Test Loss: 71.5061\n",
      "Epoch [11330/50000], Train Loss: 106.3060, Test Loss: 59.5712\n",
      "Epoch [11335/50000], Train Loss: 82.0220, Test Loss: 52.7709\n",
      "Epoch [11340/50000], Train Loss: 50.7542, Test Loss: 68.6681\n",
      "Epoch [11345/50000], Train Loss: 46.0787, Test Loss: 57.7598\n",
      "Epoch [11350/50000], Train Loss: 47.4462, Test Loss: 55.7569\n",
      "Epoch [11355/50000], Train Loss: 43.9725, Test Loss: 53.7796\n",
      "Epoch [11360/50000], Train Loss: 53.3994, Test Loss: 56.4943\n",
      "Epoch [11365/50000], Train Loss: 52.4553, Test Loss: 53.9489\n",
      "Epoch [11370/50000], Train Loss: 58.8319, Test Loss: 55.3141\n",
      "Epoch [11375/50000], Train Loss: 42.0922, Test Loss: 55.7642\n",
      "Epoch [11380/50000], Train Loss: 51.1266, Test Loss: 54.1457\n",
      "Epoch [11385/50000], Train Loss: 49.4392, Test Loss: 54.7406\n",
      "Epoch [11390/50000], Train Loss: 67.6851, Test Loss: 60.2969\n",
      "Epoch [11395/50000], Train Loss: 117.3263, Test Loss: 71.4317\n",
      "Epoch [11400/50000], Train Loss: 67.1823, Test Loss: 58.8448\n",
      "Epoch [11405/50000], Train Loss: 40.4893, Test Loss: 54.9010\n",
      "Epoch [11410/50000], Train Loss: 87.2172, Test Loss: 54.1302\n",
      "Epoch [11415/50000], Train Loss: 51.0023, Test Loss: 54.3038\n",
      "Epoch [11420/50000], Train Loss: 59.2249, Test Loss: 54.2538\n",
      "Epoch [11425/50000], Train Loss: 55.7930, Test Loss: 59.8413\n",
      "Epoch [11430/50000], Train Loss: 70.3054, Test Loss: 51.9275\n",
      "Epoch [11435/50000], Train Loss: 57.9683, Test Loss: 61.3541\n",
      "Epoch [11440/50000], Train Loss: 53.1324, Test Loss: 60.0082\n",
      "Epoch [11445/50000], Train Loss: 50.1638, Test Loss: 62.9813\n",
      "Epoch [11450/50000], Train Loss: 46.8321, Test Loss: 53.3110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11455/50000], Train Loss: 51.0366, Test Loss: 56.1035\n",
      "Epoch [11460/50000], Train Loss: 81.0104, Test Loss: 58.8525\n",
      "Epoch [11465/50000], Train Loss: 56.4103, Test Loss: 54.4341\n",
      "Epoch [11470/50000], Train Loss: 69.9486, Test Loss: 68.8954\n",
      "Epoch [11475/50000], Train Loss: 82.2375, Test Loss: 124.6947\n",
      "Epoch [11480/50000], Train Loss: 75.5987, Test Loss: 72.9926\n",
      "Epoch [11485/50000], Train Loss: 60.4261, Test Loss: 67.9391\n",
      "Epoch [11490/50000], Train Loss: 49.1178, Test Loss: 58.2516\n",
      "Epoch [11495/50000], Train Loss: 50.0129, Test Loss: 58.7172\n",
      "Epoch [11500/50000], Train Loss: 53.4745, Test Loss: 57.6793\n",
      "Epoch [11505/50000], Train Loss: 48.9060, Test Loss: 66.9272\n",
      "Epoch [11510/50000], Train Loss: 61.3053, Test Loss: 54.8004\n",
      "Epoch [11515/50000], Train Loss: 49.3279, Test Loss: 67.2870\n",
      "Epoch [11520/50000], Train Loss: 72.0580, Test Loss: 53.0864\n",
      "Epoch [11525/50000], Train Loss: 134.4816, Test Loss: 413.6220\n",
      "Epoch [11530/50000], Train Loss: 65.7057, Test Loss: 57.8760\n",
      "Epoch [11535/50000], Train Loss: 51.5881, Test Loss: 55.5236\n",
      "Epoch [11540/50000], Train Loss: 114.6188, Test Loss: 53.9242\n",
      "Epoch [11545/50000], Train Loss: 49.2344, Test Loss: 55.6929\n",
      "Epoch [11550/50000], Train Loss: 58.5967, Test Loss: 55.4705\n",
      "Epoch [11555/50000], Train Loss: 51.3470, Test Loss: 67.5884\n",
      "Epoch [11560/50000], Train Loss: 48.4816, Test Loss: 61.9634\n",
      "Epoch [11565/50000], Train Loss: 88.4935, Test Loss: 54.5259\n",
      "Epoch [11570/50000], Train Loss: 55.0499, Test Loss: 56.4070\n",
      "Epoch [11575/50000], Train Loss: 59.5857, Test Loss: 60.0152\n",
      "Epoch [11580/50000], Train Loss: 59.8914, Test Loss: 56.9847\n",
      "Epoch [11585/50000], Train Loss: 205.7929, Test Loss: 51.6956\n",
      "Epoch [11590/50000], Train Loss: 55.5307, Test Loss: 53.1266\n",
      "Epoch [11595/50000], Train Loss: 46.5606, Test Loss: 55.8734\n",
      "Epoch [11600/50000], Train Loss: 55.4407, Test Loss: 58.2100\n",
      "Epoch [11605/50000], Train Loss: 71.9876, Test Loss: 56.5169\n",
      "Epoch [11610/50000], Train Loss: 53.1454, Test Loss: 70.9078\n",
      "Epoch [11615/50000], Train Loss: 55.0374, Test Loss: 55.4568\n",
      "Epoch [11620/50000], Train Loss: 54.3949, Test Loss: 58.4876\n",
      "Epoch [11625/50000], Train Loss: 53.9706, Test Loss: 54.3219\n",
      "Epoch [11630/50000], Train Loss: 51.8193, Test Loss: 54.9754\n",
      "Epoch [11635/50000], Train Loss: 132.7863, Test Loss: 108.1047\n",
      "Epoch [11640/50000], Train Loss: 62.4480, Test Loss: 60.7684\n",
      "Epoch [11645/50000], Train Loss: 48.7669, Test Loss: 53.2352\n",
      "Epoch [11650/50000], Train Loss: 55.6268, Test Loss: 65.0076\n",
      "Epoch [11655/50000], Train Loss: 48.7343, Test Loss: 55.6715\n",
      "Epoch [11660/50000], Train Loss: 51.9021, Test Loss: 69.1080\n",
      "Epoch [11665/50000], Train Loss: 78.2515, Test Loss: 52.0781\n",
      "Epoch [11670/50000], Train Loss: 49.1378, Test Loss: 54.1694\n",
      "Epoch [11675/50000], Train Loss: 60.3060, Test Loss: 53.2122\n",
      "Epoch [11680/50000], Train Loss: 49.0865, Test Loss: 66.5148\n",
      "Epoch [11685/50000], Train Loss: 51.7175, Test Loss: 53.9953\n",
      "Epoch [11690/50000], Train Loss: 56.2250, Test Loss: 55.3451\n",
      "Epoch [11695/50000], Train Loss: 42.4606, Test Loss: 61.1168\n",
      "Epoch [11700/50000], Train Loss: 48.0683, Test Loss: 61.2801\n",
      "Epoch [11705/50000], Train Loss: 45.9670, Test Loss: 53.6848\n",
      "Epoch [11710/50000], Train Loss: 54.4637, Test Loss: 58.4257\n",
      "Epoch [11715/50000], Train Loss: 45.3403, Test Loss: 56.4275\n",
      "Epoch [11720/50000], Train Loss: 58.9296, Test Loss: 60.7875\n",
      "Epoch [11725/50000], Train Loss: 52.4911, Test Loss: 55.6641\n",
      "Epoch [11730/50000], Train Loss: 43.1634, Test Loss: 53.6398\n",
      "Epoch [11735/50000], Train Loss: 83.0001, Test Loss: 240.5772\n",
      "Epoch [11740/50000], Train Loss: 58.5521, Test Loss: 59.1990\n",
      "Epoch [11745/50000], Train Loss: 49.0883, Test Loss: 53.6470\n",
      "Epoch [11750/50000], Train Loss: 52.0559, Test Loss: 57.1432\n",
      "Epoch [11755/50000], Train Loss: 44.8751, Test Loss: 52.8068\n",
      "Epoch [11760/50000], Train Loss: 128.1228, Test Loss: 56.6300\n",
      "Epoch [11765/50000], Train Loss: 59.9272, Test Loss: 62.9940\n",
      "Epoch [11770/50000], Train Loss: 191.2965, Test Loss: 54.6771\n",
      "Epoch [11775/50000], Train Loss: 58.6825, Test Loss: 56.0478\n",
      "Epoch [11780/50000], Train Loss: 53.6819, Test Loss: 52.6817\n",
      "Epoch [11785/50000], Train Loss: 54.8868, Test Loss: 60.3822\n",
      "Epoch [11790/50000], Train Loss: 49.7294, Test Loss: 55.2708\n",
      "Epoch [11795/50000], Train Loss: 46.7345, Test Loss: 55.8827\n",
      "Epoch [11800/50000], Train Loss: 128.0184, Test Loss: 56.1313\n",
      "Epoch [11805/50000], Train Loss: 52.2115, Test Loss: 59.7004\n",
      "Epoch [11810/50000], Train Loss: 42.7267, Test Loss: 57.2225\n",
      "Epoch [11815/50000], Train Loss: 56.4453, Test Loss: 54.5843\n",
      "Epoch [11820/50000], Train Loss: 52.2121, Test Loss: 55.6172\n",
      "Epoch [11825/50000], Train Loss: 54.6959, Test Loss: 110.9086\n",
      "Epoch [11830/50000], Train Loss: 48.4850, Test Loss: 64.0649\n",
      "Epoch [11835/50000], Train Loss: 52.1892, Test Loss: 52.9845\n",
      "Epoch [11840/50000], Train Loss: 79.2167, Test Loss: 54.0573\n",
      "Epoch [11845/50000], Train Loss: 43.7551, Test Loss: 60.7241\n",
      "Epoch [11850/50000], Train Loss: 47.3039, Test Loss: 57.4294\n",
      "Epoch [11855/50000], Train Loss: 70.1827, Test Loss: 57.5299\n",
      "Epoch [11860/50000], Train Loss: 62.3035, Test Loss: 62.6676\n",
      "Epoch [11865/50000], Train Loss: 56.2883, Test Loss: 51.9600\n",
      "Epoch [11870/50000], Train Loss: 52.2496, Test Loss: 51.8753\n",
      "Epoch [11875/50000], Train Loss: 54.1246, Test Loss: 59.4590\n",
      "Epoch [11880/50000], Train Loss: 43.6810, Test Loss: 57.0331\n",
      "Epoch [11885/50000], Train Loss: 53.3345, Test Loss: 56.4458\n",
      "Epoch [11890/50000], Train Loss: 45.3433, Test Loss: 59.4741\n",
      "Epoch [11895/50000], Train Loss: 45.6747, Test Loss: 54.7895\n",
      "Epoch [11900/50000], Train Loss: 65.3971, Test Loss: 53.6636\n",
      "Epoch [11905/50000], Train Loss: 53.8211, Test Loss: 55.8708\n",
      "Epoch [11910/50000], Train Loss: 60.5862, Test Loss: 54.8785\n",
      "Epoch [11915/50000], Train Loss: 45.7331, Test Loss: 53.5291\n",
      "Epoch [11920/50000], Train Loss: 53.9714, Test Loss: 53.7053\n",
      "Epoch [11925/50000], Train Loss: 46.5343, Test Loss: 54.3287\n",
      "Epoch [11930/50000], Train Loss: 48.2462, Test Loss: 57.0628\n",
      "Epoch [11935/50000], Train Loss: 50.8027, Test Loss: 57.1392\n",
      "Epoch [11940/50000], Train Loss: 38.0681, Test Loss: 57.2181\n",
      "Epoch [11945/50000], Train Loss: 51.5389, Test Loss: 53.3529\n",
      "Epoch [11950/50000], Train Loss: 60.7228, Test Loss: 53.9811\n",
      "Epoch [11955/50000], Train Loss: 56.0463, Test Loss: 57.6267\n",
      "Epoch [11960/50000], Train Loss: 74.0353, Test Loss: 65.9098\n",
      "Epoch [11965/50000], Train Loss: 56.1008, Test Loss: 59.0426\n",
      "Epoch [11970/50000], Train Loss: 38.9136, Test Loss: 57.5781\n",
      "Epoch [11975/50000], Train Loss: 183.8385, Test Loss: 57.8511\n",
      "Epoch [11980/50000], Train Loss: 51.9953, Test Loss: 52.8387\n",
      "Epoch [11985/50000], Train Loss: 59.2776, Test Loss: 56.4990\n",
      "Epoch [11990/50000], Train Loss: 55.9776, Test Loss: 56.1942\n",
      "Epoch [11995/50000], Train Loss: 56.1047, Test Loss: 54.3441\n",
      "Epoch [12000/50000], Train Loss: 55.7871, Test Loss: 55.2511\n",
      "Epoch [12005/50000], Train Loss: 50.1237, Test Loss: 58.6175\n",
      "Epoch [12010/50000], Train Loss: 51.5414, Test Loss: 51.6643\n",
      "Epoch [12015/50000], Train Loss: 52.7664, Test Loss: 59.9798\n",
      "Epoch [12020/50000], Train Loss: 57.3226, Test Loss: 58.3432\n",
      "Epoch [12025/50000], Train Loss: 89.0366, Test Loss: 57.0790\n",
      "Epoch [12030/50000], Train Loss: 51.8333, Test Loss: 57.2037\n",
      "Epoch [12035/50000], Train Loss: 50.5231, Test Loss: 57.3060\n",
      "Epoch [12040/50000], Train Loss: 69.1713, Test Loss: 56.9327\n",
      "Epoch [12045/50000], Train Loss: 48.0379, Test Loss: 53.3964\n",
      "Epoch [12050/50000], Train Loss: 49.6849, Test Loss: 51.9107\n",
      "Epoch [12055/50000], Train Loss: 43.9306, Test Loss: 54.3410\n",
      "Epoch [12060/50000], Train Loss: 50.6190, Test Loss: 60.8175\n",
      "Epoch [12065/50000], Train Loss: 49.2289, Test Loss: 53.2752\n",
      "Epoch [12070/50000], Train Loss: 52.7044, Test Loss: 63.5105\n",
      "Epoch [12075/50000], Train Loss: 52.9914, Test Loss: 58.7594\n",
      "Epoch [12080/50000], Train Loss: 62.6598, Test Loss: 52.7528\n",
      "Epoch [12085/50000], Train Loss: 73.0528, Test Loss: 54.2027\n",
      "Epoch [12090/50000], Train Loss: 54.4194, Test Loss: 61.3439\n",
      "Epoch [12095/50000], Train Loss: 51.9887, Test Loss: 55.7671\n",
      "Epoch [12100/50000], Train Loss: 52.5624, Test Loss: 51.8316\n",
      "Epoch [12105/50000], Train Loss: 61.3380, Test Loss: 53.3408\n",
      "Epoch [12110/50000], Train Loss: 59.1784, Test Loss: 65.9037\n",
      "Epoch [12115/50000], Train Loss: 43.8403, Test Loss: 52.1589\n",
      "Epoch [12120/50000], Train Loss: 52.7068, Test Loss: 53.1631\n",
      "Epoch [12125/50000], Train Loss: 46.6746, Test Loss: 56.5676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12130/50000], Train Loss: 66.4850, Test Loss: 57.3873\n",
      "Epoch [12135/50000], Train Loss: 42.9773, Test Loss: 55.4948\n",
      "Epoch [12140/50000], Train Loss: 53.2802, Test Loss: 51.8262\n",
      "Epoch [12145/50000], Train Loss: 51.6833, Test Loss: 59.6567\n",
      "Epoch [12150/50000], Train Loss: 58.3690, Test Loss: 58.4861\n",
      "Epoch [12155/50000], Train Loss: 115.5752, Test Loss: 50.7307\n",
      "Epoch [12160/50000], Train Loss: 60.7901, Test Loss: 57.0221\n",
      "Epoch [12165/50000], Train Loss: 53.3150, Test Loss: 53.6570\n",
      "Epoch [12170/50000], Train Loss: 58.3781, Test Loss: 57.1146\n",
      "Epoch [12175/50000], Train Loss: 56.6231, Test Loss: 57.9144\n",
      "Epoch [12180/50000], Train Loss: 39.8034, Test Loss: 57.8025\n",
      "Epoch [12185/50000], Train Loss: 57.0811, Test Loss: 60.1532\n",
      "Epoch [12190/50000], Train Loss: 57.6635, Test Loss: 55.3749\n",
      "Epoch [12195/50000], Train Loss: 73.3478, Test Loss: 70.6610\n",
      "Epoch [12200/50000], Train Loss: 59.0249, Test Loss: 57.1422\n",
      "Epoch [12205/50000], Train Loss: 57.3478, Test Loss: 60.6294\n",
      "Epoch [12210/50000], Train Loss: 48.7608, Test Loss: 57.3495\n",
      "Epoch [12215/50000], Train Loss: 55.4716, Test Loss: 53.4037\n",
      "Epoch [12220/50000], Train Loss: 51.4781, Test Loss: 57.2607\n",
      "Epoch [12225/50000], Train Loss: 55.8904, Test Loss: 66.6129\n",
      "Epoch [12230/50000], Train Loss: 40.2576, Test Loss: 51.8116\n",
      "Epoch [12235/50000], Train Loss: 49.7661, Test Loss: 52.5579\n",
      "Epoch [12240/50000], Train Loss: 59.3535, Test Loss: 59.2953\n",
      "Epoch [12245/50000], Train Loss: 54.6770, Test Loss: 54.0818\n",
      "Epoch [12250/50000], Train Loss: 53.7178, Test Loss: 66.6779\n",
      "Epoch [12255/50000], Train Loss: 48.4047, Test Loss: 52.9411\n",
      "Epoch [12260/50000], Train Loss: 55.3169, Test Loss: 55.0062\n",
      "Epoch [12265/50000], Train Loss: 53.8568, Test Loss: 54.0536\n",
      "Epoch [12270/50000], Train Loss: 128.8298, Test Loss: 82.3624\n",
      "Epoch [12275/50000], Train Loss: 46.0157, Test Loss: 50.5595\n",
      "Epoch [12280/50000], Train Loss: 56.3147, Test Loss: 52.4021\n",
      "Epoch [12285/50000], Train Loss: 60.2327, Test Loss: 54.4676\n",
      "Epoch [12290/50000], Train Loss: 55.6815, Test Loss: 59.4731\n",
      "Epoch [12295/50000], Train Loss: 53.1720, Test Loss: 54.5672\n",
      "Epoch [12300/50000], Train Loss: 54.7393, Test Loss: 57.8361\n",
      "Epoch [12305/50000], Train Loss: 49.5138, Test Loss: 56.8265\n",
      "Epoch [12310/50000], Train Loss: 45.8938, Test Loss: 52.6620\n",
      "Epoch [12315/50000], Train Loss: 38.5061, Test Loss: 53.9837\n",
      "Epoch [12320/50000], Train Loss: 50.3859, Test Loss: 53.4062\n",
      "Epoch [12325/50000], Train Loss: 55.1412, Test Loss: 62.4194\n",
      "Epoch [12330/50000], Train Loss: 50.2626, Test Loss: 61.0559\n",
      "Epoch [12335/50000], Train Loss: 53.3518, Test Loss: 54.9941\n",
      "Epoch [12340/50000], Train Loss: 44.5001, Test Loss: 53.5564\n",
      "Epoch [12345/50000], Train Loss: 55.9545, Test Loss: 59.9163\n",
      "Epoch [12350/50000], Train Loss: 55.6589, Test Loss: 56.5707\n",
      "Epoch [12355/50000], Train Loss: 47.9025, Test Loss: 57.4018\n",
      "Epoch [12360/50000], Train Loss: 88.7149, Test Loss: 50.6825\n",
      "Epoch [12365/50000], Train Loss: 63.1688, Test Loss: 55.2715\n",
      "Epoch [12370/50000], Train Loss: 61.3756, Test Loss: 55.6898\n",
      "Epoch [12375/50000], Train Loss: 36.2192, Test Loss: 62.5903\n",
      "Epoch [12380/50000], Train Loss: 52.8213, Test Loss: 55.3641\n",
      "Epoch [12385/50000], Train Loss: 56.4693, Test Loss: 53.2221\n",
      "Epoch [12390/50000], Train Loss: 52.4776, Test Loss: 66.4382\n",
      "Epoch [12395/50000], Train Loss: 53.7804, Test Loss: 57.7614\n",
      "Epoch [12400/50000], Train Loss: 53.5515, Test Loss: 57.8457\n",
      "Epoch [12405/50000], Train Loss: 68.4478, Test Loss: 52.4520\n",
      "Epoch [12410/50000], Train Loss: 52.7777, Test Loss: 64.7158\n",
      "Epoch [12415/50000], Train Loss: 60.8734, Test Loss: 66.0171\n",
      "Epoch [12420/50000], Train Loss: 42.8005, Test Loss: 53.5595\n",
      "Epoch [12425/50000], Train Loss: 52.4365, Test Loss: 64.9657\n",
      "Epoch [12430/50000], Train Loss: 53.0506, Test Loss: 56.0000\n",
      "Epoch [12435/50000], Train Loss: 48.0848, Test Loss: 52.5432\n",
      "Epoch [12440/50000], Train Loss: 80.3716, Test Loss: 74.5423\n",
      "Epoch [12445/50000], Train Loss: 36.5185, Test Loss: 52.4566\n",
      "Epoch [12450/50000], Train Loss: 64.4071, Test Loss: 56.7825\n",
      "Epoch [12455/50000], Train Loss: 66.6725, Test Loss: 94.0396\n",
      "Epoch [12460/50000], Train Loss: 59.1111, Test Loss: 58.1983\n",
      "Epoch [12465/50000], Train Loss: 40.6364, Test Loss: 56.1581\n",
      "Epoch [12470/50000], Train Loss: 60.9478, Test Loss: 57.9785\n",
      "Epoch [12475/50000], Train Loss: 47.7973, Test Loss: 52.9783\n",
      "Epoch [12480/50000], Train Loss: 57.7935, Test Loss: 54.9402\n",
      "Epoch [12485/50000], Train Loss: 61.3173, Test Loss: 54.9305\n",
      "Epoch [12490/50000], Train Loss: 41.1582, Test Loss: 53.1226\n",
      "Epoch [12495/50000], Train Loss: 51.4806, Test Loss: 56.0722\n",
      "Epoch [12500/50000], Train Loss: 39.7875, Test Loss: 58.1630\n",
      "Epoch [12505/50000], Train Loss: 66.2069, Test Loss: 68.5384\n",
      "Epoch [12510/50000], Train Loss: 40.4263, Test Loss: 51.8225\n",
      "Epoch [12515/50000], Train Loss: 55.3522, Test Loss: 52.5459\n",
      "Epoch [12520/50000], Train Loss: 47.9987, Test Loss: 58.6471\n",
      "Epoch [12525/50000], Train Loss: 60.7055, Test Loss: 56.9927\n",
      "Epoch [12530/50000], Train Loss: 64.7617, Test Loss: 53.8872\n",
      "Epoch [12535/50000], Train Loss: 88.3478, Test Loss: 54.3446\n",
      "Epoch [12540/50000], Train Loss: 115.0952, Test Loss: 54.0396\n",
      "Epoch [12545/50000], Train Loss: 53.2394, Test Loss: 53.4430\n",
      "Epoch [12550/50000], Train Loss: 36.3592, Test Loss: 50.7360\n",
      "Epoch [12555/50000], Train Loss: 48.7069, Test Loss: 55.0321\n",
      "Epoch [12560/50000], Train Loss: 50.7955, Test Loss: 55.3594\n",
      "Epoch [12565/50000], Train Loss: 55.4449, Test Loss: 52.4505\n",
      "Epoch [12570/50000], Train Loss: 51.6505, Test Loss: 60.6078\n",
      "Epoch [12575/50000], Train Loss: 47.2063, Test Loss: 54.3780\n",
      "Epoch [12580/50000], Train Loss: 39.2403, Test Loss: 57.0424\n",
      "Epoch [12585/50000], Train Loss: 50.8136, Test Loss: 59.9955\n",
      "Epoch [12590/50000], Train Loss: 41.1188, Test Loss: 54.4886\n",
      "Epoch [12595/50000], Train Loss: 57.4399, Test Loss: 53.1057\n",
      "Epoch [12600/50000], Train Loss: 64.0174, Test Loss: 95.4044\n",
      "Epoch [12605/50000], Train Loss: 48.1482, Test Loss: 74.6193\n",
      "Epoch [12610/50000], Train Loss: 48.3679, Test Loss: 53.8778\n",
      "Epoch [12615/50000], Train Loss: 52.3954, Test Loss: 53.6469\n",
      "Epoch [12620/50000], Train Loss: 56.2528, Test Loss: 51.0859\n",
      "Epoch [12625/50000], Train Loss: 43.3681, Test Loss: 53.9050\n",
      "Epoch [12630/50000], Train Loss: 41.7845, Test Loss: 51.1401\n",
      "Epoch [12635/50000], Train Loss: 45.1193, Test Loss: 58.3222\n",
      "Epoch [12640/50000], Train Loss: 72.8428, Test Loss: 53.2314\n",
      "Epoch [12645/50000], Train Loss: 55.0662, Test Loss: 51.9390\n",
      "Epoch [12650/50000], Train Loss: 47.0574, Test Loss: 49.9507\n",
      "Epoch [12655/50000], Train Loss: 58.4571, Test Loss: 61.8766\n",
      "Epoch [12660/50000], Train Loss: 51.7994, Test Loss: 63.0338\n",
      "Epoch [12665/50000], Train Loss: 62.7269, Test Loss: 51.7842\n",
      "Epoch [12670/50000], Train Loss: 48.6815, Test Loss: 62.1865\n",
      "Epoch [12675/50000], Train Loss: 139.7677, Test Loss: 52.5261\n",
      "Epoch [12680/50000], Train Loss: 49.9534, Test Loss: 59.3444\n",
      "Epoch [12685/50000], Train Loss: 44.9026, Test Loss: 57.7121\n",
      "Epoch [12690/50000], Train Loss: 48.5047, Test Loss: 53.9192\n",
      "Epoch [12695/50000], Train Loss: 52.4078, Test Loss: 60.6495\n",
      "Epoch [12700/50000], Train Loss: 49.4880, Test Loss: 60.4142\n",
      "Epoch [12705/50000], Train Loss: 36.9079, Test Loss: 52.4010\n",
      "Epoch [12710/50000], Train Loss: 45.7400, Test Loss: 53.3165\n",
      "Epoch [12715/50000], Train Loss: 48.9144, Test Loss: 56.3278\n",
      "Epoch [12720/50000], Train Loss: 66.1257, Test Loss: 58.5065\n",
      "Epoch [12725/50000], Train Loss: 48.0934, Test Loss: 56.2936\n",
      "Epoch [12730/50000], Train Loss: 59.2147, Test Loss: 60.7105\n",
      "Epoch [12735/50000], Train Loss: 36.1850, Test Loss: 51.2658\n",
      "Epoch [12740/50000], Train Loss: 49.7659, Test Loss: 52.3907\n",
      "Epoch [12745/50000], Train Loss: 44.6267, Test Loss: 61.6367\n",
      "Epoch [12750/50000], Train Loss: 51.9941, Test Loss: 53.4652\n",
      "Epoch [12755/50000], Train Loss: 46.4891, Test Loss: 57.4450\n",
      "Epoch [12760/50000], Train Loss: 47.8819, Test Loss: 54.7880\n",
      "Epoch [12765/50000], Train Loss: 50.8608, Test Loss: 59.0248\n",
      "Epoch [12770/50000], Train Loss: 46.9525, Test Loss: 64.6466\n",
      "Epoch [12775/50000], Train Loss: 55.9137, Test Loss: 63.7569\n",
      "Epoch [12780/50000], Train Loss: 47.5860, Test Loss: 57.3863\n",
      "Epoch [12785/50000], Train Loss: 51.0991, Test Loss: 67.0080\n",
      "Epoch [12790/50000], Train Loss: 44.8842, Test Loss: 79.7518\n",
      "Epoch [12795/50000], Train Loss: 39.9812, Test Loss: 56.3317\n",
      "Epoch [12800/50000], Train Loss: 44.0697, Test Loss: 51.8454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12805/50000], Train Loss: 46.2370, Test Loss: 56.1126\n",
      "Epoch [12810/50000], Train Loss: 48.8926, Test Loss: 51.3414\n",
      "Epoch [12815/50000], Train Loss: 55.4045, Test Loss: 62.0249\n",
      "Epoch [12820/50000], Train Loss: 66.9411, Test Loss: 57.3220\n",
      "Epoch [12825/50000], Train Loss: 50.7525, Test Loss: 52.8162\n",
      "Epoch [12830/50000], Train Loss: 39.9798, Test Loss: 53.1150\n",
      "Epoch [12835/50000], Train Loss: 46.8473, Test Loss: 51.5367\n",
      "Epoch [12840/50000], Train Loss: 50.2771, Test Loss: 73.4119\n",
      "Epoch [12845/50000], Train Loss: 54.4123, Test Loss: 54.8782\n",
      "Epoch [12850/50000], Train Loss: 49.3673, Test Loss: 55.4751\n",
      "Epoch [12855/50000], Train Loss: 32.5152, Test Loss: 54.2186\n",
      "Epoch [12860/50000], Train Loss: 41.1755, Test Loss: 55.0697\n",
      "Epoch [12865/50000], Train Loss: 64.5918, Test Loss: 61.2268\n",
      "Epoch [12870/50000], Train Loss: 48.5588, Test Loss: 55.4056\n",
      "Epoch [12875/50000], Train Loss: 46.1412, Test Loss: 55.2197\n",
      "Epoch [12880/50000], Train Loss: 62.0779, Test Loss: 52.7341\n",
      "Epoch [12885/50000], Train Loss: 47.1584, Test Loss: 60.2817\n",
      "Epoch [12890/50000], Train Loss: 49.9422, Test Loss: 62.5902\n",
      "Epoch [12895/50000], Train Loss: 57.9679, Test Loss: 51.8154\n",
      "Epoch [12900/50000], Train Loss: 48.1263, Test Loss: 61.7070\n",
      "Epoch [12905/50000], Train Loss: 65.6842, Test Loss: 68.7979\n",
      "Epoch [12910/50000], Train Loss: 48.5000, Test Loss: 57.9580\n",
      "Epoch [12915/50000], Train Loss: 83.4908, Test Loss: 51.8218\n",
      "Epoch [12920/50000], Train Loss: 46.2825, Test Loss: 53.3233\n",
      "Epoch [12925/50000], Train Loss: 54.4231, Test Loss: 52.7001\n",
      "Epoch [12930/50000], Train Loss: 67.3775, Test Loss: 62.1839\n",
      "Epoch [12935/50000], Train Loss: 30.4686, Test Loss: 53.8374\n",
      "Epoch [12940/50000], Train Loss: 48.5671, Test Loss: 58.4790\n",
      "Epoch [12945/50000], Train Loss: 48.7423, Test Loss: 52.3237\n",
      "Epoch [12950/50000], Train Loss: 45.3133, Test Loss: 56.9410\n",
      "Epoch [12955/50000], Train Loss: 48.6441, Test Loss: 51.9748\n",
      "Epoch [12960/50000], Train Loss: 43.4753, Test Loss: 51.9284\n",
      "Epoch [12965/50000], Train Loss: 51.8499, Test Loss: 52.5479\n",
      "Epoch [12970/50000], Train Loss: 56.6419, Test Loss: 52.9009\n",
      "Epoch [12975/50000], Train Loss: 43.7331, Test Loss: 54.1293\n",
      "Epoch [12980/50000], Train Loss: 40.6927, Test Loss: 65.1834\n",
      "Epoch [12985/50000], Train Loss: 48.0876, Test Loss: 56.7835\n",
      "Epoch [12990/50000], Train Loss: 55.9141, Test Loss: 53.6212\n",
      "Epoch [12995/50000], Train Loss: 53.6911, Test Loss: 54.6184\n",
      "Epoch [13000/50000], Train Loss: 54.9094, Test Loss: 57.1037\n",
      "Epoch [13005/50000], Train Loss: 81.7101, Test Loss: 59.0122\n",
      "Epoch [13010/50000], Train Loss: 41.4792, Test Loss: 58.3703\n",
      "Epoch [13015/50000], Train Loss: 43.2998, Test Loss: 56.5379\n",
      "Epoch [13020/50000], Train Loss: 108.2265, Test Loss: 62.5460\n",
      "Epoch [13025/50000], Train Loss: 121.8917, Test Loss: 51.3370\n",
      "Epoch [13030/50000], Train Loss: 50.9858, Test Loss: 51.1055\n",
      "Epoch [13035/50000], Train Loss: 32.5718, Test Loss: 52.0576\n",
      "Epoch [13040/50000], Train Loss: 49.1094, Test Loss: 52.5523\n",
      "Epoch [13045/50000], Train Loss: 45.4619, Test Loss: 56.8525\n",
      "Epoch [13050/50000], Train Loss: 79.4137, Test Loss: 88.5181\n",
      "Epoch [13055/50000], Train Loss: 65.0721, Test Loss: 63.7146\n",
      "Epoch [13060/50000], Train Loss: 74.8207, Test Loss: 52.1511\n",
      "Epoch [13065/50000], Train Loss: 57.6677, Test Loss: 56.1335\n",
      "Epoch [13070/50000], Train Loss: 47.4745, Test Loss: 54.3210\n",
      "Epoch [13075/50000], Train Loss: 53.2981, Test Loss: 52.2607\n",
      "Epoch [13080/50000], Train Loss: 49.9220, Test Loss: 57.1110\n",
      "Epoch [13085/50000], Train Loss: 42.2322, Test Loss: 55.1043\n",
      "Epoch [13090/50000], Train Loss: 51.4682, Test Loss: 53.9975\n",
      "Epoch [13095/50000], Train Loss: 52.2917, Test Loss: 60.7896\n",
      "Epoch [13100/50000], Train Loss: 45.3889, Test Loss: 54.4329\n",
      "Epoch [13105/50000], Train Loss: 51.6982, Test Loss: 57.3490\n",
      "Epoch [13110/50000], Train Loss: 172.0150, Test Loss: 53.3681\n",
      "Epoch [13115/50000], Train Loss: 51.6898, Test Loss: 55.9067\n",
      "Epoch [13120/50000], Train Loss: 48.4064, Test Loss: 55.2987\n",
      "Epoch [13125/50000], Train Loss: 39.2381, Test Loss: 51.9456\n",
      "Epoch [13130/50000], Train Loss: 52.4827, Test Loss: 58.0903\n",
      "Epoch [13135/50000], Train Loss: 50.3324, Test Loss: 54.8902\n",
      "Epoch [13140/50000], Train Loss: 55.3400, Test Loss: 54.2684\n",
      "Epoch [13145/50000], Train Loss: 54.2504, Test Loss: 58.4647\n",
      "Epoch [13150/50000], Train Loss: 58.0859, Test Loss: 59.4281\n",
      "Epoch [13155/50000], Train Loss: 55.2921, Test Loss: 53.9521\n",
      "Epoch [13160/50000], Train Loss: 42.6194, Test Loss: 55.5799\n",
      "Epoch [13165/50000], Train Loss: 49.7564, Test Loss: 54.1124\n",
      "Epoch [13170/50000], Train Loss: 54.3269, Test Loss: 56.8420\n",
      "Epoch [13175/50000], Train Loss: 37.0832, Test Loss: 54.2113\n",
      "Epoch [13180/50000], Train Loss: 59.4452, Test Loss: 61.0107\n",
      "Epoch [13185/50000], Train Loss: 54.1579, Test Loss: 67.4460\n",
      "Epoch [13190/50000], Train Loss: 105.5227, Test Loss: 51.9293\n",
      "Epoch [13195/50000], Train Loss: 47.8901, Test Loss: 59.7946\n",
      "Epoch [13200/50000], Train Loss: 46.0922, Test Loss: 59.0866\n",
      "Epoch [13205/50000], Train Loss: 46.0102, Test Loss: 52.3934\n",
      "Epoch [13210/50000], Train Loss: 36.9254, Test Loss: 51.9185\n",
      "Epoch [13215/50000], Train Loss: 46.9336, Test Loss: 60.4239\n",
      "Epoch [13220/50000], Train Loss: 61.9356, Test Loss: 62.9516\n",
      "Epoch [13225/50000], Train Loss: 57.0760, Test Loss: 52.4448\n",
      "Epoch [13230/50000], Train Loss: 57.6997, Test Loss: 51.8741\n",
      "Epoch [13235/50000], Train Loss: 46.3057, Test Loss: 53.5444\n",
      "Epoch [13240/50000], Train Loss: 49.1140, Test Loss: 53.8557\n",
      "Epoch [13245/50000], Train Loss: 63.7498, Test Loss: 55.1443\n",
      "Epoch [13250/50000], Train Loss: 60.2742, Test Loss: 54.7390\n",
      "Epoch [13255/50000], Train Loss: 45.4707, Test Loss: 54.5230\n",
      "Epoch [13260/50000], Train Loss: 42.9837, Test Loss: 58.2887\n",
      "Epoch [13265/50000], Train Loss: 45.3651, Test Loss: 68.1827\n",
      "Epoch [13270/50000], Train Loss: 51.5781, Test Loss: 52.9299\n",
      "Epoch [13275/50000], Train Loss: 40.9572, Test Loss: 52.7395\n",
      "Epoch [13280/50000], Train Loss: 47.0713, Test Loss: 79.2841\n",
      "Epoch [13285/50000], Train Loss: 46.5030, Test Loss: 52.9468\n",
      "Epoch [13290/50000], Train Loss: 111.9364, Test Loss: 123.1882\n",
      "Epoch [13295/50000], Train Loss: 53.4536, Test Loss: 61.0750\n",
      "Epoch [13300/50000], Train Loss: 47.5436, Test Loss: 57.4908\n",
      "Epoch [13305/50000], Train Loss: 41.1572, Test Loss: 59.5947\n",
      "Epoch [13310/50000], Train Loss: 54.4086, Test Loss: 53.5048\n",
      "Epoch [13315/50000], Train Loss: 47.2870, Test Loss: 58.6164\n",
      "Epoch [13320/50000], Train Loss: 62.0825, Test Loss: 57.0174\n",
      "Epoch [13325/50000], Train Loss: 48.3010, Test Loss: 53.8851\n",
      "Epoch [13330/50000], Train Loss: 57.2013, Test Loss: 54.5581\n",
      "Epoch [13335/50000], Train Loss: 51.0812, Test Loss: 57.6177\n",
      "Epoch [13340/50000], Train Loss: 50.1612, Test Loss: 53.4383\n",
      "Epoch [13345/50000], Train Loss: 42.9802, Test Loss: 52.7119\n",
      "Epoch [13350/50000], Train Loss: 136.9579, Test Loss: 52.1545\n",
      "Epoch [13355/50000], Train Loss: 44.4334, Test Loss: 52.1908\n",
      "Epoch [13360/50000], Train Loss: 49.0643, Test Loss: 53.6760\n",
      "Epoch [13365/50000], Train Loss: 53.6580, Test Loss: 58.4499\n",
      "Epoch [13370/50000], Train Loss: 48.3257, Test Loss: 55.3040\n",
      "Epoch [13375/50000], Train Loss: 44.6355, Test Loss: 54.9486\n",
      "Epoch [13380/50000], Train Loss: 56.0867, Test Loss: 72.9435\n",
      "Epoch [13385/50000], Train Loss: 99.3611, Test Loss: 56.6972\n",
      "Epoch [13390/50000], Train Loss: 48.5230, Test Loss: 53.9447\n",
      "Epoch [13395/50000], Train Loss: 43.1595, Test Loss: 52.7008\n",
      "Epoch [13400/50000], Train Loss: 45.1921, Test Loss: 52.9496\n",
      "Epoch [13405/50000], Train Loss: 43.5356, Test Loss: 62.8492\n",
      "Epoch [13410/50000], Train Loss: 53.9594, Test Loss: 56.4894\n",
      "Epoch [13415/50000], Train Loss: 59.7447, Test Loss: 52.9920\n",
      "Epoch [13420/50000], Train Loss: 44.3106, Test Loss: 59.2948\n",
      "Epoch [13425/50000], Train Loss: 48.2602, Test Loss: 52.1574\n",
      "Epoch [13430/50000], Train Loss: 37.5181, Test Loss: 51.1729\n",
      "Epoch [13435/50000], Train Loss: 77.0647, Test Loss: 60.2549\n",
      "Epoch [13440/50000], Train Loss: 65.0766, Test Loss: 55.0388\n",
      "Epoch [13445/50000], Train Loss: 49.8023, Test Loss: 52.0426\n",
      "Epoch [13450/50000], Train Loss: 39.1584, Test Loss: 59.6795\n",
      "Epoch [13455/50000], Train Loss: 46.0849, Test Loss: 55.6613\n",
      "Epoch [13460/50000], Train Loss: 42.1700, Test Loss: 59.2928\n",
      "Epoch [13465/50000], Train Loss: 80.4698, Test Loss: 53.2300\n",
      "Epoch [13470/50000], Train Loss: 49.9921, Test Loss: 58.5594\n",
      "Epoch [13475/50000], Train Loss: 83.5056, Test Loss: 53.0467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13480/50000], Train Loss: 80.5204, Test Loss: 53.9672\n",
      "Epoch [13485/50000], Train Loss: 56.5864, Test Loss: 61.5524\n",
      "Epoch [13490/50000], Train Loss: 53.3368, Test Loss: 54.1231\n",
      "Epoch [13495/50000], Train Loss: 49.0881, Test Loss: 60.6584\n",
      "Epoch [13500/50000], Train Loss: 59.3766, Test Loss: 54.1291\n",
      "Epoch [13505/50000], Train Loss: 40.8989, Test Loss: 53.2243\n",
      "Epoch [13510/50000], Train Loss: 48.2797, Test Loss: 60.5247\n",
      "Epoch [13515/50000], Train Loss: 49.7233, Test Loss: 75.9503\n",
      "Epoch [13520/50000], Train Loss: 57.2750, Test Loss: 51.8845\n",
      "Epoch [13525/50000], Train Loss: 40.3080, Test Loss: 56.0850\n",
      "Epoch [13530/50000], Train Loss: 50.0940, Test Loss: 54.8595\n",
      "Epoch [13535/50000], Train Loss: 77.7510, Test Loss: 54.9685\n",
      "Epoch [13540/50000], Train Loss: 48.5231, Test Loss: 66.1744\n",
      "Epoch [13545/50000], Train Loss: 43.4789, Test Loss: 54.3650\n",
      "Epoch [13550/50000], Train Loss: 40.9849, Test Loss: 62.3532\n",
      "Epoch [13555/50000], Train Loss: 44.0591, Test Loss: 54.8650\n",
      "Epoch [13560/50000], Train Loss: 52.2136, Test Loss: 52.1218\n",
      "Epoch [13565/50000], Train Loss: 48.7553, Test Loss: 64.9642\n",
      "Epoch [13570/50000], Train Loss: 58.6937, Test Loss: 56.7672\n",
      "Epoch [13575/50000], Train Loss: 39.2894, Test Loss: 69.3088\n",
      "Epoch [13580/50000], Train Loss: 41.4917, Test Loss: 51.5765\n",
      "Epoch [13585/50000], Train Loss: 42.6258, Test Loss: 54.0510\n",
      "Epoch [13590/50000], Train Loss: 42.0662, Test Loss: 50.6019\n",
      "Epoch [13595/50000], Train Loss: 46.2568, Test Loss: 52.1070\n",
      "Epoch [13600/50000], Train Loss: 46.8447, Test Loss: 52.3978\n",
      "Epoch [13605/50000], Train Loss: 46.1467, Test Loss: 52.0883\n",
      "Epoch [13610/50000], Train Loss: 40.3968, Test Loss: 63.6198\n",
      "Epoch [13615/50000], Train Loss: 50.1940, Test Loss: 58.5218\n",
      "Epoch [13620/50000], Train Loss: 46.0087, Test Loss: 58.0354\n",
      "Epoch [13625/50000], Train Loss: 40.3165, Test Loss: 52.2371\n",
      "Epoch [13630/50000], Train Loss: 45.7084, Test Loss: 61.0799\n",
      "Epoch [13635/50000], Train Loss: 39.5468, Test Loss: 52.3262\n",
      "Epoch [13640/50000], Train Loss: 111.2305, Test Loss: 58.6786\n",
      "Epoch [13645/50000], Train Loss: 52.7276, Test Loss: 50.3456\n",
      "Epoch [13650/50000], Train Loss: 50.9929, Test Loss: 52.1940\n",
      "Epoch [13655/50000], Train Loss: 40.2532, Test Loss: 54.3616\n",
      "Epoch [13660/50000], Train Loss: 60.0439, Test Loss: 58.5414\n",
      "Epoch [13665/50000], Train Loss: 41.4983, Test Loss: 53.3919\n",
      "Epoch [13670/50000], Train Loss: 43.3906, Test Loss: 74.2926\n",
      "Epoch [13675/50000], Train Loss: 48.0187, Test Loss: 54.3068\n",
      "Epoch [13680/50000], Train Loss: 50.6619, Test Loss: 54.6316\n",
      "Epoch [13685/50000], Train Loss: 43.9793, Test Loss: 54.0029\n",
      "Epoch [13690/50000], Train Loss: 107.5924, Test Loss: 54.1543\n",
      "Epoch [13695/50000], Train Loss: 51.1794, Test Loss: 54.7905\n",
      "Epoch [13700/50000], Train Loss: 44.8524, Test Loss: 50.8376\n",
      "Epoch [13705/50000], Train Loss: 47.6845, Test Loss: 54.0598\n",
      "Epoch [13710/50000], Train Loss: 44.9409, Test Loss: 51.6909\n",
      "Epoch [13715/50000], Train Loss: 105.9156, Test Loss: 50.8702\n",
      "Epoch [13720/50000], Train Loss: 48.8818, Test Loss: 57.6152\n",
      "Epoch [13725/50000], Train Loss: 51.3692, Test Loss: 54.5112\n",
      "Epoch [13730/50000], Train Loss: 47.7956, Test Loss: 55.7488\n",
      "Epoch [13735/50000], Train Loss: 62.2167, Test Loss: 56.2226\n",
      "Epoch [13740/50000], Train Loss: 62.2901, Test Loss: 130.8606\n",
      "Epoch [13745/50000], Train Loss: 39.3488, Test Loss: 55.7739\n",
      "Epoch [13750/50000], Train Loss: 84.0286, Test Loss: 54.3285\n",
      "Epoch [13755/50000], Train Loss: 56.0455, Test Loss: 58.5034\n",
      "Epoch [13760/50000], Train Loss: 96.6420, Test Loss: 56.3195\n",
      "Epoch [13765/50000], Train Loss: 75.9664, Test Loss: 114.0241\n",
      "Epoch [13770/50000], Train Loss: 47.8340, Test Loss: 56.2186\n",
      "Epoch [13775/50000], Train Loss: 60.5966, Test Loss: 59.5027\n",
      "Epoch [13780/50000], Train Loss: 113.8605, Test Loss: 54.4358\n",
      "Epoch [13785/50000], Train Loss: 52.8219, Test Loss: 52.3249\n",
      "Epoch [13790/50000], Train Loss: 54.7584, Test Loss: 63.2385\n",
      "Epoch [13795/50000], Train Loss: 49.6392, Test Loss: 60.9613\n",
      "Epoch [13800/50000], Train Loss: 44.2812, Test Loss: 49.9597\n",
      "Epoch [13805/50000], Train Loss: 53.0066, Test Loss: 73.8158\n",
      "Epoch [13810/50000], Train Loss: 44.3629, Test Loss: 55.4426\n",
      "Epoch [13815/50000], Train Loss: 46.1472, Test Loss: 57.9397\n",
      "Epoch [13820/50000], Train Loss: 36.4525, Test Loss: 49.8569\n",
      "Epoch [13825/50000], Train Loss: 199.9210, Test Loss: 50.5469\n",
      "Epoch [13830/50000], Train Loss: 50.3211, Test Loss: 58.0184\n",
      "Epoch [13835/50000], Train Loss: 45.7058, Test Loss: 55.4998\n",
      "Epoch [13840/50000], Train Loss: 66.3187, Test Loss: 57.8985\n",
      "Epoch [13845/50000], Train Loss: 51.9968, Test Loss: 57.0046\n",
      "Epoch [13850/50000], Train Loss: 81.2239, Test Loss: 51.3748\n",
      "Epoch [13855/50000], Train Loss: 47.9786, Test Loss: 57.5086\n",
      "Epoch [13860/50000], Train Loss: 39.3100, Test Loss: 53.6117\n",
      "Epoch [13865/50000], Train Loss: 44.2979, Test Loss: 52.9156\n",
      "Epoch [13870/50000], Train Loss: 41.5206, Test Loss: 51.7761\n",
      "Epoch [13875/50000], Train Loss: 44.4822, Test Loss: 51.7521\n",
      "Epoch [13880/50000], Train Loss: 47.7777, Test Loss: 55.3281\n",
      "Epoch [13885/50000], Train Loss: 58.0457, Test Loss: 54.2896\n",
      "Epoch [13890/50000], Train Loss: 66.0851, Test Loss: 53.1500\n",
      "Epoch [13895/50000], Train Loss: 136.9256, Test Loss: 50.9009\n",
      "Epoch [13900/50000], Train Loss: 50.0105, Test Loss: 66.0161\n",
      "Epoch [13905/50000], Train Loss: 59.9262, Test Loss: 52.0924\n",
      "Epoch [13910/50000], Train Loss: 43.4290, Test Loss: 54.3008\n",
      "Epoch [13915/50000], Train Loss: 51.5897, Test Loss: 51.9130\n",
      "Epoch [13920/50000], Train Loss: 52.2405, Test Loss: 53.6232\n",
      "Epoch [13925/50000], Train Loss: 49.3086, Test Loss: 58.7966\n",
      "Epoch [13930/50000], Train Loss: 41.9983, Test Loss: 54.3158\n",
      "Epoch [13935/50000], Train Loss: 66.0551, Test Loss: 53.0291\n",
      "Epoch [13940/50000], Train Loss: 54.3237, Test Loss: 53.1914\n",
      "Epoch [13945/50000], Train Loss: 52.8506, Test Loss: 53.4717\n",
      "Epoch [13950/50000], Train Loss: 165.7654, Test Loss: 50.5965\n",
      "Epoch [13955/50000], Train Loss: 44.5482, Test Loss: 53.7110\n",
      "Epoch [13960/50000], Train Loss: 41.2689, Test Loss: 52.8895\n",
      "Epoch [13965/50000], Train Loss: 32.8620, Test Loss: 52.7867\n",
      "Epoch [13970/50000], Train Loss: 70.8408, Test Loss: 56.8051\n",
      "Epoch [13975/50000], Train Loss: 51.9853, Test Loss: 52.1377\n",
      "Epoch [13980/50000], Train Loss: 44.8359, Test Loss: 54.7410\n",
      "Epoch [13985/50000], Train Loss: 50.8244, Test Loss: 54.9686\n",
      "Epoch [13990/50000], Train Loss: 38.0181, Test Loss: 50.3713\n",
      "Epoch [13995/50000], Train Loss: 42.0971, Test Loss: 57.1024\n",
      "Epoch [14000/50000], Train Loss: 101.3401, Test Loss: 51.8560\n",
      "Epoch [14005/50000], Train Loss: 43.7872, Test Loss: 62.6432\n",
      "Epoch [14010/50000], Train Loss: 45.0886, Test Loss: 52.1917\n",
      "Epoch [14015/50000], Train Loss: 63.4534, Test Loss: 52.0009\n",
      "Epoch [14020/50000], Train Loss: 176.6776, Test Loss: 50.9273\n",
      "Epoch [14025/50000], Train Loss: 47.4323, Test Loss: 60.3203\n",
      "Epoch [14030/50000], Train Loss: 53.8606, Test Loss: 52.7995\n",
      "Epoch [14035/50000], Train Loss: 43.9482, Test Loss: 54.2220\n",
      "Epoch [14040/50000], Train Loss: 39.2922, Test Loss: 53.0794\n",
      "Epoch [14045/50000], Train Loss: 46.0349, Test Loss: 58.1708\n",
      "Epoch [14050/50000], Train Loss: 41.2372, Test Loss: 51.8044\n",
      "Epoch [14055/50000], Train Loss: 42.0633, Test Loss: 55.6186\n",
      "Epoch [14060/50000], Train Loss: 42.2518, Test Loss: 55.7784\n",
      "Epoch [14065/50000], Train Loss: 44.8088, Test Loss: 54.4568\n",
      "Epoch [14070/50000], Train Loss: 44.9162, Test Loss: 56.8620\n",
      "Epoch [14075/50000], Train Loss: 41.5811, Test Loss: 58.0109\n",
      "Epoch [14080/50000], Train Loss: 43.1010, Test Loss: 54.5224\n",
      "Epoch [14085/50000], Train Loss: 34.3621, Test Loss: 50.5386\n",
      "Epoch [14090/50000], Train Loss: 47.3302, Test Loss: 55.3460\n",
      "Epoch [14095/50000], Train Loss: 86.5758, Test Loss: 53.4964\n",
      "Epoch [14100/50000], Train Loss: 62.2676, Test Loss: 50.6891\n",
      "Epoch [14105/50000], Train Loss: 57.3263, Test Loss: 81.6229\n",
      "Epoch [14110/50000], Train Loss: 42.2780, Test Loss: 57.0019\n",
      "Epoch [14115/50000], Train Loss: 48.6211, Test Loss: 57.2143\n",
      "Epoch [14120/50000], Train Loss: 46.5972, Test Loss: 62.9469\n",
      "Epoch [14125/50000], Train Loss: 44.4879, Test Loss: 52.0747\n",
      "Epoch [14130/50000], Train Loss: 42.4367, Test Loss: 56.0633\n",
      "Epoch [14135/50000], Train Loss: 47.5168, Test Loss: 51.0013\n",
      "Epoch [14140/50000], Train Loss: 50.4974, Test Loss: 55.9688\n",
      "Epoch [14145/50000], Train Loss: 47.7043, Test Loss: 54.1659\n",
      "Epoch [14150/50000], Train Loss: 53.8429, Test Loss: 52.6854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14155/50000], Train Loss: 40.6175, Test Loss: 54.1735\n",
      "Epoch [14160/50000], Train Loss: 69.3185, Test Loss: 59.7308\n",
      "Epoch [14165/50000], Train Loss: 116.0712, Test Loss: 54.4069\n",
      "Epoch [14170/50000], Train Loss: 95.4503, Test Loss: 53.0270\n",
      "Epoch [14175/50000], Train Loss: 41.2617, Test Loss: 59.9595\n",
      "Epoch [14180/50000], Train Loss: 44.0733, Test Loss: 81.7846\n",
      "Epoch [14185/50000], Train Loss: 45.8569, Test Loss: 54.4501\n",
      "Epoch [14190/50000], Train Loss: 49.7061, Test Loss: 56.0061\n",
      "Epoch [14195/50000], Train Loss: 60.1889, Test Loss: 51.9498\n",
      "Epoch [14200/50000], Train Loss: 38.3811, Test Loss: 54.5092\n",
      "Epoch [14205/50000], Train Loss: 40.7801, Test Loss: 51.1652\n",
      "Epoch [14210/50000], Train Loss: 95.1512, Test Loss: 117.6306\n",
      "Epoch [14215/50000], Train Loss: 44.6019, Test Loss: 51.1517\n",
      "Epoch [14220/50000], Train Loss: 84.9847, Test Loss: 50.7351\n",
      "Epoch [14225/50000], Train Loss: 64.0109, Test Loss: 54.6898\n",
      "Epoch [14230/50000], Train Loss: 43.1170, Test Loss: 53.7425\n",
      "Epoch [14235/50000], Train Loss: 48.1283, Test Loss: 55.6316\n",
      "Epoch [14240/50000], Train Loss: 49.0526, Test Loss: 58.7093\n",
      "Epoch [14245/50000], Train Loss: 53.2110, Test Loss: 58.8628\n",
      "Epoch [14250/50000], Train Loss: 48.2585, Test Loss: 60.6582\n",
      "Epoch [14255/50000], Train Loss: 54.4574, Test Loss: 56.7292\n",
      "Epoch [14260/50000], Train Loss: 43.9248, Test Loss: 50.0154\n",
      "Epoch [14265/50000], Train Loss: 52.4866, Test Loss: 64.0646\n",
      "Epoch [14270/50000], Train Loss: 40.9446, Test Loss: 64.1179\n",
      "Epoch [14275/50000], Train Loss: 74.9469, Test Loss: 59.9880\n",
      "Epoch [14280/50000], Train Loss: 76.4064, Test Loss: 53.6571\n",
      "Epoch [14285/50000], Train Loss: 47.3083, Test Loss: 54.9143\n",
      "Epoch [14290/50000], Train Loss: 74.0415, Test Loss: 57.3449\n",
      "Epoch [14295/50000], Train Loss: 46.0956, Test Loss: 70.3570\n",
      "Epoch [14300/50000], Train Loss: 46.4719, Test Loss: 54.9096\n",
      "Epoch [14305/50000], Train Loss: 44.5778, Test Loss: 56.9795\n",
      "Epoch [14310/50000], Train Loss: 42.9378, Test Loss: 53.3070\n",
      "Epoch [14315/50000], Train Loss: 49.0751, Test Loss: 51.4235\n",
      "Epoch [14320/50000], Train Loss: 100.4868, Test Loss: 56.5687\n",
      "Epoch [14325/50000], Train Loss: 55.5187, Test Loss: 60.2421\n",
      "Epoch [14330/50000], Train Loss: 41.1759, Test Loss: 53.1647\n",
      "Epoch [14335/50000], Train Loss: 73.7472, Test Loss: 51.8334\n",
      "Epoch [14340/50000], Train Loss: 43.7125, Test Loss: 53.9759\n",
      "Epoch [14345/50000], Train Loss: 37.5075, Test Loss: 58.0407\n",
      "Epoch [14350/50000], Train Loss: 37.6860, Test Loss: 56.6114\n",
      "Epoch [14355/50000], Train Loss: 44.4067, Test Loss: 50.5039\n",
      "Epoch [14360/50000], Train Loss: 38.9151, Test Loss: 99.6954\n",
      "Epoch [14365/50000], Train Loss: 120.3708, Test Loss: 60.4061\n",
      "Epoch [14370/50000], Train Loss: 51.2590, Test Loss: 52.3432\n",
      "Epoch [14375/50000], Train Loss: 34.0874, Test Loss: 50.5008\n",
      "Epoch [14380/50000], Train Loss: 48.3431, Test Loss: 53.5334\n",
      "Epoch [14385/50000], Train Loss: 36.1334, Test Loss: 56.7009\n",
      "Epoch [14390/50000], Train Loss: 53.7333, Test Loss: 50.2774\n",
      "Epoch [14395/50000], Train Loss: 60.0562, Test Loss: 53.8720\n",
      "Epoch [14400/50000], Train Loss: 41.8423, Test Loss: 63.7981\n",
      "Epoch [14405/50000], Train Loss: 42.7325, Test Loss: 54.6369\n",
      "Epoch [14410/50000], Train Loss: 44.5641, Test Loss: 54.6559\n",
      "Epoch [14415/50000], Train Loss: 47.6669, Test Loss: 56.6252\n",
      "Epoch [14420/50000], Train Loss: 162.0521, Test Loss: 49.0120\n",
      "Epoch [14425/50000], Train Loss: 48.9901, Test Loss: 54.5776\n",
      "Epoch [14430/50000], Train Loss: 54.3087, Test Loss: 57.3140\n",
      "Epoch [14435/50000], Train Loss: 40.7518, Test Loss: 52.1781\n",
      "Epoch [14440/50000], Train Loss: 47.3672, Test Loss: 52.7388\n",
      "Epoch [14445/50000], Train Loss: 56.5879, Test Loss: 55.5553\n",
      "Epoch [14450/50000], Train Loss: 33.0914, Test Loss: 52.3716\n",
      "Epoch [14455/50000], Train Loss: 54.3658, Test Loss: 57.8515\n",
      "Epoch [14460/50000], Train Loss: 43.2966, Test Loss: 53.4211\n",
      "Epoch [14465/50000], Train Loss: 44.1427, Test Loss: 55.1741\n",
      "Epoch [14470/50000], Train Loss: 40.0779, Test Loss: 58.9135\n",
      "Epoch [14475/50000], Train Loss: 90.7442, Test Loss: 53.4755\n",
      "Epoch [14480/50000], Train Loss: 49.8461, Test Loss: 56.2511\n",
      "Epoch [14485/50000], Train Loss: 47.5666, Test Loss: 60.8595\n",
      "Epoch [14490/50000], Train Loss: 41.4493, Test Loss: 56.5713\n",
      "Epoch [14495/50000], Train Loss: 41.5955, Test Loss: 53.3696\n",
      "Epoch [14500/50000], Train Loss: 41.6972, Test Loss: 60.6379\n",
      "Epoch [14505/50000], Train Loss: 44.7932, Test Loss: 53.4124\n",
      "Epoch [14510/50000], Train Loss: 39.2637, Test Loss: 54.0481\n",
      "Epoch [14515/50000], Train Loss: 70.8282, Test Loss: 54.3914\n",
      "Epoch [14520/50000], Train Loss: 53.4656, Test Loss: 52.6371\n",
      "Epoch [14525/50000], Train Loss: 46.7120, Test Loss: 51.4178\n",
      "Epoch [14530/50000], Train Loss: 44.3646, Test Loss: 54.7179\n",
      "Epoch [14535/50000], Train Loss: 52.6158, Test Loss: 53.5177\n",
      "Epoch [14540/50000], Train Loss: 45.3738, Test Loss: 61.8654\n",
      "Epoch [14545/50000], Train Loss: 113.6521, Test Loss: 72.5910\n",
      "Epoch [14550/50000], Train Loss: 48.1692, Test Loss: 52.8439\n",
      "Epoch [14555/50000], Train Loss: 46.7430, Test Loss: 56.0215\n",
      "Epoch [14560/50000], Train Loss: 57.6793, Test Loss: 64.9468\n",
      "Epoch [14565/50000], Train Loss: 50.5880, Test Loss: 54.4784\n",
      "Epoch [14570/50000], Train Loss: 47.7907, Test Loss: 49.9695\n",
      "Epoch [14575/50000], Train Loss: 46.7984, Test Loss: 55.4632\n",
      "Epoch [14580/50000], Train Loss: 37.9214, Test Loss: 50.2903\n",
      "Epoch [14585/50000], Train Loss: 49.4250, Test Loss: 52.2837\n",
      "Epoch [14590/50000], Train Loss: 73.6495, Test Loss: 62.8801\n",
      "Epoch [14595/50000], Train Loss: 45.3345, Test Loss: 54.7785\n",
      "Epoch [14600/50000], Train Loss: 57.9379, Test Loss: 51.0718\n",
      "Epoch [14605/50000], Train Loss: 98.9366, Test Loss: 53.2309\n",
      "Epoch [14610/50000], Train Loss: 46.3647, Test Loss: 51.3895\n",
      "Epoch [14615/50000], Train Loss: 44.9038, Test Loss: 52.6319\n",
      "Epoch [14620/50000], Train Loss: 45.2523, Test Loss: 53.5840\n",
      "Epoch [14625/50000], Train Loss: 49.9502, Test Loss: 53.6599\n",
      "Epoch [14630/50000], Train Loss: 35.6580, Test Loss: 49.9156\n",
      "Epoch [14635/50000], Train Loss: 42.3504, Test Loss: 50.1322\n",
      "Epoch [14640/50000], Train Loss: 48.7963, Test Loss: 53.3876\n",
      "Epoch [14645/50000], Train Loss: 54.2709, Test Loss: 52.1196\n",
      "Epoch [14650/50000], Train Loss: 80.2792, Test Loss: 51.8680\n",
      "Epoch [14655/50000], Train Loss: 36.9918, Test Loss: 51.5819\n",
      "Epoch [14660/50000], Train Loss: 90.2419, Test Loss: 53.8675\n",
      "Epoch [14665/50000], Train Loss: 39.2482, Test Loss: 53.4673\n",
      "Epoch [14670/50000], Train Loss: 46.0038, Test Loss: 51.7370\n",
      "Epoch [14675/50000], Train Loss: 52.2240, Test Loss: 69.8971\n",
      "Epoch [14680/50000], Train Loss: 62.8498, Test Loss: 54.8404\n",
      "Epoch [14685/50000], Train Loss: 44.6253, Test Loss: 65.7783\n",
      "Epoch [14690/50000], Train Loss: 46.8037, Test Loss: 52.4537\n",
      "Epoch [14695/50000], Train Loss: 74.8510, Test Loss: 51.2059\n",
      "Epoch [14700/50000], Train Loss: 41.2008, Test Loss: 55.5972\n",
      "Epoch [14705/50000], Train Loss: 41.6007, Test Loss: 55.8343\n",
      "Epoch [14710/50000], Train Loss: 43.7596, Test Loss: 52.8779\n",
      "Epoch [14715/50000], Train Loss: 42.5811, Test Loss: 53.6821\n",
      "Epoch [14720/50000], Train Loss: 49.8384, Test Loss: 57.0235\n",
      "Epoch [14725/50000], Train Loss: 43.1748, Test Loss: 51.7549\n",
      "Epoch [14730/50000], Train Loss: 43.2970, Test Loss: 57.0441\n",
      "Epoch [14735/50000], Train Loss: 40.8887, Test Loss: 54.0711\n",
      "Epoch [14740/50000], Train Loss: 42.9789, Test Loss: 52.4224\n",
      "Epoch [14745/50000], Train Loss: 38.5480, Test Loss: 54.8190\n",
      "Epoch [14750/50000], Train Loss: 63.1692, Test Loss: 55.7331\n",
      "Epoch [14755/50000], Train Loss: 36.5631, Test Loss: 56.6530\n",
      "Epoch [14760/50000], Train Loss: 40.0909, Test Loss: 50.2134\n",
      "Epoch [14765/50000], Train Loss: 48.5326, Test Loss: 56.5057\n",
      "Epoch [14770/50000], Train Loss: 43.2091, Test Loss: 61.5039\n",
      "Epoch [14775/50000], Train Loss: 78.0782, Test Loss: 51.4285\n",
      "Epoch [14780/50000], Train Loss: 48.6319, Test Loss: 54.3763\n",
      "Epoch [14785/50000], Train Loss: 42.6719, Test Loss: 55.1583\n",
      "Epoch [14790/50000], Train Loss: 36.3740, Test Loss: 52.0569\n",
      "Epoch [14795/50000], Train Loss: 49.6335, Test Loss: 51.7325\n",
      "Epoch [14800/50000], Train Loss: 55.7370, Test Loss: 52.3823\n",
      "Epoch [14805/50000], Train Loss: 48.5910, Test Loss: 62.9751\n",
      "Epoch [14810/50000], Train Loss: 42.4033, Test Loss: 49.8660\n",
      "Epoch [14815/50000], Train Loss: 43.6082, Test Loss: 50.6387\n",
      "Epoch [14820/50000], Train Loss: 69.0660, Test Loss: 51.0838\n",
      "Epoch [14825/50000], Train Loss: 51.0590, Test Loss: 60.5276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14830/50000], Train Loss: 39.2117, Test Loss: 53.1755\n",
      "Epoch [14835/50000], Train Loss: 62.4579, Test Loss: 56.0515\n",
      "Epoch [14840/50000], Train Loss: 46.8810, Test Loss: 58.5047\n",
      "Epoch [14845/50000], Train Loss: 47.8214, Test Loss: 62.0771\n",
      "Epoch [14850/50000], Train Loss: 39.9095, Test Loss: 55.4515\n",
      "Epoch [14855/50000], Train Loss: 43.6499, Test Loss: 51.0080\n",
      "Epoch [14860/50000], Train Loss: 39.5814, Test Loss: 50.8539\n",
      "Epoch [14865/50000], Train Loss: 66.3507, Test Loss: 57.3205\n",
      "Epoch [14870/50000], Train Loss: 48.4519, Test Loss: 52.6803\n",
      "Epoch [14875/50000], Train Loss: 48.8844, Test Loss: 55.1765\n",
      "Epoch [14880/50000], Train Loss: 57.4954, Test Loss: 54.1505\n",
      "Epoch [14885/50000], Train Loss: 49.8724, Test Loss: 53.2632\n",
      "Epoch [14890/50000], Train Loss: 42.5951, Test Loss: 53.5232\n",
      "Epoch [14895/50000], Train Loss: 41.7163, Test Loss: 51.7868\n",
      "Epoch [14900/50000], Train Loss: 38.6425, Test Loss: 50.3002\n",
      "Epoch [14905/50000], Train Loss: 45.2244, Test Loss: 51.1195\n",
      "Epoch [14910/50000], Train Loss: 44.7131, Test Loss: 51.0296\n",
      "Epoch [14915/50000], Train Loss: 42.0762, Test Loss: 56.5095\n",
      "Epoch [14920/50000], Train Loss: 49.8730, Test Loss: 72.6956\n",
      "Epoch [14925/50000], Train Loss: 45.5652, Test Loss: 55.7855\n",
      "Epoch [14930/50000], Train Loss: 48.2252, Test Loss: 53.3267\n",
      "Epoch [14935/50000], Train Loss: 38.6040, Test Loss: 51.1701\n",
      "Epoch [14940/50000], Train Loss: 55.4471, Test Loss: 51.6592\n",
      "Epoch [14945/50000], Train Loss: 31.4198, Test Loss: 49.2148\n",
      "Epoch [14950/50000], Train Loss: 50.3310, Test Loss: 52.2409\n",
      "Epoch [14955/50000], Train Loss: 44.7850, Test Loss: 55.8740\n",
      "Epoch [14960/50000], Train Loss: 43.5962, Test Loss: 56.5780\n",
      "Epoch [14965/50000], Train Loss: 43.5032, Test Loss: 58.2204\n",
      "Epoch [14970/50000], Train Loss: 41.3339, Test Loss: 56.0964\n",
      "Epoch [14975/50000], Train Loss: 93.5078, Test Loss: 57.9858\n",
      "Epoch [14980/50000], Train Loss: 46.3899, Test Loss: 59.2923\n",
      "Epoch [14985/50000], Train Loss: 42.0763, Test Loss: 54.1524\n",
      "Epoch [14990/50000], Train Loss: 135.8198, Test Loss: 52.9035\n",
      "Epoch [14995/50000], Train Loss: 44.1205, Test Loss: 51.8863\n",
      "Epoch [15000/50000], Train Loss: 57.9963, Test Loss: 51.2084\n",
      "Epoch [15005/50000], Train Loss: 47.4874, Test Loss: 55.0331\n",
      "Epoch [15010/50000], Train Loss: 40.0405, Test Loss: 54.3720\n",
      "Epoch [15015/50000], Train Loss: 49.2996, Test Loss: 54.6895\n",
      "Epoch [15020/50000], Train Loss: 42.3518, Test Loss: 59.8926\n",
      "Epoch [15025/50000], Train Loss: 42.1118, Test Loss: 53.1883\n",
      "Epoch [15030/50000], Train Loss: 47.6352, Test Loss: 52.9457\n",
      "Epoch [15035/50000], Train Loss: 45.8298, Test Loss: 56.1800\n",
      "Epoch [15040/50000], Train Loss: 33.7845, Test Loss: 51.0471\n",
      "Epoch [15045/50000], Train Loss: 43.8624, Test Loss: 56.1005\n",
      "Epoch [15050/50000], Train Loss: 159.9586, Test Loss: 50.3353\n",
      "Epoch [15055/50000], Train Loss: 38.7026, Test Loss: 51.8524\n",
      "Epoch [15060/50000], Train Loss: 43.1919, Test Loss: 50.2716\n",
      "Epoch [15065/50000], Train Loss: 37.2854, Test Loss: 54.0599\n",
      "Epoch [15070/50000], Train Loss: 46.7454, Test Loss: 53.0615\n",
      "Epoch [15075/50000], Train Loss: 39.0942, Test Loss: 56.0253\n",
      "Epoch [15080/50000], Train Loss: 116.4838, Test Loss: 65.0767\n",
      "Epoch [15085/50000], Train Loss: 77.3856, Test Loss: 52.5550\n",
      "Epoch [15090/50000], Train Loss: 51.6350, Test Loss: 55.2355\n",
      "Epoch [15095/50000], Train Loss: 43.4775, Test Loss: 59.8069\n",
      "Epoch [15100/50000], Train Loss: 45.2923, Test Loss: 54.0018\n",
      "Epoch [15105/50000], Train Loss: 37.3833, Test Loss: 54.6889\n",
      "Epoch [15110/50000], Train Loss: 51.6814, Test Loss: 54.1514\n",
      "Epoch [15115/50000], Train Loss: 86.4662, Test Loss: 53.8128\n",
      "Epoch [15120/50000], Train Loss: 46.1205, Test Loss: 51.7441\n",
      "Epoch [15125/50000], Train Loss: 42.7138, Test Loss: 49.9032\n",
      "Epoch [15130/50000], Train Loss: 33.1949, Test Loss: 53.9094\n",
      "Epoch [15135/50000], Train Loss: 46.5648, Test Loss: 57.4305\n",
      "Epoch [15140/50000], Train Loss: 44.3440, Test Loss: 53.7025\n",
      "Epoch [15145/50000], Train Loss: 48.4189, Test Loss: 51.4873\n",
      "Epoch [15150/50000], Train Loss: 43.5369, Test Loss: 54.2989\n",
      "Epoch [15155/50000], Train Loss: 54.2222, Test Loss: 50.7694\n",
      "Epoch [15160/50000], Train Loss: 52.2874, Test Loss: 59.7483\n",
      "Epoch [15165/50000], Train Loss: 43.6158, Test Loss: 58.8664\n",
      "Epoch [15170/50000], Train Loss: 36.5768, Test Loss: 56.9129\n",
      "Epoch [15175/50000], Train Loss: 34.9187, Test Loss: 49.3657\n",
      "Epoch [15180/50000], Train Loss: 40.7372, Test Loss: 53.7674\n",
      "Epoch [15185/50000], Train Loss: 52.4669, Test Loss: 55.1562\n",
      "Epoch [15190/50000], Train Loss: 33.7909, Test Loss: 53.9842\n",
      "Epoch [15195/50000], Train Loss: 97.6910, Test Loss: 55.2326\n",
      "Epoch [15200/50000], Train Loss: 33.3357, Test Loss: 50.3304\n",
      "Epoch [15205/50000], Train Loss: 44.6273, Test Loss: 52.0005\n",
      "Epoch [15210/50000], Train Loss: 47.4023, Test Loss: 60.5690\n",
      "Epoch [15215/50000], Train Loss: 43.6337, Test Loss: 53.5885\n",
      "Epoch [15220/50000], Train Loss: 48.3765, Test Loss: 54.8280\n",
      "Epoch [15225/50000], Train Loss: 36.8468, Test Loss: 49.2971\n",
      "Epoch [15230/50000], Train Loss: 34.0973, Test Loss: 51.1255\n",
      "Epoch [15235/50000], Train Loss: 39.5940, Test Loss: 56.1394\n",
      "Epoch [15240/50000], Train Loss: 42.6219, Test Loss: 78.3546\n",
      "Epoch [15245/50000], Train Loss: 46.7444, Test Loss: 50.1623\n",
      "Epoch [15250/50000], Train Loss: 37.5006, Test Loss: 53.0106\n",
      "Epoch [15255/50000], Train Loss: 45.3032, Test Loss: 51.6972\n",
      "Epoch [15260/50000], Train Loss: 58.0016, Test Loss: 56.3599\n",
      "Epoch [15265/50000], Train Loss: 87.4672, Test Loss: 127.1803\n",
      "Epoch [15270/50000], Train Loss: 40.6169, Test Loss: 51.9291\n",
      "Epoch [15275/50000], Train Loss: 47.3066, Test Loss: 54.7258\n",
      "Epoch [15280/50000], Train Loss: 49.5280, Test Loss: 52.0642\n",
      "Epoch [15285/50000], Train Loss: 42.0626, Test Loss: 53.0827\n",
      "Epoch [15290/50000], Train Loss: 34.2072, Test Loss: 54.6104\n",
      "Epoch [15295/50000], Train Loss: 39.9318, Test Loss: 55.0388\n",
      "Epoch [15300/50000], Train Loss: 58.5330, Test Loss: 55.4215\n",
      "Epoch [15305/50000], Train Loss: 43.2430, Test Loss: 56.4423\n",
      "Epoch [15310/50000], Train Loss: 41.8794, Test Loss: 59.8737\n",
      "Epoch [15315/50000], Train Loss: 53.5351, Test Loss: 56.1136\n",
      "Epoch [15320/50000], Train Loss: 39.3712, Test Loss: 51.0552\n",
      "Epoch [15325/50000], Train Loss: 64.3181, Test Loss: 55.9628\n",
      "Epoch [15330/50000], Train Loss: 41.3912, Test Loss: 51.3131\n",
      "Epoch [15335/50000], Train Loss: 75.5881, Test Loss: 58.7618\n",
      "Epoch [15340/50000], Train Loss: 43.2372, Test Loss: 57.4040\n",
      "Epoch [15345/50000], Train Loss: 40.7304, Test Loss: 59.2487\n",
      "Epoch [15350/50000], Train Loss: 52.6254, Test Loss: 51.7422\n",
      "Epoch [15355/50000], Train Loss: 42.0115, Test Loss: 51.7365\n",
      "Epoch [15360/50000], Train Loss: 47.5795, Test Loss: 53.5814\n",
      "Epoch [15365/50000], Train Loss: 41.6473, Test Loss: 58.8980\n",
      "Epoch [15370/50000], Train Loss: 45.1719, Test Loss: 51.8776\n",
      "Epoch [15375/50000], Train Loss: 45.5459, Test Loss: 57.5958\n",
      "Epoch [15380/50000], Train Loss: 57.7343, Test Loss: 51.4834\n",
      "Epoch [15385/50000], Train Loss: 51.8547, Test Loss: 55.2011\n",
      "Epoch [15390/50000], Train Loss: 29.9911, Test Loss: 52.4558\n",
      "Epoch [15395/50000], Train Loss: 52.1227, Test Loss: 102.0672\n",
      "Epoch [15400/50000], Train Loss: 45.3556, Test Loss: 53.7202\n",
      "Epoch [15405/50000], Train Loss: 41.7864, Test Loss: 50.5714\n",
      "Epoch [15410/50000], Train Loss: 37.4505, Test Loss: 49.3179\n",
      "Epoch [15415/50000], Train Loss: 33.8245, Test Loss: 52.4481\n",
      "Epoch [15420/50000], Train Loss: 43.3078, Test Loss: 54.4429\n",
      "Epoch [15425/50000], Train Loss: 42.3458, Test Loss: 50.1134\n",
      "Epoch [15430/50000], Train Loss: 39.6474, Test Loss: 52.2903\n",
      "Epoch [15435/50000], Train Loss: 53.0776, Test Loss: 55.7363\n",
      "Epoch [15440/50000], Train Loss: 43.1602, Test Loss: 50.6783\n",
      "Epoch [15445/50000], Train Loss: 69.4829, Test Loss: 58.4331\n",
      "Epoch [15450/50000], Train Loss: 44.1971, Test Loss: 55.3299\n",
      "Epoch [15455/50000], Train Loss: 41.9771, Test Loss: 52.9003\n",
      "Epoch [15460/50000], Train Loss: 34.3578, Test Loss: 51.2959\n",
      "Epoch [15465/50000], Train Loss: 94.0208, Test Loss: 48.1648\n",
      "Epoch [15470/50000], Train Loss: 48.7897, Test Loss: 51.5851\n",
      "Epoch [15475/50000], Train Loss: 60.5437, Test Loss: 52.9438\n",
      "Epoch [15480/50000], Train Loss: 47.9438, Test Loss: 54.2959\n",
      "Epoch [15485/50000], Train Loss: 41.6573, Test Loss: 51.4132\n",
      "Epoch [15490/50000], Train Loss: 41.9227, Test Loss: 53.6815\n",
      "Epoch [15495/50000], Train Loss: 34.2840, Test Loss: 49.5249\n",
      "Epoch [15500/50000], Train Loss: 34.4800, Test Loss: 49.3089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15505/50000], Train Loss: 44.2824, Test Loss: 52.2773\n",
      "Epoch [15510/50000], Train Loss: 41.7014, Test Loss: 54.1870\n",
      "Epoch [15515/50000], Train Loss: 35.2107, Test Loss: 56.6314\n",
      "Epoch [15520/50000], Train Loss: 56.1134, Test Loss: 49.7853\n",
      "Epoch [15525/50000], Train Loss: 34.3686, Test Loss: 56.4348\n",
      "Epoch [15530/50000], Train Loss: 56.0579, Test Loss: 50.1902\n",
      "Epoch [15535/50000], Train Loss: 96.8414, Test Loss: 51.5475\n",
      "Epoch [15540/50000], Train Loss: 36.7895, Test Loss: 50.4492\n",
      "Epoch [15545/50000], Train Loss: 48.2884, Test Loss: 58.0040\n",
      "Epoch [15550/50000], Train Loss: 36.1418, Test Loss: 56.4331\n",
      "Epoch [15555/50000], Train Loss: 68.1046, Test Loss: 53.4452\n",
      "Epoch [15560/50000], Train Loss: 49.8205, Test Loss: 58.0381\n",
      "Epoch [15565/50000], Train Loss: 57.9607, Test Loss: 53.6252\n",
      "Epoch [15570/50000], Train Loss: 30.9704, Test Loss: 54.0978\n",
      "Epoch [15575/50000], Train Loss: 42.5360, Test Loss: 59.9281\n",
      "Epoch [15580/50000], Train Loss: 45.6605, Test Loss: 53.2976\n",
      "Epoch [15585/50000], Train Loss: 40.9225, Test Loss: 51.7752\n",
      "Epoch [15590/50000], Train Loss: 45.4628, Test Loss: 54.8624\n",
      "Epoch [15595/50000], Train Loss: 44.9850, Test Loss: 55.9321\n",
      "Epoch [15600/50000], Train Loss: 42.1156, Test Loss: 50.6000\n",
      "Epoch [15605/50000], Train Loss: 40.2982, Test Loss: 62.6956\n",
      "Epoch [15610/50000], Train Loss: 41.8263, Test Loss: 49.7680\n",
      "Epoch [15615/50000], Train Loss: 83.8131, Test Loss: 53.0475\n",
      "Epoch [15620/50000], Train Loss: 42.4972, Test Loss: 57.0165\n",
      "Epoch [15625/50000], Train Loss: 188.5537, Test Loss: 50.4751\n",
      "Epoch [15630/50000], Train Loss: 45.9568, Test Loss: 56.5619\n",
      "Epoch [15635/50000], Train Loss: 43.7662, Test Loss: 52.2343\n",
      "Epoch [15640/50000], Train Loss: 66.0882, Test Loss: 50.4923\n",
      "Epoch [15645/50000], Train Loss: 47.6562, Test Loss: 55.7833\n",
      "Epoch [15650/50000], Train Loss: 40.5607, Test Loss: 50.2229\n",
      "Epoch [15655/50000], Train Loss: 46.9290, Test Loss: 57.5874\n",
      "Epoch [15660/50000], Train Loss: 51.9241, Test Loss: 50.1756\n",
      "Epoch [15665/50000], Train Loss: 38.6976, Test Loss: 52.0957\n",
      "Epoch [15670/50000], Train Loss: 45.6511, Test Loss: 53.1909\n",
      "Epoch [15675/50000], Train Loss: 46.2568, Test Loss: 54.5025\n",
      "Epoch [15680/50000], Train Loss: 33.8143, Test Loss: 52.6766\n",
      "Epoch [15685/50000], Train Loss: 48.6072, Test Loss: 51.4262\n",
      "Epoch [15690/50000], Train Loss: 48.0097, Test Loss: 54.8540\n",
      "Epoch [15695/50000], Train Loss: 38.2833, Test Loss: 50.8484\n",
      "Epoch [15700/50000], Train Loss: 35.6457, Test Loss: 52.8670\n",
      "Epoch [15705/50000], Train Loss: 43.8061, Test Loss: 62.6097\n",
      "Epoch [15710/50000], Train Loss: 40.9735, Test Loss: 53.8903\n",
      "Epoch [15715/50000], Train Loss: 47.2028, Test Loss: 51.8630\n",
      "Epoch [15720/50000], Train Loss: 42.0298, Test Loss: 56.7080\n",
      "Epoch [15725/50000], Train Loss: 70.7258, Test Loss: 51.4247\n",
      "Epoch [15730/50000], Train Loss: 47.3782, Test Loss: 51.6307\n",
      "Epoch [15735/50000], Train Loss: 58.5525, Test Loss: 52.5835\n",
      "Epoch [15740/50000], Train Loss: 49.8913, Test Loss: 50.2898\n",
      "Epoch [15745/50000], Train Loss: 42.8255, Test Loss: 50.8588\n",
      "Epoch [15750/50000], Train Loss: 46.3378, Test Loss: 59.3759\n",
      "Epoch [15755/50000], Train Loss: 45.9482, Test Loss: 54.0626\n",
      "Epoch [15760/50000], Train Loss: 35.1980, Test Loss: 51.6382\n",
      "Epoch [15765/50000], Train Loss: 45.6185, Test Loss: 51.5708\n",
      "Epoch [15770/50000], Train Loss: 38.5065, Test Loss: 54.3879\n",
      "Epoch [15775/50000], Train Loss: 40.7347, Test Loss: 50.9969\n",
      "Epoch [15780/50000], Train Loss: 93.7924, Test Loss: 49.7668\n",
      "Epoch [15785/50000], Train Loss: 34.1312, Test Loss: 57.2785\n",
      "Epoch [15790/50000], Train Loss: 40.6762, Test Loss: 52.6757\n",
      "Epoch [15795/50000], Train Loss: 32.8401, Test Loss: 53.3620\n",
      "Epoch [15800/50000], Train Loss: 42.7922, Test Loss: 63.9307\n",
      "Epoch [15805/50000], Train Loss: 48.4476, Test Loss: 50.5474\n",
      "Epoch [15810/50000], Train Loss: 68.0843, Test Loss: 56.2758\n",
      "Epoch [15815/50000], Train Loss: 42.4902, Test Loss: 50.1885\n",
      "Epoch [15820/50000], Train Loss: 33.1926, Test Loss: 54.7719\n",
      "Epoch [15825/50000], Train Loss: 44.2346, Test Loss: 53.7336\n",
      "Epoch [15830/50000], Train Loss: 33.2427, Test Loss: 50.1289\n",
      "Epoch [15835/50000], Train Loss: 51.6814, Test Loss: 56.5911\n",
      "Epoch [15840/50000], Train Loss: 51.4225, Test Loss: 52.0187\n",
      "Epoch [15845/50000], Train Loss: 34.1001, Test Loss: 52.7920\n",
      "Epoch [15850/50000], Train Loss: 38.4633, Test Loss: 52.5774\n",
      "Epoch [15855/50000], Train Loss: 55.7700, Test Loss: 55.0461\n",
      "Epoch [15860/50000], Train Loss: 32.4881, Test Loss: 51.5411\n",
      "Epoch [15865/50000], Train Loss: 48.7508, Test Loss: 68.3924\n",
      "Epoch [15870/50000], Train Loss: 45.5789, Test Loss: 62.6254\n",
      "Epoch [15875/50000], Train Loss: 42.9030, Test Loss: 53.0145\n",
      "Epoch [15880/50000], Train Loss: 47.9280, Test Loss: 53.4744\n",
      "Epoch [15885/50000], Train Loss: 37.3335, Test Loss: 57.1383\n",
      "Epoch [15890/50000], Train Loss: 56.3402, Test Loss: 56.6635\n",
      "Epoch [15895/50000], Train Loss: 73.2349, Test Loss: 51.1030\n",
      "Epoch [15900/50000], Train Loss: 43.7337, Test Loss: 49.7598\n",
      "Epoch [15905/50000], Train Loss: 39.1255, Test Loss: 65.7825\n",
      "Epoch [15910/50000], Train Loss: 38.8157, Test Loss: 50.2709\n",
      "Epoch [15915/50000], Train Loss: 42.1887, Test Loss: 51.5124\n",
      "Epoch [15920/50000], Train Loss: 38.5419, Test Loss: 49.4515\n",
      "Epoch [15925/50000], Train Loss: 42.8388, Test Loss: 73.4700\n",
      "Epoch [15930/50000], Train Loss: 47.2299, Test Loss: 68.2043\n",
      "Epoch [15935/50000], Train Loss: 44.8853, Test Loss: 64.4761\n",
      "Epoch [15940/50000], Train Loss: 35.2510, Test Loss: 51.2553\n",
      "Epoch [15945/50000], Train Loss: 37.8609, Test Loss: 63.2196\n",
      "Epoch [15950/50000], Train Loss: 43.6893, Test Loss: 78.9299\n",
      "Epoch [15955/50000], Train Loss: 36.3693, Test Loss: 67.2228\n",
      "Epoch [15960/50000], Train Loss: 45.9234, Test Loss: 53.4403\n",
      "Epoch [15965/50000], Train Loss: 39.7602, Test Loss: 50.7598\n",
      "Epoch [15970/50000], Train Loss: 45.8246, Test Loss: 52.0205\n",
      "Epoch [15975/50000], Train Loss: 41.5938, Test Loss: 60.8377\n",
      "Epoch [15980/50000], Train Loss: 48.2840, Test Loss: 52.9351\n",
      "Epoch [15985/50000], Train Loss: 44.1535, Test Loss: 54.9853\n",
      "Epoch [15990/50000], Train Loss: 43.7953, Test Loss: 65.1741\n",
      "Epoch [15995/50000], Train Loss: 27.2028, Test Loss: 49.1732\n",
      "Epoch [16000/50000], Train Loss: 40.7624, Test Loss: 51.6850\n",
      "Epoch [16005/50000], Train Loss: 38.5295, Test Loss: 52.3050\n",
      "Epoch [16010/50000], Train Loss: 39.7996, Test Loss: 55.9126\n",
      "Epoch [16015/50000], Train Loss: 40.9216, Test Loss: 55.1237\n",
      "Epoch [16020/50000], Train Loss: 42.8535, Test Loss: 50.5932\n",
      "Epoch [16025/50000], Train Loss: 39.1131, Test Loss: 49.2036\n",
      "Epoch [16030/50000], Train Loss: 47.8662, Test Loss: 50.7192\n",
      "Epoch [16035/50000], Train Loss: 37.6518, Test Loss: 50.8697\n",
      "Epoch [16040/50000], Train Loss: 59.9663, Test Loss: 60.9589\n",
      "Epoch [16045/50000], Train Loss: 62.4439, Test Loss: 71.3471\n",
      "Epoch [16050/50000], Train Loss: 39.3565, Test Loss: 50.5511\n",
      "Epoch [16055/50000], Train Loss: 51.5613, Test Loss: 51.3987\n",
      "Epoch [16060/50000], Train Loss: 65.3104, Test Loss: 49.8505\n",
      "Epoch [16065/50000], Train Loss: 36.6975, Test Loss: 52.7386\n",
      "Epoch [16070/50000], Train Loss: 48.0752, Test Loss: 53.7527\n",
      "Epoch [16075/50000], Train Loss: 74.1722, Test Loss: 76.5750\n",
      "Epoch [16080/50000], Train Loss: 42.5914, Test Loss: 53.6325\n",
      "Epoch [16085/50000], Train Loss: 65.6629, Test Loss: 51.6365\n",
      "Epoch [16090/50000], Train Loss: 37.9416, Test Loss: 49.4380\n",
      "Epoch [16095/50000], Train Loss: 46.6497, Test Loss: 54.0960\n",
      "Epoch [16100/50000], Train Loss: 40.8137, Test Loss: 59.7021\n",
      "Epoch [16105/50000], Train Loss: 44.7586, Test Loss: 51.8946\n",
      "Epoch [16110/50000], Train Loss: 37.1621, Test Loss: 57.4866\n",
      "Epoch [16115/50000], Train Loss: 54.1332, Test Loss: 72.4418\n",
      "Epoch [16120/50000], Train Loss: 50.8008, Test Loss: 51.8940\n",
      "Epoch [16125/50000], Train Loss: 42.0648, Test Loss: 53.4940\n",
      "Epoch [16130/50000], Train Loss: 68.3267, Test Loss: 51.7483\n",
      "Epoch [16135/50000], Train Loss: 38.3446, Test Loss: 52.9830\n",
      "Epoch [16140/50000], Train Loss: 45.4257, Test Loss: 66.2589\n",
      "Epoch [16145/50000], Train Loss: 42.0625, Test Loss: 58.1380\n",
      "Epoch [16150/50000], Train Loss: 40.0828, Test Loss: 50.5601\n",
      "Epoch [16155/50000], Train Loss: 30.6513, Test Loss: 54.7184\n",
      "Epoch [16160/50000], Train Loss: 59.3660, Test Loss: 50.9805\n",
      "Epoch [16165/50000], Train Loss: 78.5907, Test Loss: 49.9469\n",
      "Epoch [16170/50000], Train Loss: 43.2370, Test Loss: 50.1975\n",
      "Epoch [16175/50000], Train Loss: 40.1295, Test Loss: 51.1085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16180/50000], Train Loss: 44.7351, Test Loss: 49.2340\n",
      "Epoch [16185/50000], Train Loss: 36.0072, Test Loss: 52.0803\n",
      "Epoch [16190/50000], Train Loss: 64.8129, Test Loss: 50.2998\n",
      "Epoch [16195/50000], Train Loss: 46.3141, Test Loss: 51.2642\n",
      "Epoch [16200/50000], Train Loss: 33.5076, Test Loss: 49.6452\n",
      "Epoch [16205/50000], Train Loss: 86.9413, Test Loss: 50.7193\n",
      "Epoch [16210/50000], Train Loss: 37.8902, Test Loss: 62.2631\n",
      "Epoch [16215/50000], Train Loss: 38.0241, Test Loss: 50.2678\n",
      "Epoch [16220/50000], Train Loss: 45.4330, Test Loss: 49.7612\n",
      "Epoch [16225/50000], Train Loss: 58.0682, Test Loss: 51.0990\n",
      "Epoch [16230/50000], Train Loss: 46.7650, Test Loss: 53.0251\n",
      "Epoch [16235/50000], Train Loss: 56.8840, Test Loss: 50.4944\n",
      "Epoch [16240/50000], Train Loss: 36.7621, Test Loss: 49.5124\n",
      "Epoch [16245/50000], Train Loss: 45.6609, Test Loss: 50.0415\n",
      "Epoch [16250/50000], Train Loss: 44.0793, Test Loss: 57.4402\n",
      "Epoch [16255/50000], Train Loss: 39.8857, Test Loss: 57.6881\n",
      "Epoch [16260/50000], Train Loss: 48.4391, Test Loss: 53.3347\n",
      "Epoch [16265/50000], Train Loss: 28.4251, Test Loss: 51.1390\n",
      "Epoch [16270/50000], Train Loss: 39.7743, Test Loss: 51.1553\n",
      "Epoch [16275/50000], Train Loss: 42.5362, Test Loss: 50.9178\n",
      "Epoch [16280/50000], Train Loss: 42.4054, Test Loss: 68.2771\n",
      "Epoch [16285/50000], Train Loss: 45.0601, Test Loss: 57.4387\n",
      "Epoch [16290/50000], Train Loss: 42.7930, Test Loss: 50.6284\n",
      "Epoch [16295/50000], Train Loss: 42.6805, Test Loss: 68.0792\n",
      "Epoch [16300/50000], Train Loss: 49.8824, Test Loss: 53.8975\n",
      "Epoch [16305/50000], Train Loss: 33.8506, Test Loss: 50.0870\n",
      "Epoch [16310/50000], Train Loss: 46.0493, Test Loss: 53.2571\n",
      "Epoch [16315/50000], Train Loss: 83.6400, Test Loss: 50.2702\n",
      "Epoch [16320/50000], Train Loss: 40.1263, Test Loss: 51.4865\n",
      "Epoch [16325/50000], Train Loss: 44.9489, Test Loss: 52.6582\n",
      "Epoch [16330/50000], Train Loss: 54.3356, Test Loss: 49.8429\n",
      "Epoch [16335/50000], Train Loss: 52.3391, Test Loss: 49.4251\n",
      "Epoch [16340/50000], Train Loss: 39.8559, Test Loss: 54.5405\n",
      "Epoch [16345/50000], Train Loss: 42.7053, Test Loss: 53.7063\n",
      "Epoch [16350/50000], Train Loss: 33.7232, Test Loss: 53.8098\n",
      "Epoch [16355/50000], Train Loss: 43.5749, Test Loss: 49.0297\n",
      "Epoch [16360/50000], Train Loss: 37.9521, Test Loss: 56.1585\n",
      "Epoch [16365/50000], Train Loss: 57.7418, Test Loss: 55.3138\n",
      "Epoch [16370/50000], Train Loss: 39.2030, Test Loss: 54.1016\n",
      "Epoch [16375/50000], Train Loss: 43.8265, Test Loss: 56.4218\n",
      "Epoch [16380/50000], Train Loss: 40.5008, Test Loss: 55.8483\n",
      "Epoch [16385/50000], Train Loss: 45.3739, Test Loss: 60.0752\n",
      "Epoch [16390/50000], Train Loss: 55.1384, Test Loss: 52.0422\n",
      "Epoch [16395/50000], Train Loss: 41.2560, Test Loss: 51.4364\n",
      "Epoch [16400/50000], Train Loss: 62.8657, Test Loss: 56.8179\n",
      "Epoch [16405/50000], Train Loss: 46.6702, Test Loss: 53.2131\n",
      "Epoch [16410/50000], Train Loss: 50.3906, Test Loss: 63.6858\n",
      "Epoch [16415/50000], Train Loss: 36.6925, Test Loss: 77.3529\n",
      "Epoch [16420/50000], Train Loss: 31.9431, Test Loss: 49.0658\n",
      "Epoch [16425/50000], Train Loss: 53.0664, Test Loss: 66.2065\n",
      "Epoch [16430/50000], Train Loss: 35.1732, Test Loss: 53.7446\n",
      "Epoch [16435/50000], Train Loss: 33.4754, Test Loss: 56.0876\n",
      "Epoch [16440/50000], Train Loss: 39.9465, Test Loss: 51.1213\n",
      "Epoch [16445/50000], Train Loss: 44.3458, Test Loss: 52.5254\n",
      "Epoch [16450/50000], Train Loss: 30.9843, Test Loss: 52.5024\n",
      "Epoch [16455/50000], Train Loss: 65.4742, Test Loss: 81.6974\n",
      "Epoch [16460/50000], Train Loss: 42.6873, Test Loss: 53.1039\n",
      "Epoch [16465/50000], Train Loss: 51.7262, Test Loss: 57.1143\n",
      "Epoch [16470/50000], Train Loss: 122.7278, Test Loss: 50.8206\n",
      "Epoch [16475/50000], Train Loss: 41.6367, Test Loss: 55.2192\n",
      "Epoch [16480/50000], Train Loss: 28.0444, Test Loss: 51.6239\n",
      "Epoch [16485/50000], Train Loss: 36.7634, Test Loss: 57.9771\n",
      "Epoch [16490/50000], Train Loss: 55.6723, Test Loss: 53.7552\n",
      "Epoch [16495/50000], Train Loss: 85.8333, Test Loss: 51.8326\n",
      "Epoch [16500/50000], Train Loss: 73.0885, Test Loss: 72.5440\n",
      "Epoch [16505/50000], Train Loss: 48.0157, Test Loss: 53.0814\n",
      "Epoch [16510/50000], Train Loss: 33.3116, Test Loss: 50.0385\n",
      "Epoch [16515/50000], Train Loss: 37.5488, Test Loss: 51.5321\n",
      "Epoch [16520/50000], Train Loss: 39.0777, Test Loss: 51.9120\n",
      "Epoch [16525/50000], Train Loss: 39.5918, Test Loss: 49.8607\n",
      "Epoch [16530/50000], Train Loss: 41.7756, Test Loss: 55.7342\n",
      "Epoch [16535/50000], Train Loss: 39.6292, Test Loss: 52.2728\n",
      "Epoch [16540/50000], Train Loss: 80.3914, Test Loss: 49.4105\n",
      "Epoch [16545/50000], Train Loss: 46.9238, Test Loss: 54.8423\n",
      "Epoch [16550/50000], Train Loss: 38.9797, Test Loss: 50.7980\n",
      "Epoch [16555/50000], Train Loss: 44.6133, Test Loss: 53.3775\n",
      "Epoch [16560/50000], Train Loss: 52.6429, Test Loss: 50.6314\n",
      "Epoch [16565/50000], Train Loss: 42.6098, Test Loss: 51.8237\n",
      "Epoch [16570/50000], Train Loss: 34.5300, Test Loss: 50.1682\n",
      "Epoch [16575/50000], Train Loss: 38.6634, Test Loss: 51.9316\n",
      "Epoch [16580/50000], Train Loss: 76.3091, Test Loss: 66.2386\n",
      "Epoch [16585/50000], Train Loss: 31.5281, Test Loss: 53.8201\n",
      "Epoch [16590/50000], Train Loss: 117.6431, Test Loss: 49.5577\n",
      "Epoch [16595/50000], Train Loss: 37.9288, Test Loss: 75.5218\n",
      "Epoch [16600/50000], Train Loss: 41.2394, Test Loss: 51.4602\n",
      "Epoch [16605/50000], Train Loss: 40.3848, Test Loss: 68.1555\n",
      "Epoch [16610/50000], Train Loss: 117.4628, Test Loss: 49.4411\n",
      "Epoch [16615/50000], Train Loss: 46.8215, Test Loss: 53.9255\n",
      "Epoch [16620/50000], Train Loss: 103.1744, Test Loss: 50.0570\n",
      "Epoch [16625/50000], Train Loss: 40.5516, Test Loss: 55.9364\n",
      "Epoch [16630/50000], Train Loss: 42.8818, Test Loss: 50.8760\n",
      "Epoch [16635/50000], Train Loss: 39.1107, Test Loss: 49.9134\n",
      "Epoch [16640/50000], Train Loss: 33.3274, Test Loss: 48.8889\n",
      "Epoch [16645/50000], Train Loss: 35.5007, Test Loss: 54.4509\n",
      "Epoch [16650/50000], Train Loss: 36.2253, Test Loss: 64.3899\n",
      "Epoch [16655/50000], Train Loss: 56.5473, Test Loss: 81.3722\n",
      "Epoch [16660/50000], Train Loss: 38.9318, Test Loss: 49.0630\n",
      "Epoch [16665/50000], Train Loss: 40.6331, Test Loss: 51.1309\n",
      "Epoch [16670/50000], Train Loss: 42.5868, Test Loss: 61.8441\n",
      "Epoch [16675/50000], Train Loss: 30.4610, Test Loss: 50.7032\n",
      "Epoch [16680/50000], Train Loss: 39.1916, Test Loss: 51.1173\n",
      "Epoch [16685/50000], Train Loss: 39.0599, Test Loss: 52.3362\n",
      "Epoch [16690/50000], Train Loss: 37.9859, Test Loss: 51.3387\n",
      "Epoch [16695/50000], Train Loss: 45.4244, Test Loss: 52.2206\n",
      "Epoch [16700/50000], Train Loss: 41.3501, Test Loss: 51.1684\n",
      "Epoch [16705/50000], Train Loss: 52.3839, Test Loss: 51.2736\n",
      "Epoch [16710/50000], Train Loss: 41.7714, Test Loss: 50.4793\n",
      "Epoch [16715/50000], Train Loss: 31.9644, Test Loss: 53.0415\n",
      "Epoch [16720/50000], Train Loss: 40.8588, Test Loss: 57.7079\n",
      "Epoch [16725/50000], Train Loss: 35.6964, Test Loss: 50.2353\n",
      "Epoch [16730/50000], Train Loss: 72.8982, Test Loss: 50.3217\n",
      "Epoch [16735/50000], Train Loss: 35.5129, Test Loss: 50.4477\n",
      "Epoch [16740/50000], Train Loss: 63.8687, Test Loss: 52.2544\n",
      "Epoch [16745/50000], Train Loss: 92.4814, Test Loss: 50.5332\n",
      "Epoch [16750/50000], Train Loss: 40.1990, Test Loss: 54.4932\n",
      "Epoch [16755/50000], Train Loss: 37.3822, Test Loss: 63.1227\n",
      "Epoch [16760/50000], Train Loss: 108.8432, Test Loss: 50.3185\n",
      "Epoch [16765/50000], Train Loss: 57.2688, Test Loss: 50.5588\n",
      "Epoch [16770/50000], Train Loss: 74.4228, Test Loss: 51.5475\n",
      "Epoch [16775/50000], Train Loss: 72.7527, Test Loss: 120.2758\n",
      "Epoch [16780/50000], Train Loss: 40.4836, Test Loss: 50.5624\n",
      "Epoch [16785/50000], Train Loss: 44.9372, Test Loss: 59.5059\n",
      "Epoch [16790/50000], Train Loss: 41.4086, Test Loss: 53.7284\n",
      "Epoch [16795/50000], Train Loss: 39.7337, Test Loss: 66.4326\n",
      "Epoch [16800/50000], Train Loss: 84.6246, Test Loss: 48.8019\n",
      "Epoch [16805/50000], Train Loss: 30.2493, Test Loss: 49.4954\n",
      "Epoch [16810/50000], Train Loss: 42.5300, Test Loss: 50.4692\n",
      "Epoch [16815/50000], Train Loss: 58.1220, Test Loss: 170.6582\n",
      "Epoch [16820/50000], Train Loss: 41.1255, Test Loss: 50.2245\n",
      "Epoch [16825/50000], Train Loss: 33.3565, Test Loss: 56.8280\n",
      "Epoch [16830/50000], Train Loss: 61.8290, Test Loss: 53.4836\n",
      "Epoch [16835/50000], Train Loss: 36.9834, Test Loss: 52.1332\n",
      "Epoch [16840/50000], Train Loss: 61.0286, Test Loss: 55.3440\n",
      "Epoch [16845/50000], Train Loss: 37.9733, Test Loss: 50.6592\n",
      "Epoch [16850/50000], Train Loss: 46.1540, Test Loss: 58.1989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16855/50000], Train Loss: 47.8831, Test Loss: 64.3290\n",
      "Epoch [16860/50000], Train Loss: 90.7817, Test Loss: 51.9129\n",
      "Epoch [16865/50000], Train Loss: 45.0788, Test Loss: 50.8653\n",
      "Epoch [16870/50000], Train Loss: 70.2802, Test Loss: 53.5663\n",
      "Epoch [16875/50000], Train Loss: 37.8189, Test Loss: 49.6391\n",
      "Epoch [16880/50000], Train Loss: 52.1105, Test Loss: 55.1712\n",
      "Epoch [16885/50000], Train Loss: 40.3054, Test Loss: 49.1258\n",
      "Epoch [16890/50000], Train Loss: 35.8018, Test Loss: 51.4098\n",
      "Epoch [16895/50000], Train Loss: 35.5644, Test Loss: 51.9823\n",
      "Epoch [16900/50000], Train Loss: 376.7069, Test Loss: 49.7422\n",
      "Epoch [16905/50000], Train Loss: 49.4476, Test Loss: 49.7434\n",
      "Epoch [16910/50000], Train Loss: 33.5531, Test Loss: 49.2056\n",
      "Epoch [16915/50000], Train Loss: 44.2886, Test Loss: 53.8699\n",
      "Epoch [16920/50000], Train Loss: 72.3682, Test Loss: 52.5467\n",
      "Epoch [16925/50000], Train Loss: 37.8404, Test Loss: 51.0576\n",
      "Epoch [16930/50000], Train Loss: 47.7031, Test Loss: 53.3191\n",
      "Epoch [16935/50000], Train Loss: 72.2695, Test Loss: 51.0100\n",
      "Epoch [16940/50000], Train Loss: 45.0404, Test Loss: 52.5497\n",
      "Epoch [16945/50000], Train Loss: 51.9503, Test Loss: 49.8152\n",
      "Epoch [16950/50000], Train Loss: 47.6146, Test Loss: 52.3025\n",
      "Epoch [16955/50000], Train Loss: 34.2694, Test Loss: 51.9218\n",
      "Epoch [16960/50000], Train Loss: 42.8838, Test Loss: 51.7938\n",
      "Epoch [16965/50000], Train Loss: 35.3030, Test Loss: 52.4205\n",
      "Epoch [16970/50000], Train Loss: 39.7178, Test Loss: 51.2225\n",
      "Epoch [16975/50000], Train Loss: 34.9511, Test Loss: 54.7115\n",
      "Epoch [16980/50000], Train Loss: 42.8312, Test Loss: 53.1758\n",
      "Epoch [16985/50000], Train Loss: 38.6629, Test Loss: 51.4181\n",
      "Epoch [16990/50000], Train Loss: 54.3228, Test Loss: 49.7509\n",
      "Epoch [16995/50000], Train Loss: 39.8192, Test Loss: 52.3858\n",
      "Epoch [17000/50000], Train Loss: 42.4487, Test Loss: 52.4843\n",
      "Epoch [17005/50000], Train Loss: 38.0813, Test Loss: 50.2899\n",
      "Epoch [17010/50000], Train Loss: 39.4254, Test Loss: 58.0639\n",
      "Epoch [17015/50000], Train Loss: 39.6814, Test Loss: 52.7194\n",
      "Epoch [17020/50000], Train Loss: 35.1347, Test Loss: 56.9878\n",
      "Epoch [17025/50000], Train Loss: 43.1068, Test Loss: 62.1270\n",
      "Epoch [17030/50000], Train Loss: 34.9881, Test Loss: 51.8731\n",
      "Epoch [17035/50000], Train Loss: 25.7351, Test Loss: 49.2181\n",
      "Epoch [17040/50000], Train Loss: 36.6095, Test Loss: 49.8513\n",
      "Epoch [17045/50000], Train Loss: 38.1868, Test Loss: 50.9514\n",
      "Epoch [17050/50000], Train Loss: 36.7580, Test Loss: 49.7981\n",
      "Epoch [17055/50000], Train Loss: 31.7680, Test Loss: 53.5909\n",
      "Epoch [17060/50000], Train Loss: 92.4788, Test Loss: 49.9086\n",
      "Epoch [17065/50000], Train Loss: 44.8198, Test Loss: 49.4023\n",
      "Epoch [17070/50000], Train Loss: 27.2387, Test Loss: 49.7064\n",
      "Epoch [17075/50000], Train Loss: 38.5905, Test Loss: 51.3310\n",
      "Epoch [17080/50000], Train Loss: 33.1308, Test Loss: 49.8793\n",
      "Epoch [17085/50000], Train Loss: 48.2891, Test Loss: 53.0905\n",
      "Epoch [17090/50000], Train Loss: 44.1409, Test Loss: 51.1050\n",
      "Epoch [17095/50000], Train Loss: 41.3480, Test Loss: 56.6476\n",
      "Epoch [17100/50000], Train Loss: 41.3999, Test Loss: 50.9874\n",
      "Epoch [17105/50000], Train Loss: 44.2844, Test Loss: 50.1102\n",
      "Epoch [17110/50000], Train Loss: 79.8055, Test Loss: 52.8656\n",
      "Epoch [17115/50000], Train Loss: 108.9515, Test Loss: 48.3797\n",
      "Epoch [17120/50000], Train Loss: 39.4323, Test Loss: 51.1694\n",
      "Epoch [17125/50000], Train Loss: 35.2704, Test Loss: 54.0890\n",
      "Epoch [17130/50000], Train Loss: 52.3859, Test Loss: 51.3001\n",
      "Epoch [17135/50000], Train Loss: 39.7130, Test Loss: 49.4362\n",
      "Epoch [17140/50000], Train Loss: 42.2494, Test Loss: 57.1585\n",
      "Epoch [17145/50000], Train Loss: 49.5347, Test Loss: 51.1665\n",
      "Epoch [17150/50000], Train Loss: 42.3543, Test Loss: 49.6593\n",
      "Epoch [17155/50000], Train Loss: 45.0320, Test Loss: 54.3456\n",
      "Epoch [17160/50000], Train Loss: 39.8887, Test Loss: 53.2030\n",
      "Epoch [17165/50000], Train Loss: 32.6121, Test Loss: 52.3520\n",
      "Epoch [17170/50000], Train Loss: 54.0787, Test Loss: 49.6287\n",
      "Epoch [17175/50000], Train Loss: 37.7162, Test Loss: 49.0122\n",
      "Epoch [17180/50000], Train Loss: 31.9715, Test Loss: 47.4670\n",
      "Epoch [17185/50000], Train Loss: 37.9320, Test Loss: 57.9835\n",
      "Epoch [17190/50000], Train Loss: 37.2695, Test Loss: 51.4135\n",
      "Epoch [17195/50000], Train Loss: 37.3972, Test Loss: 54.4287\n",
      "Epoch [17200/50000], Train Loss: 49.6370, Test Loss: 52.1859\n",
      "Epoch [17205/50000], Train Loss: 36.7437, Test Loss: 53.2521\n",
      "Epoch [17210/50000], Train Loss: 45.0986, Test Loss: 51.8561\n",
      "Epoch [17215/50000], Train Loss: 36.6477, Test Loss: 52.4361\n",
      "Epoch [17220/50000], Train Loss: 45.5636, Test Loss: 51.0526\n",
      "Epoch [17225/50000], Train Loss: 38.9366, Test Loss: 54.2543\n",
      "Epoch [17230/50000], Train Loss: 46.5890, Test Loss: 53.0659\n",
      "Epoch [17235/50000], Train Loss: 40.7288, Test Loss: 52.8238\n",
      "Epoch [17240/50000], Train Loss: 110.4569, Test Loss: 164.3736\n",
      "Epoch [17245/50000], Train Loss: 43.7792, Test Loss: 52.9752\n",
      "Epoch [17250/50000], Train Loss: 40.2849, Test Loss: 53.7932\n",
      "Epoch [17255/50000], Train Loss: 45.3285, Test Loss: 56.9285\n",
      "Epoch [17260/50000], Train Loss: 35.9428, Test Loss: 50.4607\n",
      "Epoch [17265/50000], Train Loss: 41.2738, Test Loss: 50.3220\n",
      "Epoch [17270/50000], Train Loss: 38.8592, Test Loss: 53.8277\n",
      "Epoch [17275/50000], Train Loss: 51.9935, Test Loss: 59.8381\n",
      "Epoch [17280/50000], Train Loss: 71.7416, Test Loss: 50.7996\n",
      "Epoch [17285/50000], Train Loss: 45.7152, Test Loss: 50.0361\n",
      "Epoch [17290/50000], Train Loss: 46.1371, Test Loss: 51.3574\n",
      "Epoch [17295/50000], Train Loss: 37.6215, Test Loss: 50.2506\n",
      "Epoch [17300/50000], Train Loss: 99.2899, Test Loss: 48.0191\n",
      "Epoch [17305/50000], Train Loss: 48.8293, Test Loss: 50.4214\n",
      "Epoch [17310/50000], Train Loss: 38.3842, Test Loss: 61.5135\n",
      "Epoch [17315/50000], Train Loss: 45.2482, Test Loss: 64.6089\n",
      "Epoch [17320/50000], Train Loss: 42.4858, Test Loss: 48.6598\n",
      "Epoch [17325/50000], Train Loss: 45.5581, Test Loss: 52.5637\n",
      "Epoch [17330/50000], Train Loss: 88.7466, Test Loss: 55.0139\n",
      "Epoch [17335/50000], Train Loss: 35.0583, Test Loss: 52.6708\n",
      "Epoch [17340/50000], Train Loss: 40.2120, Test Loss: 51.9040\n",
      "Epoch [17345/50000], Train Loss: 34.0786, Test Loss: 48.4712\n",
      "Epoch [17350/50000], Train Loss: 40.0355, Test Loss: 58.6959\n",
      "Epoch [17355/50000], Train Loss: 42.1217, Test Loss: 52.9976\n",
      "Epoch [17360/50000], Train Loss: 35.8064, Test Loss: 49.9860\n",
      "Epoch [17365/50000], Train Loss: 30.7517, Test Loss: 49.4293\n",
      "Epoch [17370/50000], Train Loss: 47.6777, Test Loss: 68.9528\n",
      "Epoch [17375/50000], Train Loss: 34.9371, Test Loss: 51.3687\n",
      "Epoch [17380/50000], Train Loss: 47.2631, Test Loss: 51.6648\n",
      "Epoch [17385/50000], Train Loss: 32.2447, Test Loss: 50.7078\n",
      "Epoch [17390/50000], Train Loss: 57.3972, Test Loss: 88.0943\n",
      "Epoch [17395/50000], Train Loss: 35.7592, Test Loss: 55.9742\n",
      "Epoch [17400/50000], Train Loss: 42.3326, Test Loss: 53.0545\n",
      "Epoch [17405/50000], Train Loss: 33.2919, Test Loss: 48.6861\n",
      "Epoch [17410/50000], Train Loss: 37.6325, Test Loss: 51.4861\n",
      "Epoch [17415/50000], Train Loss: 49.6804, Test Loss: 50.2108\n",
      "Epoch [17420/50000], Train Loss: 28.5674, Test Loss: 48.0292\n",
      "Epoch [17425/50000], Train Loss: 34.0079, Test Loss: 57.6945\n",
      "Epoch [17430/50000], Train Loss: 39.8830, Test Loss: 51.8534\n",
      "Epoch [17435/50000], Train Loss: 40.9289, Test Loss: 49.8582\n",
      "Epoch [17440/50000], Train Loss: 41.5968, Test Loss: 49.1904\n",
      "Epoch [17445/50000], Train Loss: 34.7920, Test Loss: 49.7705\n",
      "Epoch [17450/50000], Train Loss: 39.4466, Test Loss: 49.8120\n",
      "Epoch [17455/50000], Train Loss: 38.2004, Test Loss: 49.1849\n",
      "Epoch [17460/50000], Train Loss: 42.8147, Test Loss: 51.1890\n",
      "Epoch [17465/50000], Train Loss: 44.3512, Test Loss: 50.9472\n",
      "Epoch [17470/50000], Train Loss: 49.9823, Test Loss: 49.3791\n",
      "Epoch [17475/50000], Train Loss: 51.9347, Test Loss: 50.2197\n",
      "Epoch [17480/50000], Train Loss: 54.2859, Test Loss: 51.8810\n",
      "Epoch [17485/50000], Train Loss: 44.2844, Test Loss: 49.1645\n",
      "Epoch [17490/50000], Train Loss: 35.1482, Test Loss: 48.7006\n",
      "Epoch [17495/50000], Train Loss: 32.4659, Test Loss: 53.8181\n",
      "Epoch [17500/50000], Train Loss: 35.5485, Test Loss: 56.9041\n",
      "Epoch [17505/50000], Train Loss: 39.3653, Test Loss: 48.4343\n",
      "Epoch [17510/50000], Train Loss: 29.7853, Test Loss: 48.2384\n",
      "Epoch [17515/50000], Train Loss: 37.4655, Test Loss: 53.3282\n",
      "Epoch [17520/50000], Train Loss: 43.8437, Test Loss: 50.6170\n",
      "Epoch [17525/50000], Train Loss: 55.9935, Test Loss: 49.1910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17530/50000], Train Loss: 39.6761, Test Loss: 50.3309\n",
      "Epoch [17535/50000], Train Loss: 37.4500, Test Loss: 54.2038\n",
      "Epoch [17540/50000], Train Loss: 43.6585, Test Loss: 48.1398\n",
      "Epoch [17545/50000], Train Loss: 34.0072, Test Loss: 52.5155\n",
      "Epoch [17550/50000], Train Loss: 46.5477, Test Loss: 52.2517\n",
      "Epoch [17555/50000], Train Loss: 35.5787, Test Loss: 52.6075\n",
      "Epoch [17560/50000], Train Loss: 38.9268, Test Loss: 51.3689\n",
      "Epoch [17565/50000], Train Loss: 43.8225, Test Loss: 51.4700\n",
      "Epoch [17570/50000], Train Loss: 35.2902, Test Loss: 48.6589\n",
      "Epoch [17575/50000], Train Loss: 45.2157, Test Loss: 50.8896\n",
      "Epoch [17580/50000], Train Loss: 39.3222, Test Loss: 49.1894\n",
      "Epoch [17585/50000], Train Loss: 41.9243, Test Loss: 53.9843\n",
      "Epoch [17590/50000], Train Loss: 38.6791, Test Loss: 48.3940\n",
      "Epoch [17595/50000], Train Loss: 33.1610, Test Loss: 49.5224\n",
      "Epoch [17600/50000], Train Loss: 79.7438, Test Loss: 75.4231\n",
      "Epoch [17605/50000], Train Loss: 44.7752, Test Loss: 56.4320\n",
      "Epoch [17610/50000], Train Loss: 27.2847, Test Loss: 51.0634\n",
      "Epoch [17615/50000], Train Loss: 40.8276, Test Loss: 51.4964\n",
      "Epoch [17620/50000], Train Loss: 40.5537, Test Loss: 48.8871\n",
      "Epoch [17625/50000], Train Loss: 35.8823, Test Loss: 51.9551\n",
      "Epoch [17630/50000], Train Loss: 37.0628, Test Loss: 51.0656\n",
      "Epoch [17635/50000], Train Loss: 44.6901, Test Loss: 50.8264\n",
      "Epoch [17640/50000], Train Loss: 42.8147, Test Loss: 53.7229\n",
      "Epoch [17645/50000], Train Loss: 41.3704, Test Loss: 49.6037\n",
      "Epoch [17650/50000], Train Loss: 37.7602, Test Loss: 54.9285\n",
      "Epoch [17655/50000], Train Loss: 35.8447, Test Loss: 51.5207\n",
      "Epoch [17660/50000], Train Loss: 34.1899, Test Loss: 48.2174\n",
      "Epoch [17665/50000], Train Loss: 47.0903, Test Loss: 58.1711\n",
      "Epoch [17670/50000], Train Loss: 44.9476, Test Loss: 67.6229\n",
      "Epoch [17675/50000], Train Loss: 47.2878, Test Loss: 51.9685\n",
      "Epoch [17680/50000], Train Loss: 35.1998, Test Loss: 50.5884\n",
      "Epoch [17685/50000], Train Loss: 78.0405, Test Loss: 56.8797\n",
      "Epoch [17690/50000], Train Loss: 40.8401, Test Loss: 86.5070\n",
      "Epoch [17695/50000], Train Loss: 39.1760, Test Loss: 50.5344\n",
      "Epoch [17700/50000], Train Loss: 41.1664, Test Loss: 54.4185\n",
      "Epoch [17705/50000], Train Loss: 35.7061, Test Loss: 49.9937\n",
      "Epoch [17710/50000], Train Loss: 37.9327, Test Loss: 48.7850\n",
      "Epoch [17715/50000], Train Loss: 45.6942, Test Loss: 50.5894\n",
      "Epoch [17720/50000], Train Loss: 43.6085, Test Loss: 50.0347\n",
      "Epoch [17725/50000], Train Loss: 41.0654, Test Loss: 48.5204\n",
      "Epoch [17730/50000], Train Loss: 38.1500, Test Loss: 51.1938\n",
      "Epoch [17735/50000], Train Loss: 32.3680, Test Loss: 48.8943\n",
      "Epoch [17740/50000], Train Loss: 33.6815, Test Loss: 56.2451\n",
      "Epoch [17745/50000], Train Loss: 41.1966, Test Loss: 53.9178\n",
      "Epoch [17750/50000], Train Loss: 39.9884, Test Loss: 57.7949\n",
      "Epoch [17755/50000], Train Loss: 46.8388, Test Loss: 75.2508\n",
      "Epoch [17760/50000], Train Loss: 28.8989, Test Loss: 52.5475\n",
      "Epoch [17765/50000], Train Loss: 41.9753, Test Loss: 47.8669\n",
      "Epoch [17770/50000], Train Loss: 30.5955, Test Loss: 51.9581\n",
      "Epoch [17775/50000], Train Loss: 33.6338, Test Loss: 53.2681\n",
      "Epoch [17780/50000], Train Loss: 52.9287, Test Loss: 56.5598\n",
      "Epoch [17785/50000], Train Loss: 40.1294, Test Loss: 64.1097\n",
      "Epoch [17790/50000], Train Loss: 56.6842, Test Loss: 47.9994\n",
      "Epoch [17795/50000], Train Loss: 37.9655, Test Loss: 51.5488\n",
      "Epoch [17800/50000], Train Loss: 37.1330, Test Loss: 51.7333\n",
      "Epoch [17805/50000], Train Loss: 43.8306, Test Loss: 75.0868\n",
      "Epoch [17810/50000], Train Loss: 43.4324, Test Loss: 51.0458\n",
      "Epoch [17815/50000], Train Loss: 38.6740, Test Loss: 50.2551\n",
      "Epoch [17820/50000], Train Loss: 33.0701, Test Loss: 50.2149\n",
      "Epoch [17825/50000], Train Loss: 36.7003, Test Loss: 54.3409\n",
      "Epoch [17830/50000], Train Loss: 40.0924, Test Loss: 53.1959\n",
      "Epoch [17835/50000], Train Loss: 36.1471, Test Loss: 54.2883\n",
      "Epoch [17840/50000], Train Loss: 38.6545, Test Loss: 50.3851\n",
      "Epoch [17845/50000], Train Loss: 34.5860, Test Loss: 48.0438\n",
      "Epoch [17850/50000], Train Loss: 33.3642, Test Loss: 49.0418\n",
      "Epoch [17855/50000], Train Loss: 41.1345, Test Loss: 50.5974\n",
      "Epoch [17860/50000], Train Loss: 32.7292, Test Loss: 51.1254\n",
      "Epoch [17865/50000], Train Loss: 30.4145, Test Loss: 49.6877\n",
      "Epoch [17870/50000], Train Loss: 31.0439, Test Loss: 48.1891\n",
      "Epoch [17875/50000], Train Loss: 38.2992, Test Loss: 60.4695\n",
      "Epoch [17880/50000], Train Loss: 35.5480, Test Loss: 47.8848\n",
      "Epoch [17885/50000], Train Loss: 34.2306, Test Loss: 51.3537\n",
      "Epoch [17890/50000], Train Loss: 46.7483, Test Loss: 54.7773\n",
      "Epoch [17895/50000], Train Loss: 37.3189, Test Loss: 50.4773\n",
      "Epoch [17900/50000], Train Loss: 33.4353, Test Loss: 50.8960\n",
      "Epoch [17905/50000], Train Loss: 36.2771, Test Loss: 51.7664\n",
      "Epoch [17910/50000], Train Loss: 41.8274, Test Loss: 51.9687\n",
      "Epoch [17915/50000], Train Loss: 34.3395, Test Loss: 49.8466\n",
      "Epoch [17920/50000], Train Loss: 33.4807, Test Loss: 50.3922\n",
      "Epoch [17925/50000], Train Loss: 42.3257, Test Loss: 79.7523\n",
      "Epoch [17930/50000], Train Loss: 48.7015, Test Loss: 50.0395\n",
      "Epoch [17935/50000], Train Loss: 64.3798, Test Loss: 50.6639\n",
      "Epoch [17940/50000], Train Loss: 40.4341, Test Loss: 50.0359\n",
      "Epoch [17945/50000], Train Loss: 41.8800, Test Loss: 55.7251\n",
      "Epoch [17950/50000], Train Loss: 27.0680, Test Loss: 49.2293\n",
      "Epoch [17955/50000], Train Loss: 37.3631, Test Loss: 50.6102\n",
      "Epoch [17960/50000], Train Loss: 65.7178, Test Loss: 48.2135\n",
      "Epoch [17965/50000], Train Loss: 42.2627, Test Loss: 53.0021\n",
      "Epoch [17970/50000], Train Loss: 28.1478, Test Loss: 49.5892\n",
      "Epoch [17975/50000], Train Loss: 35.7278, Test Loss: 49.8172\n",
      "Epoch [17980/50000], Train Loss: 33.9591, Test Loss: 53.8078\n",
      "Epoch [17985/50000], Train Loss: 51.9542, Test Loss: 51.2501\n",
      "Epoch [17990/50000], Train Loss: 31.8784, Test Loss: 54.7907\n",
      "Epoch [17995/50000], Train Loss: 41.3415, Test Loss: 54.9554\n",
      "Epoch [18000/50000], Train Loss: 44.8676, Test Loss: 48.1295\n",
      "Epoch [18005/50000], Train Loss: 27.7516, Test Loss: 48.1022\n",
      "Epoch [18010/50000], Train Loss: 39.7297, Test Loss: 51.1332\n",
      "Epoch [18015/50000], Train Loss: 41.8231, Test Loss: 55.4565\n",
      "Epoch [18020/50000], Train Loss: 38.4519, Test Loss: 52.7428\n",
      "Epoch [18025/50000], Train Loss: 40.8089, Test Loss: 54.7126\n",
      "Epoch [18030/50000], Train Loss: 39.5868, Test Loss: 49.6801\n",
      "Epoch [18035/50000], Train Loss: 54.1343, Test Loss: 53.2862\n",
      "Epoch [18040/50000], Train Loss: 27.3510, Test Loss: 51.4260\n",
      "Epoch [18045/50000], Train Loss: 42.0753, Test Loss: 62.6129\n",
      "Epoch [18050/50000], Train Loss: 43.0836, Test Loss: 61.0009\n",
      "Epoch [18055/50000], Train Loss: 36.5069, Test Loss: 49.2922\n",
      "Epoch [18060/50000], Train Loss: 40.3478, Test Loss: 56.5999\n",
      "Epoch [18065/50000], Train Loss: 37.4414, Test Loss: 52.0767\n",
      "Epoch [18070/50000], Train Loss: 42.6078, Test Loss: 52.5768\n",
      "Epoch [18075/50000], Train Loss: 36.3035, Test Loss: 54.5125\n",
      "Epoch [18080/50000], Train Loss: 36.6569, Test Loss: 49.1891\n",
      "Epoch [18085/50000], Train Loss: 42.6183, Test Loss: 53.7596\n",
      "Epoch [18090/50000], Train Loss: 26.6920, Test Loss: 50.3002\n",
      "Epoch [18095/50000], Train Loss: 41.2804, Test Loss: 58.2275\n",
      "Epoch [18100/50000], Train Loss: 43.5527, Test Loss: 53.0901\n",
      "Epoch [18105/50000], Train Loss: 26.3587, Test Loss: 51.0414\n",
      "Epoch [18110/50000], Train Loss: 42.3784, Test Loss: 50.2106\n",
      "Epoch [18115/50000], Train Loss: 53.4144, Test Loss: 50.5844\n",
      "Epoch [18120/50000], Train Loss: 36.9026, Test Loss: 50.9930\n",
      "Epoch [18125/50000], Train Loss: 28.4228, Test Loss: 47.4280\n",
      "Epoch [18130/50000], Train Loss: 39.9160, Test Loss: 52.6072\n",
      "Epoch [18135/50000], Train Loss: 64.1065, Test Loss: 52.3244\n",
      "Epoch [18140/50000], Train Loss: 38.0278, Test Loss: 51.6415\n",
      "Epoch [18145/50000], Train Loss: 40.8917, Test Loss: 50.0981\n",
      "Epoch [18150/50000], Train Loss: 74.2151, Test Loss: 47.8954\n",
      "Epoch [18155/50000], Train Loss: 43.5350, Test Loss: 50.9567\n",
      "Epoch [18160/50000], Train Loss: 39.5243, Test Loss: 57.8450\n",
      "Epoch [18165/50000], Train Loss: 39.0642, Test Loss: 52.3327\n",
      "Epoch [18170/50000], Train Loss: 64.7942, Test Loss: 74.1760\n",
      "Epoch [18175/50000], Train Loss: 34.3779, Test Loss: 51.8597\n",
      "Epoch [18180/50000], Train Loss: 43.2659, Test Loss: 54.3757\n",
      "Epoch [18185/50000], Train Loss: 40.7924, Test Loss: 50.2191\n",
      "Epoch [18190/50000], Train Loss: 31.2764, Test Loss: 48.9483\n",
      "Epoch [18195/50000], Train Loss: 49.9770, Test Loss: 53.3406\n",
      "Epoch [18200/50000], Train Loss: 39.4158, Test Loss: 50.2489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18205/50000], Train Loss: 36.8181, Test Loss: 51.1930\n",
      "Epoch [18210/50000], Train Loss: 37.9176, Test Loss: 53.7964\n",
      "Epoch [18215/50000], Train Loss: 51.4511, Test Loss: 51.5014\n",
      "Epoch [18220/50000], Train Loss: 41.2784, Test Loss: 49.4024\n",
      "Epoch [18225/50000], Train Loss: 35.7398, Test Loss: 48.1636\n",
      "Epoch [18230/50000], Train Loss: 35.4667, Test Loss: 51.5865\n",
      "Epoch [18235/50000], Train Loss: 42.2599, Test Loss: 50.9571\n",
      "Epoch [18240/50000], Train Loss: 74.3834, Test Loss: 48.1080\n",
      "Epoch [18245/50000], Train Loss: 86.4100, Test Loss: 49.4400\n",
      "Epoch [18250/50000], Train Loss: 35.8738, Test Loss: 51.5948\n",
      "Epoch [18255/50000], Train Loss: 43.3293, Test Loss: 69.9389\n",
      "Epoch [18260/50000], Train Loss: 39.6993, Test Loss: 48.2245\n",
      "Epoch [18265/50000], Train Loss: 43.9458, Test Loss: 53.7913\n",
      "Epoch [18270/50000], Train Loss: 37.2617, Test Loss: 51.5480\n",
      "Epoch [18275/50000], Train Loss: 31.2086, Test Loss: 48.2185\n",
      "Epoch [18280/50000], Train Loss: 44.1099, Test Loss: 48.7509\n",
      "Epoch [18285/50000], Train Loss: 39.3630, Test Loss: 50.0987\n",
      "Epoch [18290/50000], Train Loss: 76.5713, Test Loss: 49.8586\n",
      "Epoch [18295/50000], Train Loss: 43.6047, Test Loss: 48.2456\n",
      "Epoch [18300/50000], Train Loss: 29.7389, Test Loss: 49.3744\n",
      "Epoch [18305/50000], Train Loss: 41.8862, Test Loss: 55.2583\n",
      "Epoch [18310/50000], Train Loss: 76.5604, Test Loss: 50.6968\n",
      "Epoch [18315/50000], Train Loss: 34.2379, Test Loss: 52.1627\n",
      "Epoch [18320/50000], Train Loss: 33.3787, Test Loss: 51.4183\n",
      "Epoch [18325/50000], Train Loss: 52.0282, Test Loss: 56.3909\n",
      "Epoch [18330/50000], Train Loss: 35.7775, Test Loss: 50.8162\n",
      "Epoch [18335/50000], Train Loss: 42.4692, Test Loss: 49.5686\n",
      "Epoch [18340/50000], Train Loss: 33.1222, Test Loss: 51.8697\n",
      "Epoch [18345/50000], Train Loss: 38.0971, Test Loss: 49.0693\n",
      "Epoch [18350/50000], Train Loss: 71.5920, Test Loss: 99.8825\n",
      "Epoch [18355/50000], Train Loss: 38.3129, Test Loss: 53.9614\n",
      "Epoch [18360/50000], Train Loss: 64.9882, Test Loss: 49.4774\n",
      "Epoch [18365/50000], Train Loss: 35.1199, Test Loss: 50.4789\n",
      "Epoch [18370/50000], Train Loss: 29.1318, Test Loss: 53.7208\n",
      "Epoch [18375/50000], Train Loss: 36.7790, Test Loss: 51.8348\n",
      "Epoch [18380/50000], Train Loss: 34.2656, Test Loss: 48.8107\n",
      "Epoch [18385/50000], Train Loss: 36.0269, Test Loss: 48.6304\n",
      "Epoch [18390/50000], Train Loss: 34.3884, Test Loss: 51.8546\n",
      "Epoch [18395/50000], Train Loss: 50.2935, Test Loss: 53.8838\n",
      "Epoch [18400/50000], Train Loss: 36.7785, Test Loss: 52.6412\n",
      "Epoch [18405/50000], Train Loss: 30.3880, Test Loss: 48.5687\n",
      "Epoch [18410/50000], Train Loss: 23.4287, Test Loss: 49.4078\n",
      "Epoch [18415/50000], Train Loss: 35.1107, Test Loss: 48.6613\n",
      "Epoch [18420/50000], Train Loss: 34.9475, Test Loss: 53.0533\n",
      "Epoch [18425/50000], Train Loss: 57.6771, Test Loss: 53.0590\n",
      "Epoch [18430/50000], Train Loss: 41.5902, Test Loss: 50.4168\n",
      "Epoch [18435/50000], Train Loss: 37.9142, Test Loss: 57.9914\n",
      "Epoch [18440/50000], Train Loss: 42.2625, Test Loss: 48.8345\n",
      "Epoch [18445/50000], Train Loss: 43.0965, Test Loss: 57.2751\n",
      "Epoch [18450/50000], Train Loss: 37.0365, Test Loss: 51.7797\n",
      "Epoch [18455/50000], Train Loss: 43.9911, Test Loss: 50.9656\n",
      "Epoch [18460/50000], Train Loss: 42.9242, Test Loss: 52.7985\n",
      "Epoch [18465/50000], Train Loss: 37.2866, Test Loss: 48.9942\n",
      "Epoch [18470/50000], Train Loss: 40.4170, Test Loss: 52.7234\n",
      "Epoch [18475/50000], Train Loss: 141.1855, Test Loss: 53.1281\n",
      "Epoch [18480/50000], Train Loss: 35.3838, Test Loss: 63.5746\n",
      "Epoch [18485/50000], Train Loss: 37.2291, Test Loss: 48.5021\n",
      "Epoch [18490/50000], Train Loss: 36.7388, Test Loss: 49.5181\n",
      "Epoch [18495/50000], Train Loss: 37.2671, Test Loss: 52.9383\n",
      "Epoch [18500/50000], Train Loss: 56.6674, Test Loss: 51.1674\n",
      "Epoch [18505/50000], Train Loss: 58.5028, Test Loss: 48.8147\n",
      "Epoch [18510/50000], Train Loss: 50.2864, Test Loss: 49.1688\n",
      "Epoch [18515/50000], Train Loss: 40.4662, Test Loss: 49.7363\n",
      "Epoch [18520/50000], Train Loss: 37.0522, Test Loss: 52.1266\n",
      "Epoch [18525/50000], Train Loss: 59.3708, Test Loss: 52.3136\n",
      "Epoch [18530/50000], Train Loss: 36.1686, Test Loss: 51.3789\n",
      "Epoch [18535/50000], Train Loss: 41.9080, Test Loss: 49.8063\n",
      "Epoch [18540/50000], Train Loss: 43.8867, Test Loss: 53.4130\n",
      "Epoch [18545/50000], Train Loss: 41.8412, Test Loss: 48.9344\n",
      "Epoch [18550/50000], Train Loss: 33.5768, Test Loss: 48.7815\n",
      "Epoch [18555/50000], Train Loss: 44.9261, Test Loss: 49.7411\n",
      "Epoch [18560/50000], Train Loss: 35.0100, Test Loss: 60.8580\n",
      "Epoch [18565/50000], Train Loss: 39.3009, Test Loss: 48.9887\n",
      "Epoch [18570/50000], Train Loss: 61.9858, Test Loss: 52.4222\n",
      "Epoch [18575/50000], Train Loss: 35.2188, Test Loss: 58.8721\n",
      "Epoch [18580/50000], Train Loss: 37.0019, Test Loss: 50.7486\n",
      "Epoch [18585/50000], Train Loss: 34.2017, Test Loss: 51.3253\n",
      "Epoch [18590/50000], Train Loss: 36.8375, Test Loss: 48.6219\n",
      "Epoch [18595/50000], Train Loss: 37.8780, Test Loss: 53.7613\n",
      "Epoch [18600/50000], Train Loss: 80.0622, Test Loss: 67.5704\n",
      "Epoch [18605/50000], Train Loss: 42.1482, Test Loss: 48.6289\n",
      "Epoch [18610/50000], Train Loss: 34.3813, Test Loss: 50.3324\n",
      "Epoch [18615/50000], Train Loss: 42.3669, Test Loss: 52.2267\n",
      "Epoch [18620/50000], Train Loss: 39.2079, Test Loss: 50.1198\n",
      "Epoch [18625/50000], Train Loss: 31.7679, Test Loss: 50.6453\n",
      "Epoch [18630/50000], Train Loss: 32.2521, Test Loss: 61.9860\n",
      "Epoch [18635/50000], Train Loss: 42.2923, Test Loss: 55.4865\n",
      "Epoch [18640/50000], Train Loss: 40.0466, Test Loss: 56.9960\n",
      "Epoch [18645/50000], Train Loss: 40.1050, Test Loss: 85.4611\n",
      "Epoch [18650/50000], Train Loss: 34.7632, Test Loss: 50.0610\n",
      "Epoch [18655/50000], Train Loss: 44.2935, Test Loss: 58.3382\n",
      "Epoch [18660/50000], Train Loss: 35.4711, Test Loss: 52.3260\n",
      "Epoch [18665/50000], Train Loss: 32.7177, Test Loss: 51.1200\n",
      "Epoch [18670/50000], Train Loss: 37.0794, Test Loss: 49.5754\n",
      "Epoch [18675/50000], Train Loss: 47.0063, Test Loss: 48.5831\n",
      "Epoch [18680/50000], Train Loss: 32.6427, Test Loss: 55.6316\n",
      "Epoch [18685/50000], Train Loss: 36.3270, Test Loss: 56.8449\n",
      "Epoch [18690/50000], Train Loss: 49.2482, Test Loss: 48.8488\n",
      "Epoch [18695/50000], Train Loss: 42.5137, Test Loss: 51.9879\n",
      "Epoch [18700/50000], Train Loss: 39.4767, Test Loss: 53.3592\n",
      "Epoch [18705/50000], Train Loss: 26.9238, Test Loss: 46.9891\n",
      "Epoch [18710/50000], Train Loss: 30.5441, Test Loss: 46.8051\n",
      "Epoch [18715/50000], Train Loss: 28.9417, Test Loss: 52.3033\n",
      "Epoch [18720/50000], Train Loss: 47.2257, Test Loss: 51.1793\n",
      "Epoch [18725/50000], Train Loss: 40.4054, Test Loss: 51.8425\n",
      "Epoch [18730/50000], Train Loss: 35.0644, Test Loss: 55.0471\n",
      "Epoch [18735/50000], Train Loss: 48.8942, Test Loss: 48.3757\n",
      "Epoch [18740/50000], Train Loss: 34.5267, Test Loss: 53.8877\n",
      "Epoch [18745/50000], Train Loss: 39.4217, Test Loss: 49.0710\n",
      "Epoch [18750/50000], Train Loss: 38.8258, Test Loss: 50.6038\n",
      "Epoch [18755/50000], Train Loss: 45.5966, Test Loss: 48.4865\n",
      "Epoch [18760/50000], Train Loss: 36.7244, Test Loss: 52.1587\n",
      "Epoch [18765/50000], Train Loss: 37.2931, Test Loss: 53.8737\n",
      "Epoch [18770/50000], Train Loss: 57.5046, Test Loss: 49.4637\n",
      "Epoch [18775/50000], Train Loss: 25.2815, Test Loss: 48.9990\n",
      "Epoch [18780/50000], Train Loss: 46.3432, Test Loss: 49.4433\n",
      "Epoch [18785/50000], Train Loss: 43.8198, Test Loss: 51.0640\n",
      "Epoch [18790/50000], Train Loss: 35.7060, Test Loss: 49.9175\n",
      "Epoch [18795/50000], Train Loss: 32.0503, Test Loss: 50.5626\n",
      "Epoch [18800/50000], Train Loss: 42.1385, Test Loss: 61.6143\n",
      "Epoch [18805/50000], Train Loss: 41.7164, Test Loss: 53.9829\n",
      "Epoch [18810/50000], Train Loss: 37.2105, Test Loss: 48.3953\n",
      "Epoch [18815/50000], Train Loss: 36.1201, Test Loss: 52.5123\n",
      "Epoch [18820/50000], Train Loss: 35.7103, Test Loss: 50.8557\n",
      "Epoch [18825/50000], Train Loss: 36.9456, Test Loss: 51.0793\n",
      "Epoch [18830/50000], Train Loss: 36.8166, Test Loss: 50.3623\n",
      "Epoch [18835/50000], Train Loss: 42.8533, Test Loss: 53.7110\n",
      "Epoch [18840/50000], Train Loss: 42.9328, Test Loss: 65.4831\n",
      "Epoch [18845/50000], Train Loss: 82.4481, Test Loss: 70.7803\n",
      "Epoch [18850/50000], Train Loss: 48.0209, Test Loss: 49.5307\n",
      "Epoch [18855/50000], Train Loss: 45.3267, Test Loss: 54.0521\n",
      "Epoch [18860/50000], Train Loss: 84.1656, Test Loss: 51.4968\n",
      "Epoch [18865/50000], Train Loss: 39.1577, Test Loss: 51.3605\n",
      "Epoch [18870/50000], Train Loss: 31.3136, Test Loss: 46.8399\n",
      "Epoch [18875/50000], Train Loss: 60.0127, Test Loss: 61.0220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18880/50000], Train Loss: 48.6238, Test Loss: 50.3929\n",
      "Epoch [18885/50000], Train Loss: 34.0715, Test Loss: 54.4805\n",
      "Epoch [18890/50000], Train Loss: 38.5953, Test Loss: 48.5232\n",
      "Epoch [18895/50000], Train Loss: 53.0921, Test Loss: 52.8323\n",
      "Epoch [18900/50000], Train Loss: 36.5927, Test Loss: 52.8597\n",
      "Epoch [18905/50000], Train Loss: 34.9019, Test Loss: 50.2329\n",
      "Epoch [18910/50000], Train Loss: 38.3377, Test Loss: 49.9892\n",
      "Epoch [18915/50000], Train Loss: 42.9730, Test Loss: 89.4335\n",
      "Epoch [18920/50000], Train Loss: 39.5743, Test Loss: 54.4040\n",
      "Epoch [18925/50000], Train Loss: 35.0553, Test Loss: 48.9694\n",
      "Epoch [18930/50000], Train Loss: 48.9387, Test Loss: 50.6834\n",
      "Epoch [18935/50000], Train Loss: 45.3411, Test Loss: 51.6035\n",
      "Epoch [18940/50000], Train Loss: 31.7852, Test Loss: 51.5312\n",
      "Epoch [18945/50000], Train Loss: 26.3497, Test Loss: 47.4207\n",
      "Epoch [18950/50000], Train Loss: 41.5767, Test Loss: 49.3796\n",
      "Epoch [18955/50000], Train Loss: 54.9996, Test Loss: 51.1602\n",
      "Epoch [18960/50000], Train Loss: 54.8527, Test Loss: 49.8587\n",
      "Epoch [18965/50000], Train Loss: 47.1774, Test Loss: 49.7770\n",
      "Epoch [18970/50000], Train Loss: 23.4287, Test Loss: 47.1554\n",
      "Epoch [18975/50000], Train Loss: 42.7574, Test Loss: 52.6636\n",
      "Epoch [18980/50000], Train Loss: 36.5325, Test Loss: 71.5299\n",
      "Epoch [18985/50000], Train Loss: 33.3419, Test Loss: 51.8128\n",
      "Epoch [18990/50000], Train Loss: 272.3913, Test Loss: 47.2020\n",
      "Epoch [18995/50000], Train Loss: 33.4870, Test Loss: 49.8648\n",
      "Epoch [19000/50000], Train Loss: 28.0469, Test Loss: 47.2495\n",
      "Epoch [19005/50000], Train Loss: 41.1512, Test Loss: 49.1432\n",
      "Epoch [19010/50000], Train Loss: 34.2438, Test Loss: 47.9275\n",
      "Epoch [19015/50000], Train Loss: 38.4548, Test Loss: 50.3340\n",
      "Epoch [19020/50000], Train Loss: 41.6750, Test Loss: 48.5128\n",
      "Epoch [19025/50000], Train Loss: 34.0121, Test Loss: 50.1439\n",
      "Epoch [19030/50000], Train Loss: 43.3222, Test Loss: 51.7909\n",
      "Epoch [19035/50000], Train Loss: 25.9622, Test Loss: 48.2076\n",
      "Epoch [19040/50000], Train Loss: 37.0367, Test Loss: 51.1086\n",
      "Epoch [19045/50000], Train Loss: 29.5319, Test Loss: 51.4814\n",
      "Epoch [19050/50000], Train Loss: 32.4354, Test Loss: 53.6879\n",
      "Epoch [19055/50000], Train Loss: 27.5670, Test Loss: 48.8348\n",
      "Epoch [19060/50000], Train Loss: 37.6240, Test Loss: 51.7004\n",
      "Epoch [19065/50000], Train Loss: 32.8216, Test Loss: 55.3891\n",
      "Epoch [19070/50000], Train Loss: 33.4622, Test Loss: 49.4261\n",
      "Epoch [19075/50000], Train Loss: 34.0146, Test Loss: 51.5892\n",
      "Epoch [19080/50000], Train Loss: 38.6570, Test Loss: 48.9327\n",
      "Epoch [19085/50000], Train Loss: 33.3811, Test Loss: 47.9090\n",
      "Epoch [19090/50000], Train Loss: 42.2342, Test Loss: 52.9476\n",
      "Epoch [19095/50000], Train Loss: 37.2396, Test Loss: 62.3386\n",
      "Epoch [19100/50000], Train Loss: 39.7243, Test Loss: 47.4254\n",
      "Epoch [19105/50000], Train Loss: 38.0271, Test Loss: 51.0994\n",
      "Epoch [19110/50000], Train Loss: 34.0791, Test Loss: 54.2279\n",
      "Epoch [19115/50000], Train Loss: 34.6930, Test Loss: 55.1282\n",
      "Epoch [19120/50000], Train Loss: 32.6038, Test Loss: 51.0676\n",
      "Epoch [19125/50000], Train Loss: 42.4309, Test Loss: 49.8355\n",
      "Epoch [19130/50000], Train Loss: 47.5248, Test Loss: 53.3852\n",
      "Epoch [19135/50000], Train Loss: 44.3493, Test Loss: 51.7940\n",
      "Epoch [19140/50000], Train Loss: 35.5319, Test Loss: 48.2020\n",
      "Epoch [19145/50000], Train Loss: 48.5390, Test Loss: 51.6160\n",
      "Epoch [19150/50000], Train Loss: 35.8763, Test Loss: 55.8740\n",
      "Epoch [19155/50000], Train Loss: 39.2109, Test Loss: 49.9568\n",
      "Epoch [19160/50000], Train Loss: 38.9997, Test Loss: 52.4954\n",
      "Epoch [19165/50000], Train Loss: 36.9826, Test Loss: 50.0174\n",
      "Epoch [19170/50000], Train Loss: 33.3105, Test Loss: 49.7241\n",
      "Epoch [19175/50000], Train Loss: 42.8466, Test Loss: 55.1927\n",
      "Epoch [19180/50000], Train Loss: 40.6143, Test Loss: 51.1231\n",
      "Epoch [19185/50000], Train Loss: 40.3919, Test Loss: 47.9336\n",
      "Epoch [19190/50000], Train Loss: 34.9562, Test Loss: 49.7514\n",
      "Epoch [19195/50000], Train Loss: 36.4874, Test Loss: 87.4290\n",
      "Epoch [19200/50000], Train Loss: 38.0971, Test Loss: 60.1317\n",
      "Epoch [19205/50000], Train Loss: 36.4861, Test Loss: 57.5867\n",
      "Epoch [19210/50000], Train Loss: 26.9554, Test Loss: 48.9075\n",
      "Epoch [19215/50000], Train Loss: 49.8900, Test Loss: 52.6573\n",
      "Epoch [19220/50000], Train Loss: 39.7199, Test Loss: 49.9537\n",
      "Epoch [19225/50000], Train Loss: 31.5107, Test Loss: 49.0409\n",
      "Epoch [19230/50000], Train Loss: 26.6039, Test Loss: 48.1693\n",
      "Epoch [19235/50000], Train Loss: 31.8551, Test Loss: 53.2947\n",
      "Epoch [19240/50000], Train Loss: 37.2763, Test Loss: 50.0343\n",
      "Epoch [19245/50000], Train Loss: 40.4798, Test Loss: 56.3011\n",
      "Epoch [19250/50000], Train Loss: 34.5691, Test Loss: 48.8036\n",
      "Epoch [19255/50000], Train Loss: 33.6236, Test Loss: 49.6927\n",
      "Epoch [19260/50000], Train Loss: 30.6752, Test Loss: 48.3281\n",
      "Epoch [19265/50000], Train Loss: 37.7985, Test Loss: 48.2555\n",
      "Epoch [19270/50000], Train Loss: 43.2817, Test Loss: 47.8339\n",
      "Epoch [19275/50000], Train Loss: 64.0326, Test Loss: 50.0596\n",
      "Epoch [19280/50000], Train Loss: 47.6762, Test Loss: 56.2927\n",
      "Epoch [19285/50000], Train Loss: 31.6162, Test Loss: 48.7079\n",
      "Epoch [19290/50000], Train Loss: 33.7188, Test Loss: 52.6460\n",
      "Epoch [19295/50000], Train Loss: 37.3316, Test Loss: 50.8232\n",
      "Epoch [19300/50000], Train Loss: 26.9082, Test Loss: 48.8359\n",
      "Epoch [19305/50000], Train Loss: 50.8094, Test Loss: 48.6693\n",
      "Epoch [19310/50000], Train Loss: 40.0404, Test Loss: 54.8615\n",
      "Epoch [19315/50000], Train Loss: 46.9004, Test Loss: 53.2098\n",
      "Epoch [19320/50000], Train Loss: 49.0472, Test Loss: 51.0187\n",
      "Epoch [19325/50000], Train Loss: 35.7226, Test Loss: 51.3106\n",
      "Epoch [19330/50000], Train Loss: 39.4240, Test Loss: 50.4260\n",
      "Epoch [19335/50000], Train Loss: 38.1891, Test Loss: 52.7963\n",
      "Epoch [19340/50000], Train Loss: 32.0261, Test Loss: 50.1294\n",
      "Epoch [19345/50000], Train Loss: 67.2023, Test Loss: 52.6552\n",
      "Epoch [19350/50000], Train Loss: 41.6218, Test Loss: 70.6669\n",
      "Epoch [19355/50000], Train Loss: 60.5396, Test Loss: 48.5386\n",
      "Epoch [19360/50000], Train Loss: 44.6183, Test Loss: 62.3083\n",
      "Epoch [19365/50000], Train Loss: 28.9598, Test Loss: 49.3278\n",
      "Epoch [19370/50000], Train Loss: 43.7645, Test Loss: 51.1572\n",
      "Epoch [19375/50000], Train Loss: 23.6290, Test Loss: 46.6641\n",
      "Epoch [19380/50000], Train Loss: 40.6876, Test Loss: 50.5984\n",
      "Epoch [19385/50000], Train Loss: 40.9008, Test Loss: 51.2721\n",
      "Epoch [19390/50000], Train Loss: 28.1430, Test Loss: 47.3843\n",
      "Epoch [19395/50000], Train Loss: 40.7744, Test Loss: 50.0951\n",
      "Epoch [19400/50000], Train Loss: 38.3677, Test Loss: 48.9443\n",
      "Epoch [19405/50000], Train Loss: 42.9122, Test Loss: 49.2199\n",
      "Epoch [19410/50000], Train Loss: 31.5545, Test Loss: 54.8199\n",
      "Epoch [19415/50000], Train Loss: 24.9240, Test Loss: 49.0763\n",
      "Epoch [19420/50000], Train Loss: 40.2288, Test Loss: 50.1649\n",
      "Epoch [19425/50000], Train Loss: 41.1203, Test Loss: 50.5665\n",
      "Epoch [19430/50000], Train Loss: 32.2635, Test Loss: 53.7829\n",
      "Epoch [19435/50000], Train Loss: 33.4999, Test Loss: 50.5883\n",
      "Epoch [19440/50000], Train Loss: 35.7473, Test Loss: 48.1793\n",
      "Epoch [19445/50000], Train Loss: 38.8611, Test Loss: 52.3004\n",
      "Epoch [19450/50000], Train Loss: 34.0701, Test Loss: 49.6529\n",
      "Epoch [19455/50000], Train Loss: 45.4719, Test Loss: 65.4805\n",
      "Epoch [19460/50000], Train Loss: 40.3631, Test Loss: 49.5921\n",
      "Epoch [19465/50000], Train Loss: 34.3138, Test Loss: 48.5812\n",
      "Epoch [19470/50000], Train Loss: 34.0315, Test Loss: 49.2236\n",
      "Epoch [19475/50000], Train Loss: 35.7403, Test Loss: 51.0519\n",
      "Epoch [19480/50000], Train Loss: 60.5456, Test Loss: 57.6575\n",
      "Epoch [19485/50000], Train Loss: 33.3151, Test Loss: 49.7247\n",
      "Epoch [19490/50000], Train Loss: 35.1122, Test Loss: 57.5274\n",
      "Epoch [19495/50000], Train Loss: 35.8129, Test Loss: 47.6673\n",
      "Epoch [19500/50000], Train Loss: 34.0095, Test Loss: 49.3813\n",
      "Epoch [19505/50000], Train Loss: 38.7927, Test Loss: 49.3573\n",
      "Epoch [19510/50000], Train Loss: 37.3818, Test Loss: 49.4371\n",
      "Epoch [19515/50000], Train Loss: 34.4680, Test Loss: 47.4240\n",
      "Epoch [19520/50000], Train Loss: 39.2569, Test Loss: 57.1242\n",
      "Epoch [19525/50000], Train Loss: 36.7248, Test Loss: 48.4016\n",
      "Epoch [19530/50000], Train Loss: 30.0801, Test Loss: 46.3356\n",
      "Epoch [19535/50000], Train Loss: 26.1787, Test Loss: 48.0202\n",
      "Epoch [19540/50000], Train Loss: 35.6914, Test Loss: 63.5718\n",
      "Epoch [19545/50000], Train Loss: 47.2254, Test Loss: 49.0598\n",
      "Epoch [19550/50000], Train Loss: 38.2754, Test Loss: 52.3652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19555/50000], Train Loss: 36.4542, Test Loss: 56.5503\n",
      "Epoch [19560/50000], Train Loss: 41.0804, Test Loss: 53.9944\n",
      "Epoch [19565/50000], Train Loss: 28.2847, Test Loss: 49.7681\n",
      "Epoch [19570/50000], Train Loss: 55.9691, Test Loss: 103.6699\n",
      "Epoch [19575/50000], Train Loss: 41.1172, Test Loss: 47.2231\n",
      "Epoch [19580/50000], Train Loss: 38.1107, Test Loss: 49.3190\n",
      "Epoch [19585/50000], Train Loss: 35.0494, Test Loss: 49.2980\n",
      "Epoch [19590/50000], Train Loss: 35.6307, Test Loss: 49.3670\n",
      "Epoch [19595/50000], Train Loss: 34.8574, Test Loss: 51.1138\n",
      "Epoch [19600/50000], Train Loss: 40.7301, Test Loss: 54.3332\n",
      "Epoch [19605/50000], Train Loss: 34.3747, Test Loss: 51.3032\n",
      "Epoch [19610/50000], Train Loss: 32.8160, Test Loss: 49.1121\n",
      "Epoch [19615/50000], Train Loss: 34.2535, Test Loss: 49.8275\n",
      "Epoch [19620/50000], Train Loss: 37.3464, Test Loss: 50.0419\n",
      "Epoch [19625/50000], Train Loss: 37.4746, Test Loss: 49.2808\n",
      "Epoch [19630/50000], Train Loss: 41.6880, Test Loss: 49.3999\n",
      "Epoch [19635/50000], Train Loss: 35.3040, Test Loss: 52.1391\n",
      "Epoch [19640/50000], Train Loss: 34.8194, Test Loss: 48.0765\n",
      "Epoch [19645/50000], Train Loss: 22.8943, Test Loss: 47.1730\n",
      "Epoch [19650/50000], Train Loss: 33.6923, Test Loss: 53.4894\n",
      "Epoch [19655/50000], Train Loss: 34.2370, Test Loss: 47.9248\n",
      "Epoch [19660/50000], Train Loss: 34.9234, Test Loss: 47.5431\n",
      "Epoch [19665/50000], Train Loss: 34.9140, Test Loss: 49.9079\n",
      "Epoch [19670/50000], Train Loss: 32.0132, Test Loss: 52.4356\n",
      "Epoch [19675/50000], Train Loss: 20.5022, Test Loss: 47.5711\n",
      "Epoch [19680/50000], Train Loss: 32.4645, Test Loss: 46.9386\n",
      "Epoch [19685/50000], Train Loss: 36.2972, Test Loss: 49.2105\n",
      "Epoch [19690/50000], Train Loss: 38.7367, Test Loss: 48.5519\n",
      "Epoch [19695/50000], Train Loss: 36.9460, Test Loss: 49.8813\n",
      "Epoch [19700/50000], Train Loss: 31.4022, Test Loss: 48.9198\n",
      "Epoch [19705/50000], Train Loss: 33.3439, Test Loss: 48.6883\n",
      "Epoch [19710/50000], Train Loss: 32.2088, Test Loss: 49.2221\n",
      "Epoch [19715/50000], Train Loss: 39.2299, Test Loss: 48.2511\n",
      "Epoch [19720/50000], Train Loss: 59.2316, Test Loss: 67.7956\n",
      "Epoch [19725/50000], Train Loss: 30.2545, Test Loss: 50.6692\n",
      "Epoch [19730/50000], Train Loss: 41.0195, Test Loss: 49.1766\n",
      "Epoch [19735/50000], Train Loss: 28.3074, Test Loss: 48.1777\n",
      "Epoch [19740/50000], Train Loss: 32.6047, Test Loss: 53.8965\n",
      "Epoch [19745/50000], Train Loss: 39.0370, Test Loss: 49.7009\n",
      "Epoch [19750/50000], Train Loss: 33.0705, Test Loss: 47.3529\n",
      "Epoch [19755/50000], Train Loss: 29.6191, Test Loss: 52.1519\n",
      "Epoch [19760/50000], Train Loss: 33.8669, Test Loss: 50.6868\n",
      "Epoch [19765/50000], Train Loss: 38.4933, Test Loss: 67.1576\n",
      "Epoch [19770/50000], Train Loss: 35.9504, Test Loss: 56.5948\n",
      "Epoch [19775/50000], Train Loss: 35.8661, Test Loss: 57.0053\n",
      "Epoch [19780/50000], Train Loss: 37.3196, Test Loss: 52.9604\n",
      "Epoch [19785/50000], Train Loss: 52.9452, Test Loss: 49.9296\n",
      "Epoch [19790/50000], Train Loss: 32.3093, Test Loss: 47.5508\n",
      "Epoch [19795/50000], Train Loss: 33.7457, Test Loss: 49.0954\n",
      "Epoch [19800/50000], Train Loss: 33.4662, Test Loss: 55.3502\n",
      "Epoch [19805/50000], Train Loss: 30.3936, Test Loss: 50.1091\n",
      "Epoch [19810/50000], Train Loss: 45.6545, Test Loss: 47.9220\n",
      "Epoch [19815/50000], Train Loss: 27.3822, Test Loss: 50.6101\n",
      "Epoch [19820/50000], Train Loss: 38.3869, Test Loss: 51.3359\n",
      "Epoch [19825/50000], Train Loss: 36.0253, Test Loss: 54.0494\n",
      "Epoch [19830/50000], Train Loss: 36.3659, Test Loss: 52.2876\n",
      "Epoch [19835/50000], Train Loss: 33.9447, Test Loss: 47.8669\n",
      "Epoch [19840/50000], Train Loss: 27.5709, Test Loss: 50.1971\n",
      "Epoch [19845/50000], Train Loss: 32.9172, Test Loss: 47.7487\n",
      "Epoch [19850/50000], Train Loss: 37.9520, Test Loss: 60.6451\n",
      "Epoch [19855/50000], Train Loss: 34.5915, Test Loss: 52.8386\n",
      "Epoch [19860/50000], Train Loss: 32.8351, Test Loss: 58.0542\n",
      "Epoch [19865/50000], Train Loss: 27.9523, Test Loss: 50.3021\n",
      "Epoch [19870/50000], Train Loss: 58.4574, Test Loss: 54.0759\n",
      "Epoch [19875/50000], Train Loss: 30.0397, Test Loss: 48.6096\n",
      "Epoch [19880/50000], Train Loss: 39.6922, Test Loss: 49.1021\n",
      "Epoch [19885/50000], Train Loss: 37.3005, Test Loss: 52.9756\n",
      "Epoch [19890/50000], Train Loss: 34.1246, Test Loss: 52.9329\n",
      "Epoch [19895/50000], Train Loss: 30.9077, Test Loss: 47.8319\n",
      "Epoch [19900/50000], Train Loss: 38.3083, Test Loss: 49.7381\n",
      "Epoch [19905/50000], Train Loss: 27.2428, Test Loss: 54.9929\n",
      "Epoch [19910/50000], Train Loss: 42.8366, Test Loss: 50.0202\n",
      "Epoch [19915/50000], Train Loss: 31.0859, Test Loss: 48.9260\n",
      "Epoch [19920/50000], Train Loss: 30.3847, Test Loss: 47.4481\n",
      "Epoch [19925/50000], Train Loss: 27.1042, Test Loss: 47.5756\n",
      "Epoch [19930/50000], Train Loss: 34.7639, Test Loss: 48.3936\n",
      "Epoch [19935/50000], Train Loss: 35.6204, Test Loss: 57.8293\n",
      "Epoch [19940/50000], Train Loss: 31.8767, Test Loss: 50.8445\n",
      "Epoch [19945/50000], Train Loss: 55.5103, Test Loss: 48.3670\n",
      "Epoch [19950/50000], Train Loss: 39.0370, Test Loss: 55.8193\n",
      "Epoch [19955/50000], Train Loss: 33.1750, Test Loss: 50.9645\n",
      "Epoch [19960/50000], Train Loss: 38.5687, Test Loss: 52.3381\n",
      "Epoch [19965/50000], Train Loss: 42.0278, Test Loss: 49.5516\n",
      "Epoch [19970/50000], Train Loss: 47.0542, Test Loss: 57.4402\n",
      "Epoch [19975/50000], Train Loss: 49.0611, Test Loss: 47.7356\n",
      "Epoch [19980/50000], Train Loss: 33.9385, Test Loss: 49.7498\n",
      "Epoch [19985/50000], Train Loss: 36.9170, Test Loss: 47.7089\n",
      "Epoch [19990/50000], Train Loss: 29.7207, Test Loss: 50.1887\n",
      "Epoch [19995/50000], Train Loss: 35.6899, Test Loss: 54.7856\n",
      "Epoch [20000/50000], Train Loss: 41.9219, Test Loss: 46.4444\n",
      "Epoch [20005/50000], Train Loss: 61.0459, Test Loss: 77.8062\n",
      "Epoch [20010/50000], Train Loss: 34.3636, Test Loss: 50.9470\n",
      "Epoch [20015/50000], Train Loss: 24.7196, Test Loss: 47.3636\n",
      "Epoch [20020/50000], Train Loss: 35.1524, Test Loss: 50.0695\n",
      "Epoch [20025/50000], Train Loss: 43.2706, Test Loss: 48.0388\n",
      "Epoch [20030/50000], Train Loss: 28.7921, Test Loss: 51.4709\n",
      "Epoch [20035/50000], Train Loss: 32.4818, Test Loss: 52.1894\n",
      "Epoch [20040/50000], Train Loss: 38.1942, Test Loss: 63.9847\n",
      "Epoch [20045/50000], Train Loss: 38.5874, Test Loss: 50.0487\n",
      "Epoch [20050/50000], Train Loss: 48.6585, Test Loss: 48.9535\n",
      "Epoch [20055/50000], Train Loss: 33.0574, Test Loss: 58.9301\n",
      "Epoch [20060/50000], Train Loss: 81.2737, Test Loss: 78.5740\n",
      "Epoch [20065/50000], Train Loss: 24.2726, Test Loss: 50.8343\n",
      "Epoch [20070/50000], Train Loss: 31.7193, Test Loss: 47.6799\n",
      "Epoch [20075/50000], Train Loss: 57.0510, Test Loss: 47.9469\n",
      "Epoch [20080/50000], Train Loss: 50.2356, Test Loss: 47.1251\n",
      "Epoch [20085/50000], Train Loss: 36.8603, Test Loss: 58.6761\n",
      "Epoch [20090/50000], Train Loss: 34.9266, Test Loss: 49.8069\n",
      "Epoch [20095/50000], Train Loss: 55.6716, Test Loss: 66.1219\n",
      "Epoch [20100/50000], Train Loss: 35.2079, Test Loss: 50.2365\n",
      "Epoch [20105/50000], Train Loss: 36.3752, Test Loss: 48.7273\n",
      "Epoch [20110/50000], Train Loss: 23.9058, Test Loss: 47.7747\n",
      "Epoch [20115/50000], Train Loss: 35.2304, Test Loss: 50.3187\n",
      "Epoch [20120/50000], Train Loss: 31.9288, Test Loss: 47.9766\n",
      "Epoch [20125/50000], Train Loss: 35.9136, Test Loss: 55.9953\n",
      "Epoch [20130/50000], Train Loss: 37.9529, Test Loss: 52.8846\n",
      "Epoch [20135/50000], Train Loss: 67.3171, Test Loss: 48.2229\n",
      "Epoch [20140/50000], Train Loss: 40.7323, Test Loss: 50.3948\n",
      "Epoch [20145/50000], Train Loss: 35.1245, Test Loss: 48.4681\n",
      "Epoch [20150/50000], Train Loss: 36.8959, Test Loss: 49.4069\n",
      "Epoch [20155/50000], Train Loss: 58.0148, Test Loss: 49.7560\n",
      "Epoch [20160/50000], Train Loss: 46.7849, Test Loss: 52.0699\n",
      "Epoch [20165/50000], Train Loss: 57.5876, Test Loss: 67.4704\n",
      "Epoch [20170/50000], Train Loss: 32.1422, Test Loss: 47.9841\n",
      "Epoch [20175/50000], Train Loss: 33.4505, Test Loss: 51.8494\n",
      "Epoch [20180/50000], Train Loss: 33.8397, Test Loss: 59.5606\n",
      "Epoch [20185/50000], Train Loss: 35.7369, Test Loss: 56.0942\n",
      "Epoch [20190/50000], Train Loss: 33.0123, Test Loss: 49.5885\n",
      "Epoch [20195/50000], Train Loss: 38.3253, Test Loss: 49.2517\n",
      "Epoch [20200/50000], Train Loss: 35.6140, Test Loss: 48.7406\n",
      "Epoch [20205/50000], Train Loss: 37.4581, Test Loss: 55.8827\n",
      "Epoch [20210/50000], Train Loss: 33.4411, Test Loss: 54.5523\n",
      "Epoch [20215/50000], Train Loss: 33.1301, Test Loss: 52.7333\n",
      "Epoch [20220/50000], Train Loss: 40.8635, Test Loss: 88.4685\n",
      "Epoch [20225/50000], Train Loss: 35.7596, Test Loss: 52.9365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20230/50000], Train Loss: 33.3027, Test Loss: 54.1231\n",
      "Epoch [20235/50000], Train Loss: 89.2222, Test Loss: 58.9475\n",
      "Epoch [20240/50000], Train Loss: 36.0023, Test Loss: 50.5927\n",
      "Epoch [20245/50000], Train Loss: 34.3791, Test Loss: 49.6482\n",
      "Epoch [20250/50000], Train Loss: 35.4523, Test Loss: 52.7222\n",
      "Epoch [20255/50000], Train Loss: 31.3852, Test Loss: 52.7386\n",
      "Epoch [20260/50000], Train Loss: 34.2018, Test Loss: 51.4739\n",
      "Epoch [20265/50000], Train Loss: 33.0434, Test Loss: 47.0050\n",
      "Epoch [20270/50000], Train Loss: 41.4087, Test Loss: 49.7576\n",
      "Epoch [20275/50000], Train Loss: 28.8381, Test Loss: 50.9301\n",
      "Epoch [20280/50000], Train Loss: 35.6686, Test Loss: 49.2159\n",
      "Epoch [20285/50000], Train Loss: 45.9109, Test Loss: 49.5031\n",
      "Epoch [20290/50000], Train Loss: 40.3645, Test Loss: 58.8245\n",
      "Epoch [20295/50000], Train Loss: 26.4956, Test Loss: 48.4423\n",
      "Epoch [20300/50000], Train Loss: 32.2994, Test Loss: 53.8697\n",
      "Epoch [20305/50000], Train Loss: 39.6424, Test Loss: 55.5674\n",
      "Epoch [20310/50000], Train Loss: 36.8680, Test Loss: 51.6794\n",
      "Epoch [20315/50000], Train Loss: 31.7253, Test Loss: 49.0022\n",
      "Epoch [20320/50000], Train Loss: 31.7006, Test Loss: 50.9718\n",
      "Epoch [20325/50000], Train Loss: 29.4432, Test Loss: 50.8535\n",
      "Epoch [20330/50000], Train Loss: 43.0768, Test Loss: 80.7892\n",
      "Epoch [20335/50000], Train Loss: 30.2733, Test Loss: 47.2699\n",
      "Epoch [20340/50000], Train Loss: 36.4078, Test Loss: 49.9695\n",
      "Epoch [20345/50000], Train Loss: 32.3489, Test Loss: 49.3823\n",
      "Epoch [20350/50000], Train Loss: 137.2545, Test Loss: 61.3240\n",
      "Epoch [20355/50000], Train Loss: 35.3372, Test Loss: 46.4372\n",
      "Epoch [20360/50000], Train Loss: 33.4666, Test Loss: 46.8042\n",
      "Epoch [20365/50000], Train Loss: 36.0762, Test Loss: 50.0308\n",
      "Epoch [20370/50000], Train Loss: 39.1046, Test Loss: 49.1027\n",
      "Epoch [20375/50000], Train Loss: 37.3173, Test Loss: 52.6809\n",
      "Epoch [20380/50000], Train Loss: 52.7857, Test Loss: 50.4670\n",
      "Epoch [20385/50000], Train Loss: 30.4920, Test Loss: 48.5801\n",
      "Epoch [20390/50000], Train Loss: 43.4515, Test Loss: 59.4445\n",
      "Epoch [20395/50000], Train Loss: 41.2708, Test Loss: 60.8088\n",
      "Epoch [20400/50000], Train Loss: 38.1472, Test Loss: 49.2877\n",
      "Epoch [20405/50000], Train Loss: 23.7704, Test Loss: 50.3449\n",
      "Epoch [20410/50000], Train Loss: 49.0557, Test Loss: 51.4318\n",
      "Epoch [20415/50000], Train Loss: 34.5790, Test Loss: 47.6013\n",
      "Epoch [20420/50000], Train Loss: 37.3777, Test Loss: 48.3516\n",
      "Epoch [20425/50000], Train Loss: 26.3339, Test Loss: 47.9067\n",
      "Epoch [20430/50000], Train Loss: 48.5345, Test Loss: 49.2016\n",
      "Epoch [20435/50000], Train Loss: 37.8107, Test Loss: 50.5549\n",
      "Epoch [20440/50000], Train Loss: 34.6101, Test Loss: 53.0989\n",
      "Epoch [20445/50000], Train Loss: 32.7776, Test Loss: 49.8189\n",
      "Epoch [20450/50000], Train Loss: 33.3633, Test Loss: 48.3545\n",
      "Epoch [20455/50000], Train Loss: 41.8276, Test Loss: 48.5459\n",
      "Epoch [20460/50000], Train Loss: 33.7388, Test Loss: 47.6839\n",
      "Epoch [20465/50000], Train Loss: 32.2253, Test Loss: 50.8361\n",
      "Epoch [20470/50000], Train Loss: 40.8575, Test Loss: 48.1540\n",
      "Epoch [20475/50000], Train Loss: 37.2008, Test Loss: 54.9885\n",
      "Epoch [20480/50000], Train Loss: 26.4807, Test Loss: 49.2913\n",
      "Epoch [20485/50000], Train Loss: 36.5239, Test Loss: 53.7228\n",
      "Epoch [20490/50000], Train Loss: 28.0476, Test Loss: 48.6367\n",
      "Epoch [20495/50000], Train Loss: 70.2625, Test Loss: 49.7873\n",
      "Epoch [20500/50000], Train Loss: 36.9636, Test Loss: 50.9366\n",
      "Epoch [20505/50000], Train Loss: 39.5483, Test Loss: 55.1766\n",
      "Epoch [20510/50000], Train Loss: 69.9445, Test Loss: 57.6304\n",
      "Epoch [20515/50000], Train Loss: 37.7547, Test Loss: 57.6010\n",
      "Epoch [20520/50000], Train Loss: 28.3167, Test Loss: 48.8159\n",
      "Epoch [20525/50000], Train Loss: 39.2355, Test Loss: 48.8631\n",
      "Epoch [20530/50000], Train Loss: 25.8463, Test Loss: 48.1141\n",
      "Epoch [20535/50000], Train Loss: 46.3938, Test Loss: 61.1163\n",
      "Epoch [20540/50000], Train Loss: 38.1608, Test Loss: 56.5252\n",
      "Epoch [20545/50000], Train Loss: 33.4128, Test Loss: 48.7715\n",
      "Epoch [20550/50000], Train Loss: 37.8885, Test Loss: 57.3440\n",
      "Epoch [20555/50000], Train Loss: 41.2011, Test Loss: 47.9143\n",
      "Epoch [20560/50000], Train Loss: 35.4845, Test Loss: 49.4333\n",
      "Epoch [20565/50000], Train Loss: 58.0983, Test Loss: 49.7507\n",
      "Epoch [20570/50000], Train Loss: 31.0891, Test Loss: 47.1300\n",
      "Epoch [20575/50000], Train Loss: 30.3970, Test Loss: 46.8045\n",
      "Epoch [20580/50000], Train Loss: 33.1665, Test Loss: 48.9576\n",
      "Epoch [20585/50000], Train Loss: 31.6375, Test Loss: 52.5794\n",
      "Epoch [20590/50000], Train Loss: 44.3253, Test Loss: 50.9497\n",
      "Epoch [20595/50000], Train Loss: 38.3738, Test Loss: 52.2384\n",
      "Epoch [20600/50000], Train Loss: 27.4118, Test Loss: 47.1802\n",
      "Epoch [20605/50000], Train Loss: 35.4231, Test Loss: 50.4558\n",
      "Epoch [20610/50000], Train Loss: 31.4468, Test Loss: 50.8933\n",
      "Epoch [20615/50000], Train Loss: 31.2218, Test Loss: 46.9741\n",
      "Epoch [20620/50000], Train Loss: 32.8478, Test Loss: 64.0763\n",
      "Epoch [20625/50000], Train Loss: 37.8618, Test Loss: 55.8332\n",
      "Epoch [20630/50000], Train Loss: 36.1259, Test Loss: 49.9269\n",
      "Epoch [20635/50000], Train Loss: 36.6423, Test Loss: 47.9805\n",
      "Epoch [20640/50000], Train Loss: 35.0268, Test Loss: 50.9843\n",
      "Epoch [20645/50000], Train Loss: 31.6922, Test Loss: 50.2869\n",
      "Epoch [20650/50000], Train Loss: 35.7891, Test Loss: 51.0490\n",
      "Epoch [20655/50000], Train Loss: 33.7392, Test Loss: 47.6712\n",
      "Epoch [20660/50000], Train Loss: 37.7757, Test Loss: 52.2557\n",
      "Epoch [20665/50000], Train Loss: 51.6525, Test Loss: 49.2194\n",
      "Epoch [20670/50000], Train Loss: 73.3167, Test Loss: 48.8429\n",
      "Epoch [20675/50000], Train Loss: 30.4046, Test Loss: 47.9247\n",
      "Epoch [20680/50000], Train Loss: 39.7402, Test Loss: 49.3024\n",
      "Epoch [20685/50000], Train Loss: 27.3170, Test Loss: 52.7798\n",
      "Epoch [20690/50000], Train Loss: 33.8430, Test Loss: 48.9262\n",
      "Epoch [20695/50000], Train Loss: 35.4839, Test Loss: 60.1196\n",
      "Epoch [20700/50000], Train Loss: 28.9916, Test Loss: 48.1817\n",
      "Epoch [20705/50000], Train Loss: 50.8298, Test Loss: 51.2868\n",
      "Epoch [20710/50000], Train Loss: 69.6025, Test Loss: 46.8514\n",
      "Epoch [20715/50000], Train Loss: 31.9194, Test Loss: 53.6624\n",
      "Epoch [20720/50000], Train Loss: 42.2200, Test Loss: 54.4092\n",
      "Epoch [20725/50000], Train Loss: 34.0462, Test Loss: 52.3461\n",
      "Epoch [20730/50000], Train Loss: 37.8961, Test Loss: 50.9767\n",
      "Epoch [20735/50000], Train Loss: 46.1414, Test Loss: 51.2901\n",
      "Epoch [20740/50000], Train Loss: 34.9711, Test Loss: 49.4354\n",
      "Epoch [20745/50000], Train Loss: 44.6395, Test Loss: 53.8048\n",
      "Epoch [20750/50000], Train Loss: 30.4169, Test Loss: 52.9022\n",
      "Epoch [20755/50000], Train Loss: 36.5262, Test Loss: 49.4376\n",
      "Epoch [20760/50000], Train Loss: 27.9310, Test Loss: 48.1840\n",
      "Epoch [20765/50000], Train Loss: 30.1822, Test Loss: 50.6003\n",
      "Epoch [20770/50000], Train Loss: 49.6168, Test Loss: 54.6180\n",
      "Epoch [20775/50000], Train Loss: 31.1418, Test Loss: 49.9528\n",
      "Epoch [20780/50000], Train Loss: 37.5039, Test Loss: 48.2467\n",
      "Epoch [20785/50000], Train Loss: 37.1843, Test Loss: 65.0871\n",
      "Epoch [20790/50000], Train Loss: 29.4454, Test Loss: 49.1308\n",
      "Epoch [20795/50000], Train Loss: 37.8435, Test Loss: 49.2777\n",
      "Epoch [20800/50000], Train Loss: 66.1729, Test Loss: 48.5256\n",
      "Epoch [20805/50000], Train Loss: 28.7946, Test Loss: 54.9594\n",
      "Epoch [20810/50000], Train Loss: 39.7690, Test Loss: 48.7959\n",
      "Epoch [20815/50000], Train Loss: 29.9418, Test Loss: 47.2234\n",
      "Epoch [20820/50000], Train Loss: 29.1177, Test Loss: 52.6572\n",
      "Epoch [20825/50000], Train Loss: 42.8716, Test Loss: 51.6620\n",
      "Epoch [20830/50000], Train Loss: 22.3065, Test Loss: 46.5761\n",
      "Epoch [20835/50000], Train Loss: 30.1796, Test Loss: 53.5631\n",
      "Epoch [20840/50000], Train Loss: 35.7680, Test Loss: 51.4857\n",
      "Epoch [20845/50000], Train Loss: 64.1607, Test Loss: 48.2565\n",
      "Epoch [20850/50000], Train Loss: 37.1566, Test Loss: 49.7067\n",
      "Epoch [20855/50000], Train Loss: 25.3517, Test Loss: 46.6942\n",
      "Epoch [20860/50000], Train Loss: 29.2295, Test Loss: 48.9213\n",
      "Epoch [20865/50000], Train Loss: 34.0444, Test Loss: 55.8534\n",
      "Epoch [20870/50000], Train Loss: 33.6379, Test Loss: 48.5076\n",
      "Epoch [20875/50000], Train Loss: 21.8009, Test Loss: 47.5156\n",
      "Epoch [20880/50000], Train Loss: 40.0321, Test Loss: 47.0959\n",
      "Epoch [20885/50000], Train Loss: 49.7244, Test Loss: 212.8256\n",
      "Epoch [20890/50000], Train Loss: 36.2062, Test Loss: 52.7710\n",
      "Epoch [20895/50000], Train Loss: 31.1708, Test Loss: 50.1192\n",
      "Epoch [20900/50000], Train Loss: 41.6962, Test Loss: 53.8186\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20905/50000], Train Loss: 33.8217, Test Loss: 50.1948\n",
      "Epoch [20910/50000], Train Loss: 38.0837, Test Loss: 52.0998\n",
      "Epoch [20915/50000], Train Loss: 99.5549, Test Loss: 47.6185\n",
      "Epoch [20920/50000], Train Loss: 37.9741, Test Loss: 55.9904\n",
      "Epoch [20925/50000], Train Loss: 39.0248, Test Loss: 48.2248\n",
      "Epoch [20930/50000], Train Loss: 34.7473, Test Loss: 67.0671\n",
      "Epoch [20935/50000], Train Loss: 33.5307, Test Loss: 55.6386\n",
      "Epoch [20940/50000], Train Loss: 53.8674, Test Loss: 55.7313\n",
      "Epoch [20945/50000], Train Loss: 40.3009, Test Loss: 50.2900\n",
      "Epoch [20950/50000], Train Loss: 53.0373, Test Loss: 48.0456\n",
      "Epoch [20955/50000], Train Loss: 31.2560, Test Loss: 47.7946\n",
      "Epoch [20960/50000], Train Loss: 40.3556, Test Loss: 47.3550\n",
      "Epoch [20965/50000], Train Loss: 35.1998, Test Loss: 47.3931\n",
      "Epoch [20970/50000], Train Loss: 32.6214, Test Loss: 47.1295\n",
      "Epoch [20975/50000], Train Loss: 93.0463, Test Loss: 48.3322\n",
      "Epoch [20980/50000], Train Loss: 31.2124, Test Loss: 48.6744\n",
      "Epoch [20985/50000], Train Loss: 24.4689, Test Loss: 48.6885\n",
      "Epoch [20990/50000], Train Loss: 34.3741, Test Loss: 49.2865\n",
      "Epoch [20995/50000], Train Loss: 31.5506, Test Loss: 50.4736\n",
      "Epoch [21000/50000], Train Loss: 34.4118, Test Loss: 51.7577\n",
      "Epoch [21005/50000], Train Loss: 33.3399, Test Loss: 51.1579\n",
      "Epoch [21010/50000], Train Loss: 34.8684, Test Loss: 52.5082\n",
      "Epoch [21015/50000], Train Loss: 29.6248, Test Loss: 52.7435\n",
      "Epoch [21020/50000], Train Loss: 32.5209, Test Loss: 50.7664\n",
      "Epoch [21025/50000], Train Loss: 63.5420, Test Loss: 45.7253\n",
      "Epoch [21030/50000], Train Loss: 28.5956, Test Loss: 50.9412\n",
      "Epoch [21035/50000], Train Loss: 30.3842, Test Loss: 48.8836\n",
      "Epoch [21040/50000], Train Loss: 31.2855, Test Loss: 53.9563\n",
      "Epoch [21045/50000], Train Loss: 34.9944, Test Loss: 55.8532\n",
      "Epoch [21050/50000], Train Loss: 34.1673, Test Loss: 48.6220\n",
      "Epoch [21055/50000], Train Loss: 34.8749, Test Loss: 49.1143\n",
      "Epoch [21060/50000], Train Loss: 34.9304, Test Loss: 50.2926\n",
      "Epoch [21065/50000], Train Loss: 58.7204, Test Loss: 46.7009\n",
      "Epoch [21070/50000], Train Loss: 34.8575, Test Loss: 51.8775\n",
      "Epoch [21075/50000], Train Loss: 36.0407, Test Loss: 49.4016\n",
      "Epoch [21080/50000], Train Loss: 36.1727, Test Loss: 50.9284\n",
      "Epoch [21085/50000], Train Loss: 36.1205, Test Loss: 51.1807\n",
      "Epoch [21090/50000], Train Loss: 38.7936, Test Loss: 87.7950\n",
      "Epoch [21095/50000], Train Loss: 31.2578, Test Loss: 47.6244\n",
      "Epoch [21100/50000], Train Loss: 30.9645, Test Loss: 51.8644\n",
      "Epoch [21105/50000], Train Loss: 38.7873, Test Loss: 47.8388\n",
      "Epoch [21110/50000], Train Loss: 33.4802, Test Loss: 48.4522\n",
      "Epoch [21115/50000], Train Loss: 47.7924, Test Loss: 58.3809\n",
      "Epoch [21120/50000], Train Loss: 33.8090, Test Loss: 49.3770\n",
      "Epoch [21125/50000], Train Loss: 46.0130, Test Loss: 54.1131\n",
      "Epoch [21130/50000], Train Loss: 47.6409, Test Loss: 51.5313\n",
      "Epoch [21135/50000], Train Loss: 26.5945, Test Loss: 49.3828\n",
      "Epoch [21140/50000], Train Loss: 36.9754, Test Loss: 49.2061\n",
      "Epoch [21145/50000], Train Loss: 34.9825, Test Loss: 49.4823\n",
      "Epoch [21150/50000], Train Loss: 32.0642, Test Loss: 52.5614\n",
      "Epoch [21155/50000], Train Loss: 33.5821, Test Loss: 47.5849\n",
      "Epoch [21160/50000], Train Loss: 33.8098, Test Loss: 51.6757\n",
      "Epoch [21165/50000], Train Loss: 29.9613, Test Loss: 48.6996\n",
      "Epoch [21170/50000], Train Loss: 30.0749, Test Loss: 49.5611\n",
      "Epoch [21175/50000], Train Loss: 36.1552, Test Loss: 47.6131\n",
      "Epoch [21180/50000], Train Loss: 35.0726, Test Loss: 48.4228\n",
      "Epoch [21185/50000], Train Loss: 43.3874, Test Loss: 47.5419\n",
      "Epoch [21190/50000], Train Loss: 43.9613, Test Loss: 53.5872\n",
      "Epoch [21195/50000], Train Loss: 38.0625, Test Loss: 46.9877\n",
      "Epoch [21200/50000], Train Loss: 34.8292, Test Loss: 47.9152\n",
      "Epoch [21205/50000], Train Loss: 24.8820, Test Loss: 52.4832\n",
      "Epoch [21210/50000], Train Loss: 33.3688, Test Loss: 52.7229\n",
      "Epoch [21215/50000], Train Loss: 30.6970, Test Loss: 46.7327\n",
      "Epoch [21220/50000], Train Loss: 34.1982, Test Loss: 51.4838\n",
      "Epoch [21225/50000], Train Loss: 54.5170, Test Loss: 46.9939\n",
      "Epoch [21230/50000], Train Loss: 55.6631, Test Loss: 47.5241\n",
      "Epoch [21235/50000], Train Loss: 28.1394, Test Loss: 49.0074\n",
      "Epoch [21240/50000], Train Loss: 31.6749, Test Loss: 48.7904\n",
      "Epoch [21245/50000], Train Loss: 32.6830, Test Loss: 49.7233\n",
      "Epoch [21250/50000], Train Loss: 42.1256, Test Loss: 51.4770\n",
      "Epoch [21255/50000], Train Loss: 36.0617, Test Loss: 50.3625\n",
      "Epoch [21260/50000], Train Loss: 41.7524, Test Loss: 46.1713\n",
      "Epoch [21265/50000], Train Loss: 42.5684, Test Loss: 47.5861\n",
      "Epoch [21270/50000], Train Loss: 37.4111, Test Loss: 51.3523\n",
      "Epoch [21275/50000], Train Loss: 40.1696, Test Loss: 54.2730\n",
      "Epoch [21280/50000], Train Loss: 29.1023, Test Loss: 46.2452\n",
      "Epoch [21285/50000], Train Loss: 31.7687, Test Loss: 51.4122\n",
      "Epoch [21290/50000], Train Loss: 43.3256, Test Loss: 55.0540\n",
      "Epoch [21295/50000], Train Loss: 38.7216, Test Loss: 49.7727\n",
      "Epoch [21300/50000], Train Loss: 27.9735, Test Loss: 48.5385\n",
      "Epoch [21305/50000], Train Loss: 37.5442, Test Loss: 47.3160\n",
      "Epoch [21310/50000], Train Loss: 36.8664, Test Loss: 51.2326\n",
      "Epoch [21315/50000], Train Loss: 39.2138, Test Loss: 53.5345\n",
      "Epoch [21320/50000], Train Loss: 35.6250, Test Loss: 49.0466\n",
      "Epoch [21325/50000], Train Loss: 35.5002, Test Loss: 49.5483\n",
      "Epoch [21330/50000], Train Loss: 30.4661, Test Loss: 46.0489\n",
      "Epoch [21335/50000], Train Loss: 31.0998, Test Loss: 47.3941\n",
      "Epoch [21340/50000], Train Loss: 30.0049, Test Loss: 65.2000\n",
      "Epoch [21345/50000], Train Loss: 30.7566, Test Loss: 49.7817\n",
      "Epoch [21350/50000], Train Loss: 32.1443, Test Loss: 51.5555\n",
      "Epoch [21355/50000], Train Loss: 39.4914, Test Loss: 48.5346\n",
      "Epoch [21360/50000], Train Loss: 31.1556, Test Loss: 52.8557\n",
      "Epoch [21365/50000], Train Loss: 49.1498, Test Loss: 50.3233\n",
      "Epoch [21370/50000], Train Loss: 43.8167, Test Loss: 90.7368\n",
      "Epoch [21375/50000], Train Loss: 32.9489, Test Loss: 55.2693\n",
      "Epoch [21380/50000], Train Loss: 45.5659, Test Loss: 54.2663\n",
      "Epoch [21385/50000], Train Loss: 37.7479, Test Loss: 51.2356\n",
      "Epoch [21390/50000], Train Loss: 34.4070, Test Loss: 52.1164\n",
      "Epoch [21395/50000], Train Loss: 42.7432, Test Loss: 46.7199\n",
      "Epoch [21400/50000], Train Loss: 34.6529, Test Loss: 52.7517\n",
      "Epoch [21405/50000], Train Loss: 43.8726, Test Loss: 46.6075\n",
      "Epoch [21410/50000], Train Loss: 33.5177, Test Loss: 49.6614\n",
      "Epoch [21415/50000], Train Loss: 41.7636, Test Loss: 57.8953\n",
      "Epoch [21420/50000], Train Loss: 55.9127, Test Loss: 47.6582\n",
      "Epoch [21425/50000], Train Loss: 33.2785, Test Loss: 51.6123\n",
      "Epoch [21430/50000], Train Loss: 29.4339, Test Loss: 47.6619\n",
      "Epoch [21435/50000], Train Loss: 92.1987, Test Loss: 46.9362\n",
      "Epoch [21440/50000], Train Loss: 35.5185, Test Loss: 49.1560\n",
      "Epoch [21445/50000], Train Loss: 31.4381, Test Loss: 57.5035\n",
      "Epoch [21450/50000], Train Loss: 33.6661, Test Loss: 53.6293\n",
      "Epoch [21455/50000], Train Loss: 32.9208, Test Loss: 60.0241\n",
      "Epoch [21460/50000], Train Loss: 31.3376, Test Loss: 52.0331\n",
      "Epoch [21465/50000], Train Loss: 40.8082, Test Loss: 46.8008\n",
      "Epoch [21470/50000], Train Loss: 36.2339, Test Loss: 49.7263\n",
      "Epoch [21475/50000], Train Loss: 25.6013, Test Loss: 50.6859\n",
      "Epoch [21480/50000], Train Loss: 34.0470, Test Loss: 48.8501\n",
      "Epoch [21485/50000], Train Loss: 35.8673, Test Loss: 47.3526\n",
      "Epoch [21490/50000], Train Loss: 32.6024, Test Loss: 53.2752\n",
      "Epoch [21495/50000], Train Loss: 33.0335, Test Loss: 48.3089\n",
      "Epoch [21500/50000], Train Loss: 31.8040, Test Loss: 54.2592\n",
      "Epoch [21505/50000], Train Loss: 40.8553, Test Loss: 56.4193\n",
      "Epoch [21510/50000], Train Loss: 44.4456, Test Loss: 109.0669\n",
      "Epoch [21515/50000], Train Loss: 35.5731, Test Loss: 50.2581\n",
      "Epoch [21520/50000], Train Loss: 29.8909, Test Loss: 52.0141\n",
      "Epoch [21525/50000], Train Loss: 34.5442, Test Loss: 47.0919\n",
      "Epoch [21530/50000], Train Loss: 49.9614, Test Loss: 47.6560\n",
      "Epoch [21535/50000], Train Loss: 22.4665, Test Loss: 47.3514\n",
      "Epoch [21540/50000], Train Loss: 31.9288, Test Loss: 47.2319\n",
      "Epoch [21545/50000], Train Loss: 34.8531, Test Loss: 52.7746\n",
      "Epoch [21550/50000], Train Loss: 40.4706, Test Loss: 55.9335\n",
      "Epoch [21555/50000], Train Loss: 36.4078, Test Loss: 49.4960\n",
      "Epoch [21560/50000], Train Loss: 37.7079, Test Loss: 49.4818\n",
      "Epoch [21565/50000], Train Loss: 23.8843, Test Loss: 49.4314\n",
      "Epoch [21570/50000], Train Loss: 33.4818, Test Loss: 49.9899\n",
      "Epoch [21575/50000], Train Loss: 43.5023, Test Loss: 72.6848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21580/50000], Train Loss: 30.6846, Test Loss: 51.5540\n",
      "Epoch [21585/50000], Train Loss: 102.8941, Test Loss: 45.6449\n",
      "Epoch [21590/50000], Train Loss: 33.8039, Test Loss: 47.7215\n",
      "Epoch [21595/50000], Train Loss: 33.2291, Test Loss: 50.9655\n",
      "Epoch [21600/50000], Train Loss: 41.3245, Test Loss: 48.5519\n",
      "Epoch [21605/50000], Train Loss: 47.0941, Test Loss: 49.9640\n",
      "Epoch [21610/50000], Train Loss: 35.7148, Test Loss: 47.9339\n",
      "Epoch [21615/50000], Train Loss: 92.8695, Test Loss: 58.3291\n",
      "Epoch [21620/50000], Train Loss: 39.1997, Test Loss: 54.8960\n",
      "Epoch [21625/50000], Train Loss: 29.4886, Test Loss: 47.2428\n",
      "Epoch [21630/50000], Train Loss: 31.6241, Test Loss: 47.9446\n",
      "Epoch [21635/50000], Train Loss: 30.1906, Test Loss: 47.3587\n",
      "Epoch [21640/50000], Train Loss: 39.2909, Test Loss: 55.5355\n",
      "Epoch [21645/50000], Train Loss: 24.6504, Test Loss: 46.3182\n",
      "Epoch [21650/50000], Train Loss: 33.6617, Test Loss: 52.0634\n",
      "Epoch [21655/50000], Train Loss: 31.1128, Test Loss: 49.0978\n",
      "Epoch [21660/50000], Train Loss: 26.4391, Test Loss: 47.2241\n",
      "Epoch [21665/50000], Train Loss: 34.9043, Test Loss: 51.5159\n",
      "Epoch [21670/50000], Train Loss: 39.1278, Test Loss: 49.2596\n",
      "Epoch [21675/50000], Train Loss: 38.1601, Test Loss: 48.2249\n",
      "Epoch [21680/50000], Train Loss: 30.4396, Test Loss: 46.6281\n",
      "Epoch [21685/50000], Train Loss: 44.2216, Test Loss: 48.0370\n",
      "Epoch [21690/50000], Train Loss: 35.9176, Test Loss: 49.3291\n",
      "Epoch [21695/50000], Train Loss: 30.9296, Test Loss: 48.5494\n",
      "Epoch [21700/50000], Train Loss: 34.7143, Test Loss: 50.7599\n",
      "Epoch [21705/50000], Train Loss: 34.3933, Test Loss: 53.6919\n",
      "Epoch [21710/50000], Train Loss: 31.0123, Test Loss: 49.3995\n",
      "Epoch [21715/50000], Train Loss: 26.0912, Test Loss: 49.3430\n",
      "Epoch [21720/50000], Train Loss: 74.0072, Test Loss: 47.2907\n",
      "Epoch [21725/50000], Train Loss: 26.3566, Test Loss: 50.4785\n",
      "Epoch [21730/50000], Train Loss: 28.4056, Test Loss: 46.5858\n",
      "Epoch [21735/50000], Train Loss: 34.3322, Test Loss: 45.8732\n",
      "Epoch [21740/50000], Train Loss: 27.8188, Test Loss: 52.2870\n",
      "Epoch [21745/50000], Train Loss: 36.1828, Test Loss: 55.5499\n",
      "Epoch [21750/50000], Train Loss: 22.5387, Test Loss: 52.4360\n",
      "Epoch [21755/50000], Train Loss: 41.4086, Test Loss: 47.2190\n",
      "Epoch [21760/50000], Train Loss: 43.5126, Test Loss: 83.2070\n",
      "Epoch [21765/50000], Train Loss: 28.0176, Test Loss: 54.1117\n",
      "Epoch [21770/50000], Train Loss: 36.9660, Test Loss: 54.1025\n",
      "Epoch [21775/50000], Train Loss: 30.5741, Test Loss: 51.3876\n",
      "Epoch [21780/50000], Train Loss: 63.3471, Test Loss: 46.8888\n",
      "Epoch [21785/50000], Train Loss: 32.0432, Test Loss: 51.2225\n",
      "Epoch [21790/50000], Train Loss: 32.3723, Test Loss: 53.4986\n",
      "Epoch [21795/50000], Train Loss: 26.8603, Test Loss: 48.2889\n",
      "Epoch [21800/50000], Train Loss: 34.0375, Test Loss: 50.0708\n",
      "Epoch [21805/50000], Train Loss: 34.7906, Test Loss: 49.6691\n",
      "Epoch [21810/50000], Train Loss: 19.8085, Test Loss: 46.4189\n",
      "Epoch [21815/50000], Train Loss: 40.2371, Test Loss: 51.3562\n",
      "Epoch [21820/50000], Train Loss: 33.5549, Test Loss: 48.5703\n",
      "Epoch [21825/50000], Train Loss: 27.2494, Test Loss: 48.1593\n",
      "Epoch [21830/50000], Train Loss: 30.9926, Test Loss: 45.5249\n",
      "Epoch [21835/50000], Train Loss: 38.3020, Test Loss: 49.4855\n",
      "Epoch [21840/50000], Train Loss: 27.6367, Test Loss: 49.5264\n",
      "Epoch [21845/50000], Train Loss: 37.3232, Test Loss: 52.0489\n",
      "Epoch [21850/50000], Train Loss: 23.8293, Test Loss: 49.3068\n",
      "Epoch [21855/50000], Train Loss: 32.6737, Test Loss: 48.3980\n",
      "Epoch [21860/50000], Train Loss: 34.7655, Test Loss: 49.1811\n",
      "Epoch [21865/50000], Train Loss: 29.5021, Test Loss: 48.7026\n",
      "Epoch [21870/50000], Train Loss: 22.7664, Test Loss: 51.2898\n",
      "Epoch [21875/50000], Train Loss: 36.4237, Test Loss: 51.5049\n",
      "Epoch [21880/50000], Train Loss: 31.4141, Test Loss: 50.9275\n",
      "Epoch [21885/50000], Train Loss: 32.2320, Test Loss: 55.3868\n",
      "Epoch [21890/50000], Train Loss: 31.3959, Test Loss: 47.4325\n",
      "Epoch [21895/50000], Train Loss: 27.8467, Test Loss: 49.7034\n",
      "Epoch [21900/50000], Train Loss: 43.5391, Test Loss: 52.9844\n",
      "Epoch [21905/50000], Train Loss: 25.3343, Test Loss: 47.1597\n",
      "Epoch [21910/50000], Train Loss: 35.0760, Test Loss: 50.1956\n",
      "Epoch [21915/50000], Train Loss: 28.4378, Test Loss: 53.6141\n",
      "Epoch [21920/50000], Train Loss: 34.3623, Test Loss: 56.3878\n",
      "Epoch [21925/50000], Train Loss: 36.1813, Test Loss: 48.2604\n",
      "Epoch [21930/50000], Train Loss: 37.8918, Test Loss: 60.0831\n",
      "Epoch [21935/50000], Train Loss: 43.1673, Test Loss: 47.7509\n",
      "Epoch [21940/50000], Train Loss: 35.4769, Test Loss: 48.4988\n",
      "Epoch [21945/50000], Train Loss: 31.8499, Test Loss: 66.9616\n",
      "Epoch [21950/50000], Train Loss: 36.7694, Test Loss: 47.6674\n",
      "Epoch [21955/50000], Train Loss: 41.0759, Test Loss: 59.8987\n",
      "Epoch [21960/50000], Train Loss: 32.2738, Test Loss: 49.6805\n",
      "Epoch [21965/50000], Train Loss: 72.3020, Test Loss: 46.0673\n",
      "Epoch [21970/50000], Train Loss: 35.8559, Test Loss: 57.9364\n",
      "Epoch [21975/50000], Train Loss: 38.9892, Test Loss: 50.1590\n",
      "Epoch [21980/50000], Train Loss: 32.1867, Test Loss: 49.2551\n",
      "Epoch [21985/50000], Train Loss: 35.8650, Test Loss: 53.9773\n",
      "Epoch [21990/50000], Train Loss: 35.6290, Test Loss: 56.5184\n",
      "Epoch [21995/50000], Train Loss: 24.8055, Test Loss: 45.6229\n",
      "Epoch [22000/50000], Train Loss: 27.9138, Test Loss: 47.1663\n",
      "Epoch [22005/50000], Train Loss: 39.8757, Test Loss: 51.8479\n",
      "Epoch [22010/50000], Train Loss: 21.7208, Test Loss: 50.1640\n",
      "Epoch [22015/50000], Train Loss: 33.4015, Test Loss: 48.1299\n",
      "Epoch [22020/50000], Train Loss: 61.0224, Test Loss: 46.8807\n",
      "Epoch [22025/50000], Train Loss: 37.2466, Test Loss: 48.7743\n",
      "Epoch [22030/50000], Train Loss: 41.8279, Test Loss: 47.0021\n",
      "Epoch [22035/50000], Train Loss: 44.9130, Test Loss: 46.0407\n",
      "Epoch [22040/50000], Train Loss: 39.4863, Test Loss: 48.5243\n",
      "Epoch [22045/50000], Train Loss: 49.2326, Test Loss: 45.6166\n",
      "Epoch [22050/50000], Train Loss: 50.5635, Test Loss: 46.7921\n",
      "Epoch [22055/50000], Train Loss: 34.2539, Test Loss: 47.8364\n",
      "Epoch [22060/50000], Train Loss: 29.9621, Test Loss: 52.4493\n",
      "Epoch [22065/50000], Train Loss: 28.8083, Test Loss: 45.8414\n",
      "Epoch [22070/50000], Train Loss: 29.6352, Test Loss: 47.1705\n",
      "Epoch [22075/50000], Train Loss: 33.8303, Test Loss: 47.0841\n",
      "Epoch [22080/50000], Train Loss: 35.0742, Test Loss: 51.1742\n",
      "Epoch [22085/50000], Train Loss: 37.2040, Test Loss: 87.4781\n",
      "Epoch [22090/50000], Train Loss: 29.0945, Test Loss: 47.7795\n",
      "Epoch [22095/50000], Train Loss: 39.3707, Test Loss: 48.7213\n",
      "Epoch [22100/50000], Train Loss: 31.9910, Test Loss: 50.0628\n",
      "Epoch [22105/50000], Train Loss: 33.8671, Test Loss: 46.9038\n",
      "Epoch [22110/50000], Train Loss: 32.6948, Test Loss: 46.7426\n",
      "Epoch [22115/50000], Train Loss: 33.4949, Test Loss: 48.3092\n",
      "Epoch [22120/50000], Train Loss: 31.5387, Test Loss: 49.0495\n",
      "Epoch [22125/50000], Train Loss: 21.1584, Test Loss: 46.7678\n",
      "Epoch [22130/50000], Train Loss: 33.1085, Test Loss: 48.9646\n",
      "Epoch [22135/50000], Train Loss: 26.3143, Test Loss: 47.9415\n",
      "Epoch [22140/50000], Train Loss: 26.2706, Test Loss: 50.7251\n",
      "Epoch [22145/50000], Train Loss: 40.4423, Test Loss: 46.1023\n",
      "Epoch [22150/50000], Train Loss: 91.8056, Test Loss: 85.3975\n",
      "Epoch [22155/50000], Train Loss: 35.3385, Test Loss: 46.8427\n",
      "Epoch [22160/50000], Train Loss: 25.0028, Test Loss: 45.9500\n",
      "Epoch [22165/50000], Train Loss: 31.1380, Test Loss: 54.9023\n",
      "Epoch [22170/50000], Train Loss: 28.7859, Test Loss: 50.7265\n",
      "Epoch [22175/50000], Train Loss: 53.6114, Test Loss: 47.3485\n",
      "Epoch [22180/50000], Train Loss: 34.7625, Test Loss: 49.9493\n",
      "Epoch [22185/50000], Train Loss: 34.6083, Test Loss: 49.5399\n",
      "Epoch [22190/50000], Train Loss: 28.4053, Test Loss: 46.8959\n",
      "Epoch [22195/50000], Train Loss: 44.2807, Test Loss: 50.6900\n",
      "Epoch [22200/50000], Train Loss: 51.2452, Test Loss: 47.1949\n",
      "Epoch [22205/50000], Train Loss: 76.6973, Test Loss: 52.8464\n",
      "Epoch [22210/50000], Train Loss: 40.6363, Test Loss: 46.5851\n",
      "Epoch [22215/50000], Train Loss: 33.7387, Test Loss: 52.4758\n",
      "Epoch [22220/50000], Train Loss: 29.5630, Test Loss: 47.1251\n",
      "Epoch [22225/50000], Train Loss: 21.1015, Test Loss: 47.1794\n",
      "Epoch [22230/50000], Train Loss: 32.5967, Test Loss: 52.1816\n",
      "Epoch [22235/50000], Train Loss: 50.2326, Test Loss: 86.8709\n",
      "Epoch [22240/50000], Train Loss: 31.6986, Test Loss: 51.0754\n",
      "Epoch [22245/50000], Train Loss: 47.8054, Test Loss: 47.3319\n",
      "Epoch [22250/50000], Train Loss: 34.2413, Test Loss: 60.2117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22255/50000], Train Loss: 29.6555, Test Loss: 57.5391\n",
      "Epoch [22260/50000], Train Loss: 31.2205, Test Loss: 46.0247\n",
      "Epoch [22265/50000], Train Loss: 28.6238, Test Loss: 46.9233\n",
      "Epoch [22270/50000], Train Loss: 32.2861, Test Loss: 52.8639\n",
      "Epoch [22275/50000], Train Loss: 22.3998, Test Loss: 44.8378\n",
      "Epoch [22280/50000], Train Loss: 28.0524, Test Loss: 46.6063\n",
      "Epoch [22285/50000], Train Loss: 43.5938, Test Loss: 56.4738\n",
      "Epoch [22290/50000], Train Loss: 28.8005, Test Loss: 47.6247\n",
      "Epoch [22295/50000], Train Loss: 36.1349, Test Loss: 47.5721\n",
      "Epoch [22300/50000], Train Loss: 29.2523, Test Loss: 50.3973\n",
      "Epoch [22305/50000], Train Loss: 48.9977, Test Loss: 48.9304\n",
      "Epoch [22310/50000], Train Loss: 37.1846, Test Loss: 45.7862\n",
      "Epoch [22315/50000], Train Loss: 31.3811, Test Loss: 58.2464\n",
      "Epoch [22320/50000], Train Loss: 34.5483, Test Loss: 62.4007\n",
      "Epoch [22325/50000], Train Loss: 30.9935, Test Loss: 51.0663\n",
      "Epoch [22330/50000], Train Loss: 36.1554, Test Loss: 48.5216\n",
      "Epoch [22335/50000], Train Loss: 30.9556, Test Loss: 48.1523\n",
      "Epoch [22340/50000], Train Loss: 32.8422, Test Loss: 49.5671\n",
      "Epoch [22345/50000], Train Loss: 44.9477, Test Loss: 47.9056\n",
      "Epoch [22350/50000], Train Loss: 38.9379, Test Loss: 46.0237\n",
      "Epoch [22355/50000], Train Loss: 49.3224, Test Loss: 44.9982\n",
      "Epoch [22360/50000], Train Loss: 34.8661, Test Loss: 51.8070\n",
      "Epoch [22365/50000], Train Loss: 29.7746, Test Loss: 56.8239\n",
      "Epoch [22370/50000], Train Loss: 51.3681, Test Loss: 48.9891\n",
      "Epoch [22375/50000], Train Loss: 35.4941, Test Loss: 46.5966\n",
      "Epoch [22380/50000], Train Loss: 30.4221, Test Loss: 50.2650\n",
      "Epoch [22385/50000], Train Loss: 27.3006, Test Loss: 48.9877\n",
      "Epoch [22390/50000], Train Loss: 27.9120, Test Loss: 51.9011\n",
      "Epoch [22395/50000], Train Loss: 33.7137, Test Loss: 50.9577\n",
      "Epoch [22400/50000], Train Loss: 24.8096, Test Loss: 45.6196\n",
      "Epoch [22405/50000], Train Loss: 61.3912, Test Loss: 46.4249\n",
      "Epoch [22410/50000], Train Loss: 26.0339, Test Loss: 52.8371\n",
      "Epoch [22415/50000], Train Loss: 30.6859, Test Loss: 51.7258\n",
      "Epoch [22420/50000], Train Loss: 34.9860, Test Loss: 48.0777\n",
      "Epoch [22425/50000], Train Loss: 41.6670, Test Loss: 50.4189\n",
      "Epoch [22430/50000], Train Loss: 29.9592, Test Loss: 48.9658\n",
      "Epoch [22435/50000], Train Loss: 32.5450, Test Loss: 57.0337\n",
      "Epoch [22440/50000], Train Loss: 38.2155, Test Loss: 101.5885\n",
      "Epoch [22445/50000], Train Loss: 36.2055, Test Loss: 58.6930\n",
      "Epoch [22450/50000], Train Loss: 32.5379, Test Loss: 49.2698\n",
      "Epoch [22455/50000], Train Loss: 52.8259, Test Loss: 57.3233\n",
      "Epoch [22460/50000], Train Loss: 34.6030, Test Loss: 54.0284\n",
      "Epoch [22465/50000], Train Loss: 37.8428, Test Loss: 46.5157\n",
      "Epoch [22470/50000], Train Loss: 34.7694, Test Loss: 47.3936\n",
      "Epoch [22475/50000], Train Loss: 33.4319, Test Loss: 47.8193\n",
      "Epoch [22480/50000], Train Loss: 32.6253, Test Loss: 51.6689\n",
      "Epoch [22485/50000], Train Loss: 50.5722, Test Loss: 47.0189\n",
      "Epoch [22490/50000], Train Loss: 24.5867, Test Loss: 45.2896\n",
      "Epoch [22495/50000], Train Loss: 39.5734, Test Loss: 49.4170\n",
      "Epoch [22500/50000], Train Loss: 29.6829, Test Loss: 47.1836\n",
      "Epoch [22505/50000], Train Loss: 42.1939, Test Loss: 47.2391\n",
      "Epoch [22510/50000], Train Loss: 30.8115, Test Loss: 48.5745\n",
      "Epoch [22515/50000], Train Loss: 31.0109, Test Loss: 54.8487\n",
      "Epoch [22520/50000], Train Loss: 25.7536, Test Loss: 48.6929\n",
      "Epoch [22525/50000], Train Loss: 30.1371, Test Loss: 48.7562\n",
      "Epoch [22530/50000], Train Loss: 28.2410, Test Loss: 48.4033\n",
      "Epoch [22535/50000], Train Loss: 28.2887, Test Loss: 47.1731\n",
      "Epoch [22540/50000], Train Loss: 40.4041, Test Loss: 60.0145\n",
      "Epoch [22545/50000], Train Loss: 24.8408, Test Loss: 48.0962\n",
      "Epoch [22550/50000], Train Loss: 28.2356, Test Loss: 61.3135\n",
      "Epoch [22555/50000], Train Loss: 32.5265, Test Loss: 52.4536\n",
      "Epoch [22560/50000], Train Loss: 31.4793, Test Loss: 51.6012\n",
      "Epoch [22565/50000], Train Loss: 28.4967, Test Loss: 45.2773\n",
      "Epoch [22570/50000], Train Loss: 34.3480, Test Loss: 47.4780\n",
      "Epoch [22575/50000], Train Loss: 37.7475, Test Loss: 55.2860\n",
      "Epoch [22580/50000], Train Loss: 31.5503, Test Loss: 49.5782\n",
      "Epoch [22585/50000], Train Loss: 20.8743, Test Loss: 45.2326\n",
      "Epoch [22590/50000], Train Loss: 32.7215, Test Loss: 47.6098\n",
      "Epoch [22595/50000], Train Loss: 32.5720, Test Loss: 50.8243\n",
      "Epoch [22600/50000], Train Loss: 21.7836, Test Loss: 46.6104\n",
      "Epoch [22605/50000], Train Loss: 38.4328, Test Loss: 52.4168\n",
      "Epoch [22610/50000], Train Loss: 31.0404, Test Loss: 48.2654\n",
      "Epoch [22615/50000], Train Loss: 30.7159, Test Loss: 46.4017\n",
      "Epoch [22620/50000], Train Loss: 33.3230, Test Loss: 75.8664\n",
      "Epoch [22625/50000], Train Loss: 26.8449, Test Loss: 48.9840\n",
      "Epoch [22630/50000], Train Loss: 62.8713, Test Loss: 47.4188\n",
      "Epoch [22635/50000], Train Loss: 30.7502, Test Loss: 47.7153\n",
      "Epoch [22640/50000], Train Loss: 33.8300, Test Loss: 52.1343\n",
      "Epoch [22645/50000], Train Loss: 30.8391, Test Loss: 48.4295\n",
      "Epoch [22650/50000], Train Loss: 36.2865, Test Loss: 48.6758\n",
      "Epoch [22655/50000], Train Loss: 27.9978, Test Loss: 46.3898\n",
      "Epoch [22660/50000], Train Loss: 42.3237, Test Loss: 50.5058\n",
      "Epoch [22665/50000], Train Loss: 30.9990, Test Loss: 58.6170\n",
      "Epoch [22670/50000], Train Loss: 29.1670, Test Loss: 47.5392\n",
      "Epoch [22675/50000], Train Loss: 41.0424, Test Loss: 46.2496\n",
      "Epoch [22680/50000], Train Loss: 25.8891, Test Loss: 46.7058\n",
      "Epoch [22685/50000], Train Loss: 31.8981, Test Loss: 51.3414\n",
      "Epoch [22690/50000], Train Loss: 37.0831, Test Loss: 48.3122\n",
      "Epoch [22695/50000], Train Loss: 34.2221, Test Loss: 51.9589\n",
      "Epoch [22700/50000], Train Loss: 26.5181, Test Loss: 46.7567\n",
      "Epoch [22705/50000], Train Loss: 31.9930, Test Loss: 60.7297\n",
      "Epoch [22710/50000], Train Loss: 31.2558, Test Loss: 60.5185\n",
      "Epoch [22715/50000], Train Loss: 31.9581, Test Loss: 48.8788\n",
      "Epoch [22720/50000], Train Loss: 48.6827, Test Loss: 47.6729\n",
      "Epoch [22725/50000], Train Loss: 44.2860, Test Loss: 49.7027\n",
      "Epoch [22730/50000], Train Loss: 30.8336, Test Loss: 48.2032\n",
      "Epoch [22735/50000], Train Loss: 45.8121, Test Loss: 70.2735\n",
      "Epoch [22740/50000], Train Loss: 30.5171, Test Loss: 50.5609\n",
      "Epoch [22745/50000], Train Loss: 29.4790, Test Loss: 49.9873\n",
      "Epoch [22750/50000], Train Loss: 32.1864, Test Loss: 49.5143\n",
      "Epoch [22755/50000], Train Loss: 33.4968, Test Loss: 55.8239\n",
      "Epoch [22760/50000], Train Loss: 53.0530, Test Loss: 129.2248\n",
      "Epoch [22765/50000], Train Loss: 33.0979, Test Loss: 45.0231\n",
      "Epoch [22770/50000], Train Loss: 33.3139, Test Loss: 50.9290\n",
      "Epoch [22775/50000], Train Loss: 63.9558, Test Loss: 58.0338\n",
      "Epoch [22780/50000], Train Loss: 32.3401, Test Loss: 47.6548\n",
      "Epoch [22785/50000], Train Loss: 43.8826, Test Loss: 49.7518\n",
      "Epoch [22790/50000], Train Loss: 34.3225, Test Loss: 58.7946\n",
      "Epoch [22795/50000], Train Loss: 41.3169, Test Loss: 57.5181\n",
      "Epoch [22800/50000], Train Loss: 32.9842, Test Loss: 48.9279\n",
      "Epoch [22805/50000], Train Loss: 23.3257, Test Loss: 58.4311\n",
      "Epoch [22810/50000], Train Loss: 40.1297, Test Loss: 47.6677\n",
      "Epoch [22815/50000], Train Loss: 28.4300, Test Loss: 50.3734\n",
      "Epoch [22820/50000], Train Loss: 197.5210, Test Loss: 54.2088\n",
      "Epoch [22825/50000], Train Loss: 26.4345, Test Loss: 45.4904\n",
      "Epoch [22830/50000], Train Loss: 32.9713, Test Loss: 48.0448\n",
      "Epoch [22835/50000], Train Loss: 45.6757, Test Loss: 46.4548\n",
      "Epoch [22840/50000], Train Loss: 34.9573, Test Loss: 51.4517\n",
      "Epoch [22845/50000], Train Loss: 36.4509, Test Loss: 45.9009\n",
      "Epoch [22850/50000], Train Loss: 35.2137, Test Loss: 48.9406\n",
      "Epoch [22855/50000], Train Loss: 34.1880, Test Loss: 61.5995\n",
      "Epoch [22860/50000], Train Loss: 41.2060, Test Loss: 50.7177\n",
      "Epoch [22865/50000], Train Loss: 31.5963, Test Loss: 47.8385\n",
      "Epoch [22870/50000], Train Loss: 30.1288, Test Loss: 47.9822\n",
      "Epoch [22875/50000], Train Loss: 32.5761, Test Loss: 46.8030\n",
      "Epoch [22880/50000], Train Loss: 36.9502, Test Loss: 49.6341\n",
      "Epoch [22885/50000], Train Loss: 31.5225, Test Loss: 50.9378\n",
      "Epoch [22890/50000], Train Loss: 39.0289, Test Loss: 50.1703\n",
      "Epoch [22895/50000], Train Loss: 37.3003, Test Loss: 51.1026\n",
      "Epoch [22900/50000], Train Loss: 34.8515, Test Loss: 49.4039\n",
      "Epoch [22905/50000], Train Loss: 37.5769, Test Loss: 46.6632\n",
      "Epoch [22910/50000], Train Loss: 55.8733, Test Loss: 49.0257\n",
      "Epoch [22915/50000], Train Loss: 47.7560, Test Loss: 46.8520\n",
      "Epoch [22920/50000], Train Loss: 32.8123, Test Loss: 47.4841\n",
      "Epoch [22925/50000], Train Loss: 37.0764, Test Loss: 52.4835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22930/50000], Train Loss: 25.0254, Test Loss: 52.8464\n",
      "Epoch [22935/50000], Train Loss: 27.1688, Test Loss: 46.5378\n",
      "Epoch [22940/50000], Train Loss: 30.3136, Test Loss: 47.1614\n",
      "Epoch [22945/50000], Train Loss: 32.6588, Test Loss: 47.7045\n",
      "Epoch [22950/50000], Train Loss: 27.0705, Test Loss: 47.2972\n",
      "Epoch [22955/50000], Train Loss: 36.1781, Test Loss: 53.7449\n",
      "Epoch [22960/50000], Train Loss: 31.1493, Test Loss: 46.6005\n",
      "Epoch [22965/50000], Train Loss: 31.1297, Test Loss: 51.9620\n",
      "Epoch [22970/50000], Train Loss: 33.3136, Test Loss: 50.9322\n",
      "Epoch [22975/50000], Train Loss: 32.5229, Test Loss: 46.8298\n",
      "Epoch [22980/50000], Train Loss: 37.6733, Test Loss: 48.4237\n",
      "Epoch [22985/50000], Train Loss: 21.1777, Test Loss: 45.8808\n",
      "Epoch [22990/50000], Train Loss: 29.8125, Test Loss: 47.8443\n",
      "Epoch [22995/50000], Train Loss: 29.0232, Test Loss: 46.3574\n",
      "Epoch [23000/50000], Train Loss: 29.5070, Test Loss: 49.6055\n",
      "Epoch [23005/50000], Train Loss: 30.5655, Test Loss: 49.1981\n",
      "Epoch [23010/50000], Train Loss: 25.4072, Test Loss: 46.6451\n",
      "Epoch [23015/50000], Train Loss: 37.2225, Test Loss: 51.3120\n",
      "Epoch [23020/50000], Train Loss: 31.9419, Test Loss: 52.4674\n",
      "Epoch [23025/50000], Train Loss: 31.5723, Test Loss: 45.5744\n",
      "Epoch [23030/50000], Train Loss: 35.0840, Test Loss: 49.3398\n",
      "Epoch [23035/50000], Train Loss: 34.1235, Test Loss: 50.4782\n",
      "Epoch [23040/50000], Train Loss: 32.1844, Test Loss: 50.4279\n",
      "Epoch [23045/50000], Train Loss: 35.9304, Test Loss: 53.4120\n",
      "Epoch [23050/50000], Train Loss: 32.7550, Test Loss: 45.4108\n",
      "Epoch [23055/50000], Train Loss: 29.9661, Test Loss: 48.6474\n",
      "Epoch [23060/50000], Train Loss: 34.8608, Test Loss: 47.5726\n",
      "Epoch [23065/50000], Train Loss: 30.9363, Test Loss: 52.9469\n",
      "Epoch [23070/50000], Train Loss: 31.6594, Test Loss: 51.4804\n",
      "Epoch [23075/50000], Train Loss: 39.1652, Test Loss: 47.4060\n",
      "Epoch [23080/50000], Train Loss: 33.9547, Test Loss: 55.0882\n",
      "Epoch [23085/50000], Train Loss: 42.6487, Test Loss: 48.6528\n",
      "Epoch [23090/50000], Train Loss: 28.5966, Test Loss: 46.7451\n",
      "Epoch [23095/50000], Train Loss: 31.8136, Test Loss: 47.7775\n",
      "Epoch [23100/50000], Train Loss: 33.9198, Test Loss: 46.6482\n",
      "Epoch [23105/50000], Train Loss: 28.9497, Test Loss: 46.3408\n",
      "Epoch [23110/50000], Train Loss: 34.9194, Test Loss: 48.7623\n",
      "Epoch [23115/50000], Train Loss: 26.6458, Test Loss: 46.5376\n",
      "Epoch [23120/50000], Train Loss: 35.0951, Test Loss: 47.7187\n",
      "Epoch [23125/50000], Train Loss: 33.7346, Test Loss: 48.9127\n",
      "Epoch [23130/50000], Train Loss: 31.7299, Test Loss: 48.6782\n",
      "Epoch [23135/50000], Train Loss: 42.0871, Test Loss: 48.1695\n",
      "Epoch [23140/50000], Train Loss: 28.7269, Test Loss: 45.1662\n",
      "Epoch [23145/50000], Train Loss: 30.7119, Test Loss: 54.8298\n",
      "Epoch [23150/50000], Train Loss: 31.7130, Test Loss: 55.0689\n",
      "Epoch [23155/50000], Train Loss: 33.4401, Test Loss: 49.2991\n",
      "Epoch [23160/50000], Train Loss: 34.6477, Test Loss: 45.9460\n",
      "Epoch [23165/50000], Train Loss: 32.9951, Test Loss: 47.7582\n",
      "Epoch [23170/50000], Train Loss: 40.0881, Test Loss: 46.7701\n",
      "Epoch [23175/50000], Train Loss: 32.9781, Test Loss: 46.3492\n",
      "Epoch [23180/50000], Train Loss: 25.9119, Test Loss: 50.5455\n",
      "Epoch [23185/50000], Train Loss: 34.3171, Test Loss: 49.5805\n",
      "Epoch [23190/50000], Train Loss: 30.1660, Test Loss: 46.7707\n",
      "Epoch [23195/50000], Train Loss: 31.5419, Test Loss: 49.3509\n",
      "Epoch [23200/50000], Train Loss: 34.3929, Test Loss: 48.3744\n",
      "Epoch [23205/50000], Train Loss: 32.0550, Test Loss: 46.2300\n",
      "Epoch [23210/50000], Train Loss: 29.1688, Test Loss: 49.6476\n",
      "Epoch [23215/50000], Train Loss: 31.3621, Test Loss: 48.3643\n",
      "Epoch [23220/50000], Train Loss: 26.8655, Test Loss: 48.9016\n",
      "Epoch [23225/50000], Train Loss: 27.2070, Test Loss: 49.1088\n",
      "Epoch [23230/50000], Train Loss: 27.8457, Test Loss: 46.4314\n",
      "Epoch [23235/50000], Train Loss: 28.1725, Test Loss: 46.6083\n",
      "Epoch [23240/50000], Train Loss: 29.3904, Test Loss: 47.2758\n",
      "Epoch [23245/50000], Train Loss: 32.3936, Test Loss: 46.9742\n",
      "Epoch [23250/50000], Train Loss: 32.3519, Test Loss: 62.5766\n",
      "Epoch [23255/50000], Train Loss: 29.8122, Test Loss: 52.9212\n",
      "Epoch [23260/50000], Train Loss: 36.5974, Test Loss: 77.3811\n",
      "Epoch [23265/50000], Train Loss: 42.1618, Test Loss: 47.3341\n",
      "Epoch [23270/50000], Train Loss: 26.2198, Test Loss: 47.8066\n",
      "Epoch [23275/50000], Train Loss: 27.0112, Test Loss: 47.3234\n",
      "Epoch [23280/50000], Train Loss: 26.9818, Test Loss: 48.7018\n",
      "Epoch [23285/50000], Train Loss: 29.0153, Test Loss: 57.0620\n",
      "Epoch [23290/50000], Train Loss: 37.6728, Test Loss: 48.0354\n",
      "Epoch [23295/50000], Train Loss: 37.4290, Test Loss: 48.8328\n",
      "Epoch [23300/50000], Train Loss: 30.1672, Test Loss: 55.2089\n",
      "Epoch [23305/50000], Train Loss: 62.7577, Test Loss: 47.7032\n",
      "Epoch [23310/50000], Train Loss: 37.3248, Test Loss: 91.5120\n",
      "Epoch [23315/50000], Train Loss: 26.3674, Test Loss: 52.1760\n",
      "Epoch [23320/50000], Train Loss: 32.8509, Test Loss: 49.6317\n",
      "Epoch [23325/50000], Train Loss: 42.4842, Test Loss: 47.6714\n",
      "Epoch [23330/50000], Train Loss: 29.5998, Test Loss: 45.8855\n",
      "Epoch [23335/50000], Train Loss: 28.2965, Test Loss: 51.3168\n",
      "Epoch [23340/50000], Train Loss: 21.3038, Test Loss: 45.4210\n",
      "Epoch [23345/50000], Train Loss: 30.0658, Test Loss: 49.8520\n",
      "Epoch [23350/50000], Train Loss: 22.2758, Test Loss: 47.1112\n",
      "Epoch [23355/50000], Train Loss: 46.9232, Test Loss: 45.1800\n",
      "Epoch [23360/50000], Train Loss: 29.6702, Test Loss: 51.2971\n",
      "Epoch [23365/50000], Train Loss: 33.9311, Test Loss: 47.0564\n",
      "Epoch [23370/50000], Train Loss: 32.6447, Test Loss: 46.6224\n",
      "Epoch [23375/50000], Train Loss: 32.0294, Test Loss: 51.4659\n",
      "Epoch [23380/50000], Train Loss: 34.1456, Test Loss: 46.5341\n",
      "Epoch [23385/50000], Train Loss: 31.1832, Test Loss: 47.1736\n",
      "Epoch [23390/50000], Train Loss: 30.3974, Test Loss: 45.7806\n",
      "Epoch [23395/50000], Train Loss: 32.0499, Test Loss: 51.5673\n",
      "Epoch [23400/50000], Train Loss: 35.9596, Test Loss: 50.1048\n",
      "Epoch [23405/50000], Train Loss: 25.5896, Test Loss: 45.7860\n",
      "Epoch [23410/50000], Train Loss: 32.7650, Test Loss: 47.9593\n",
      "Epoch [23415/50000], Train Loss: 34.5659, Test Loss: 50.3720\n",
      "Epoch [23420/50000], Train Loss: 20.9936, Test Loss: 45.7060\n",
      "Epoch [23425/50000], Train Loss: 36.0767, Test Loss: 50.3418\n",
      "Epoch [23430/50000], Train Loss: 33.9206, Test Loss: 48.7579\n",
      "Epoch [23435/50000], Train Loss: 38.8266, Test Loss: 46.7060\n",
      "Epoch [23440/50000], Train Loss: 32.3073, Test Loss: 55.2514\n",
      "Epoch [23445/50000], Train Loss: 31.9035, Test Loss: 47.8888\n",
      "Epoch [23450/50000], Train Loss: 25.8359, Test Loss: 45.9899\n",
      "Epoch [23455/50000], Train Loss: 35.3095, Test Loss: 46.6231\n",
      "Epoch [23460/50000], Train Loss: 30.5868, Test Loss: 58.6889\n",
      "Epoch [23465/50000], Train Loss: 30.2038, Test Loss: 45.9312\n",
      "Epoch [23470/50000], Train Loss: 32.6691, Test Loss: 47.0095\n",
      "Epoch [23475/50000], Train Loss: 33.9773, Test Loss: 47.2084\n",
      "Epoch [23480/50000], Train Loss: 33.6293, Test Loss: 45.8339\n",
      "Epoch [23485/50000], Train Loss: 87.5086, Test Loss: 46.0566\n",
      "Epoch [23490/50000], Train Loss: 31.8530, Test Loss: 45.9403\n",
      "Epoch [23495/50000], Train Loss: 25.3989, Test Loss: 46.3700\n",
      "Epoch [23500/50000], Train Loss: 27.5841, Test Loss: 46.6890\n",
      "Epoch [23505/50000], Train Loss: 38.9558, Test Loss: 47.4266\n",
      "Epoch [23510/50000], Train Loss: 35.3158, Test Loss: 45.2751\n",
      "Epoch [23515/50000], Train Loss: 29.8987, Test Loss: 45.0843\n",
      "Epoch [23520/50000], Train Loss: 25.9041, Test Loss: 47.1133\n",
      "Epoch [23525/50000], Train Loss: 34.5912, Test Loss: 49.1827\n",
      "Epoch [23530/50000], Train Loss: 23.8964, Test Loss: 46.3857\n",
      "Epoch [23535/50000], Train Loss: 37.1540, Test Loss: 49.6179\n",
      "Epoch [23540/50000], Train Loss: 33.4165, Test Loss: 47.4959\n",
      "Epoch [23545/50000], Train Loss: 29.8796, Test Loss: 47.7156\n",
      "Epoch [23550/50000], Train Loss: 29.5273, Test Loss: 48.6041\n",
      "Epoch [23555/50000], Train Loss: 48.7410, Test Loss: 45.2730\n",
      "Epoch [23560/50000], Train Loss: 35.1010, Test Loss: 48.3533\n",
      "Epoch [23565/50000], Train Loss: 35.3991, Test Loss: 45.5650\n",
      "Epoch [23570/50000], Train Loss: 31.3790, Test Loss: 52.6779\n",
      "Epoch [23575/50000], Train Loss: 31.7958, Test Loss: 47.0261\n",
      "Epoch [23580/50000], Train Loss: 26.4223, Test Loss: 46.7972\n",
      "Epoch [23585/50000], Train Loss: 42.2132, Test Loss: 51.6197\n",
      "Epoch [23590/50000], Train Loss: 24.9809, Test Loss: 48.2402\n",
      "Epoch [23595/50000], Train Loss: 27.0940, Test Loss: 49.1941\n",
      "Epoch [23600/50000], Train Loss: 22.8250, Test Loss: 51.2115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23605/50000], Train Loss: 35.5439, Test Loss: 53.0717\n",
      "Epoch [23610/50000], Train Loss: 30.4917, Test Loss: 64.8769\n",
      "Epoch [23615/50000], Train Loss: 32.3765, Test Loss: 49.5767\n",
      "Epoch [23620/50000], Train Loss: 33.8075, Test Loss: 48.0192\n",
      "Epoch [23625/50000], Train Loss: 30.1351, Test Loss: 46.6907\n",
      "Epoch [23630/50000], Train Loss: 31.7920, Test Loss: 50.8232\n",
      "Epoch [23635/50000], Train Loss: 32.2837, Test Loss: 57.6933\n",
      "Epoch [23640/50000], Train Loss: 42.8929, Test Loss: 44.7692\n",
      "Epoch [23645/50000], Train Loss: 45.9958, Test Loss: 81.7533\n",
      "Epoch [23650/50000], Train Loss: 34.6267, Test Loss: 48.8980\n",
      "Epoch [23655/50000], Train Loss: 32.6612, Test Loss: 47.1333\n",
      "Epoch [23660/50000], Train Loss: 32.5189, Test Loss: 54.7482\n",
      "Epoch [23665/50000], Train Loss: 27.7485, Test Loss: 50.6483\n",
      "Epoch [23670/50000], Train Loss: 18.3044, Test Loss: 45.0952\n",
      "Epoch [23675/50000], Train Loss: 36.2049, Test Loss: 48.4892\n",
      "Epoch [23680/50000], Train Loss: 34.2387, Test Loss: 50.4866\n",
      "Epoch [23685/50000], Train Loss: 25.0168, Test Loss: 46.7197\n",
      "Epoch [23690/50000], Train Loss: 54.4663, Test Loss: 47.6863\n",
      "Epoch [23695/50000], Train Loss: 29.3019, Test Loss: 52.3387\n",
      "Epoch [23700/50000], Train Loss: 24.7797, Test Loss: 50.4148\n",
      "Epoch [23705/50000], Train Loss: 28.3098, Test Loss: 45.8107\n",
      "Epoch [23710/50000], Train Loss: 25.3293, Test Loss: 46.2051\n",
      "Epoch [23715/50000], Train Loss: 34.6987, Test Loss: 56.5053\n",
      "Epoch [23720/50000], Train Loss: 29.5536, Test Loss: 52.7961\n",
      "Epoch [23725/50000], Train Loss: 26.4434, Test Loss: 45.5774\n",
      "Epoch [23730/50000], Train Loss: 27.4151, Test Loss: 47.8175\n",
      "Epoch [23735/50000], Train Loss: 31.4861, Test Loss: 49.4445\n",
      "Epoch [23740/50000], Train Loss: 31.5269, Test Loss: 48.3049\n",
      "Epoch [23745/50000], Train Loss: 32.2450, Test Loss: 57.7408\n",
      "Epoch [23750/50000], Train Loss: 28.4347, Test Loss: 52.1115\n",
      "Epoch [23755/50000], Train Loss: 31.0471, Test Loss: 50.7876\n",
      "Epoch [23760/50000], Train Loss: 31.5106, Test Loss: 45.5554\n",
      "Epoch [23765/50000], Train Loss: 38.6690, Test Loss: 47.9197\n",
      "Epoch [23770/50000], Train Loss: 36.8716, Test Loss: 45.8233\n",
      "Epoch [23775/50000], Train Loss: 27.6955, Test Loss: 45.7455\n",
      "Epoch [23780/50000], Train Loss: 79.8618, Test Loss: 44.6753\n",
      "Epoch [23785/50000], Train Loss: 30.2891, Test Loss: 58.7351\n",
      "Epoch [23790/50000], Train Loss: 30.6608, Test Loss: 51.9905\n",
      "Epoch [23795/50000], Train Loss: 30.9648, Test Loss: 45.1249\n",
      "Epoch [23800/50000], Train Loss: 47.4560, Test Loss: 60.1012\n",
      "Epoch [23805/50000], Train Loss: 65.4029, Test Loss: 47.1259\n",
      "Epoch [23810/50000], Train Loss: 24.9866, Test Loss: 46.9878\n",
      "Epoch [23815/50000], Train Loss: 29.5310, Test Loss: 45.0075\n",
      "Epoch [23820/50000], Train Loss: 32.5067, Test Loss: 50.6431\n",
      "Epoch [23825/50000], Train Loss: 37.5685, Test Loss: 46.0612\n",
      "Epoch [23830/50000], Train Loss: 40.4609, Test Loss: 49.6367\n",
      "Epoch [23835/50000], Train Loss: 33.9709, Test Loss: 47.0999\n",
      "Epoch [23840/50000], Train Loss: 38.1748, Test Loss: 50.0223\n",
      "Epoch [23845/50000], Train Loss: 30.6485, Test Loss: 46.6496\n",
      "Epoch [23850/50000], Train Loss: 41.8379, Test Loss: 108.4036\n",
      "Epoch [23855/50000], Train Loss: 25.9506, Test Loss: 46.4553\n",
      "Epoch [23860/50000], Train Loss: 27.7460, Test Loss: 50.6801\n",
      "Epoch [23865/50000], Train Loss: 28.5144, Test Loss: 53.3835\n",
      "Epoch [23870/50000], Train Loss: 40.2870, Test Loss: 46.5713\n",
      "Epoch [23875/50000], Train Loss: 48.6062, Test Loss: 51.9234\n",
      "Epoch [23880/50000], Train Loss: 34.2703, Test Loss: 54.4109\n",
      "Epoch [23885/50000], Train Loss: 33.5856, Test Loss: 47.2164\n",
      "Epoch [23890/50000], Train Loss: 31.8636, Test Loss: 46.6613\n",
      "Epoch [23895/50000], Train Loss: 35.0286, Test Loss: 45.2789\n",
      "Epoch [23900/50000], Train Loss: 33.0211, Test Loss: 47.9717\n",
      "Epoch [23905/50000], Train Loss: 28.1893, Test Loss: 50.1674\n",
      "Epoch [23910/50000], Train Loss: 57.3431, Test Loss: 103.4361\n",
      "Epoch [23915/50000], Train Loss: 33.2998, Test Loss: 45.8045\n",
      "Epoch [23920/50000], Train Loss: 29.0984, Test Loss: 47.7857\n",
      "Epoch [23925/50000], Train Loss: 35.0321, Test Loss: 46.2549\n",
      "Epoch [23930/50000], Train Loss: 29.6751, Test Loss: 49.2616\n",
      "Epoch [23935/50000], Train Loss: 52.7399, Test Loss: 55.4456\n",
      "Epoch [23940/50000], Train Loss: 36.3415, Test Loss: 51.9295\n",
      "Epoch [23945/50000], Train Loss: 27.7660, Test Loss: 54.1361\n",
      "Epoch [23950/50000], Train Loss: 30.7964, Test Loss: 54.5393\n",
      "Epoch [23955/50000], Train Loss: 117.9391, Test Loss: 49.7830\n",
      "Epoch [23960/50000], Train Loss: 43.1571, Test Loss: 58.4668\n",
      "Epoch [23965/50000], Train Loss: 28.5225, Test Loss: 48.2072\n",
      "Epoch [23970/50000], Train Loss: 27.8273, Test Loss: 47.9279\n",
      "Epoch [23975/50000], Train Loss: 29.2575, Test Loss: 47.3160\n",
      "Epoch [23980/50000], Train Loss: 73.6758, Test Loss: 49.2030\n",
      "Epoch [23985/50000], Train Loss: 33.8259, Test Loss: 48.9069\n",
      "Epoch [23990/50000], Train Loss: 25.4516, Test Loss: 46.2086\n",
      "Epoch [23995/50000], Train Loss: 29.1129, Test Loss: 49.0756\n",
      "Epoch [24000/50000], Train Loss: 42.4179, Test Loss: 45.4991\n",
      "Epoch [24005/50000], Train Loss: 39.9832, Test Loss: 48.2639\n",
      "Epoch [24010/50000], Train Loss: 30.0910, Test Loss: 46.8836\n",
      "Epoch [24015/50000], Train Loss: 19.7517, Test Loss: 47.3626\n",
      "Epoch [24020/50000], Train Loss: 31.8900, Test Loss: 54.4330\n",
      "Epoch [24025/50000], Train Loss: 26.2559, Test Loss: 46.4503\n",
      "Epoch [24030/50000], Train Loss: 30.1218, Test Loss: 48.1972\n",
      "Epoch [24035/50000], Train Loss: 21.9727, Test Loss: 47.7948\n",
      "Epoch [24040/50000], Train Loss: 26.2868, Test Loss: 48.5487\n",
      "Epoch [24045/50000], Train Loss: 32.1153, Test Loss: 46.4130\n",
      "Epoch [24050/50000], Train Loss: 27.1703, Test Loss: 47.6691\n",
      "Epoch [24055/50000], Train Loss: 32.3332, Test Loss: 48.4885\n",
      "Epoch [24060/50000], Train Loss: 26.0835, Test Loss: 45.2796\n",
      "Epoch [24065/50000], Train Loss: 28.6636, Test Loss: 47.4229\n",
      "Epoch [24070/50000], Train Loss: 25.2430, Test Loss: 48.9786\n",
      "Epoch [24075/50000], Train Loss: 34.8329, Test Loss: 49.3937\n",
      "Epoch [24080/50000], Train Loss: 28.9599, Test Loss: 48.2526\n",
      "Epoch [24085/50000], Train Loss: 30.8166, Test Loss: 46.2561\n",
      "Epoch [24090/50000], Train Loss: 33.0801, Test Loss: 49.4130\n",
      "Epoch [24095/50000], Train Loss: 19.5442, Test Loss: 46.7979\n",
      "Epoch [24100/50000], Train Loss: 30.5127, Test Loss: 48.9317\n",
      "Epoch [24105/50000], Train Loss: 28.8439, Test Loss: 56.1352\n",
      "Epoch [24110/50000], Train Loss: 27.7162, Test Loss: 48.3649\n",
      "Epoch [24115/50000], Train Loss: 22.5440, Test Loss: 45.5695\n",
      "Epoch [24120/50000], Train Loss: 38.0034, Test Loss: 52.3311\n",
      "Epoch [24125/50000], Train Loss: 28.8975, Test Loss: 47.9169\n",
      "Epoch [24130/50000], Train Loss: 33.1937, Test Loss: 46.7833\n",
      "Epoch [24135/50000], Train Loss: 30.8888, Test Loss: 44.8737\n",
      "Epoch [24140/50000], Train Loss: 29.3068, Test Loss: 46.0285\n",
      "Epoch [24145/50000], Train Loss: 31.0324, Test Loss: 46.1840\n",
      "Epoch [24150/50000], Train Loss: 25.8158, Test Loss: 52.8069\n",
      "Epoch [24155/50000], Train Loss: 28.4675, Test Loss: 47.3998\n",
      "Epoch [24160/50000], Train Loss: 29.2800, Test Loss: 47.7927\n",
      "Epoch [24165/50000], Train Loss: 28.8472, Test Loss: 49.9199\n",
      "Epoch [24170/50000], Train Loss: 30.0687, Test Loss: 48.1097\n",
      "Epoch [24175/50000], Train Loss: 32.6723, Test Loss: 47.7001\n",
      "Epoch [24180/50000], Train Loss: 29.5078, Test Loss: 54.0035\n",
      "Epoch [24185/50000], Train Loss: 25.8249, Test Loss: 44.6753\n",
      "Epoch [24190/50000], Train Loss: 30.4689, Test Loss: 46.2998\n",
      "Epoch [24195/50000], Train Loss: 26.3379, Test Loss: 45.6322\n",
      "Epoch [24200/50000], Train Loss: 57.6335, Test Loss: 50.1926\n",
      "Epoch [24205/50000], Train Loss: 28.8844, Test Loss: 49.1793\n",
      "Epoch [24210/50000], Train Loss: 22.9306, Test Loss: 46.1394\n",
      "Epoch [24215/50000], Train Loss: 36.2373, Test Loss: 45.1728\n",
      "Epoch [24220/50000], Train Loss: 29.3487, Test Loss: 46.4548\n",
      "Epoch [24225/50000], Train Loss: 24.8685, Test Loss: 48.9552\n",
      "Epoch [24230/50000], Train Loss: 24.0337, Test Loss: 50.0959\n",
      "Epoch [24235/50000], Train Loss: 34.3243, Test Loss: 48.5653\n",
      "Epoch [24240/50000], Train Loss: 60.0371, Test Loss: 45.9953\n",
      "Epoch [24245/50000], Train Loss: 38.0083, Test Loss: 46.0644\n",
      "Epoch [24250/50000], Train Loss: 50.6536, Test Loss: 46.4762\n",
      "Epoch [24255/50000], Train Loss: 29.5338, Test Loss: 46.1806\n",
      "Epoch [24260/50000], Train Loss: 37.1151, Test Loss: 47.9169\n",
      "Epoch [24265/50000], Train Loss: 36.2218, Test Loss: 50.5370\n",
      "Epoch [24270/50000], Train Loss: 25.1800, Test Loss: 53.3499\n",
      "Epoch [24275/50000], Train Loss: 27.0719, Test Loss: 46.0704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24280/50000], Train Loss: 26.5377, Test Loss: 49.3738\n",
      "Epoch [24285/50000], Train Loss: 34.2754, Test Loss: 45.9818\n",
      "Epoch [24290/50000], Train Loss: 29.1889, Test Loss: 50.3438\n",
      "Epoch [24295/50000], Train Loss: 32.6275, Test Loss: 45.6845\n",
      "Epoch [24300/50000], Train Loss: 28.3011, Test Loss: 45.2604\n",
      "Epoch [24305/50000], Train Loss: 30.8277, Test Loss: 47.2012\n",
      "Epoch [24310/50000], Train Loss: 32.2539, Test Loss: 49.0688\n",
      "Epoch [24315/50000], Train Loss: 32.2920, Test Loss: 51.5311\n",
      "Epoch [24320/50000], Train Loss: 23.3633, Test Loss: 45.4550\n",
      "Epoch [24325/50000], Train Loss: 30.8084, Test Loss: 49.3271\n",
      "Epoch [24330/50000], Train Loss: 27.9267, Test Loss: 48.1781\n",
      "Epoch [24335/50000], Train Loss: 30.2539, Test Loss: 51.9021\n",
      "Epoch [24340/50000], Train Loss: 29.8417, Test Loss: 46.8095\n",
      "Epoch [24345/50000], Train Loss: 31.6098, Test Loss: 48.2482\n",
      "Epoch [24350/50000], Train Loss: 35.7557, Test Loss: 53.8519\n",
      "Epoch [24355/50000], Train Loss: 38.2363, Test Loss: 47.1034\n",
      "Epoch [24360/50000], Train Loss: 24.2042, Test Loss: 46.7066\n",
      "Epoch [24365/50000], Train Loss: 26.9497, Test Loss: 49.6163\n",
      "Epoch [24370/50000], Train Loss: 40.9184, Test Loss: 46.1122\n",
      "Epoch [24375/50000], Train Loss: 22.6764, Test Loss: 44.8095\n",
      "Epoch [24380/50000], Train Loss: 27.7877, Test Loss: 56.2132\n",
      "Epoch [24385/50000], Train Loss: 40.8891, Test Loss: 48.2061\n",
      "Epoch [24390/50000], Train Loss: 29.6606, Test Loss: 46.8465\n",
      "Epoch [24395/50000], Train Loss: 29.3001, Test Loss: 56.0897\n",
      "Epoch [24400/50000], Train Loss: 31.0853, Test Loss: 50.0201\n",
      "Epoch [24405/50000], Train Loss: 31.3192, Test Loss: 50.6323\n",
      "Epoch [24410/50000], Train Loss: 32.6258, Test Loss: 49.2589\n",
      "Epoch [24415/50000], Train Loss: 28.1934, Test Loss: 47.1585\n",
      "Epoch [24420/50000], Train Loss: 29.8836, Test Loss: 53.4439\n",
      "Epoch [24425/50000], Train Loss: 33.0118, Test Loss: 47.3814\n",
      "Epoch [24430/50000], Train Loss: 23.6837, Test Loss: 46.8275\n",
      "Epoch [24435/50000], Train Loss: 32.4941, Test Loss: 47.5291\n",
      "Epoch [24440/50000], Train Loss: 46.4239, Test Loss: 69.8938\n",
      "Epoch [24445/50000], Train Loss: 27.9384, Test Loss: 45.4415\n",
      "Epoch [24450/50000], Train Loss: 24.2602, Test Loss: 47.8455\n",
      "Epoch [24455/50000], Train Loss: 27.0269, Test Loss: 46.9516\n",
      "Epoch [24460/50000], Train Loss: 23.7441, Test Loss: 48.3341\n",
      "Epoch [24465/50000], Train Loss: 30.1238, Test Loss: 50.8388\n",
      "Epoch [24470/50000], Train Loss: 47.8798, Test Loss: 47.8829\n",
      "Epoch [24475/50000], Train Loss: 26.4176, Test Loss: 46.8975\n",
      "Epoch [24480/50000], Train Loss: 30.4496, Test Loss: 47.0880\n",
      "Epoch [24485/50000], Train Loss: 38.0709, Test Loss: 44.9677\n",
      "Epoch [24490/50000], Train Loss: 30.5102, Test Loss: 45.9659\n",
      "Epoch [24495/50000], Train Loss: 20.8180, Test Loss: 45.9480\n",
      "Epoch [24500/50000], Train Loss: 30.7644, Test Loss: 48.3821\n",
      "Epoch [24505/50000], Train Loss: 25.7742, Test Loss: 46.9525\n",
      "Epoch [24510/50000], Train Loss: 32.1974, Test Loss: 46.4063\n",
      "Epoch [24515/50000], Train Loss: 31.3970, Test Loss: 45.9178\n",
      "Epoch [24520/50000], Train Loss: 39.4199, Test Loss: 46.2493\n",
      "Epoch [24525/50000], Train Loss: 50.7645, Test Loss: 66.6791\n",
      "Epoch [24530/50000], Train Loss: 29.9844, Test Loss: 46.7116\n",
      "Epoch [24535/50000], Train Loss: 31.9212, Test Loss: 47.2902\n",
      "Epoch [24540/50000], Train Loss: 30.9874, Test Loss: 44.6717\n",
      "Epoch [24545/50000], Train Loss: 39.9714, Test Loss: 49.3204\n",
      "Epoch [24550/50000], Train Loss: 21.6325, Test Loss: 49.4778\n",
      "Epoch [24555/50000], Train Loss: 45.4186, Test Loss: 45.2663\n",
      "Epoch [24560/50000], Train Loss: 27.6988, Test Loss: 46.8931\n",
      "Epoch [24565/50000], Train Loss: 32.5327, Test Loss: 46.9087\n",
      "Epoch [24570/50000], Train Loss: 36.4555, Test Loss: 48.2795\n",
      "Epoch [24575/50000], Train Loss: 26.7099, Test Loss: 45.9214\n",
      "Epoch [24580/50000], Train Loss: 25.1436, Test Loss: 45.9133\n",
      "Epoch [24585/50000], Train Loss: 28.1588, Test Loss: 45.1036\n",
      "Epoch [24590/50000], Train Loss: 27.1858, Test Loss: 47.9817\n",
      "Epoch [24595/50000], Train Loss: 25.2329, Test Loss: 51.4083\n",
      "Epoch [24600/50000], Train Loss: 37.0609, Test Loss: 49.8761\n",
      "Epoch [24605/50000], Train Loss: 28.2769, Test Loss: 47.3229\n",
      "Epoch [24610/50000], Train Loss: 32.1294, Test Loss: 47.3388\n",
      "Epoch [24615/50000], Train Loss: 50.3402, Test Loss: 45.3462\n",
      "Epoch [24620/50000], Train Loss: 38.8168, Test Loss: 45.7048\n",
      "Epoch [24625/50000], Train Loss: 34.9520, Test Loss: 46.3610\n",
      "Epoch [24630/50000], Train Loss: 26.6700, Test Loss: 46.4189\n",
      "Epoch [24635/50000], Train Loss: 32.5885, Test Loss: 45.3710\n",
      "Epoch [24640/50000], Train Loss: 27.4702, Test Loss: 45.9388\n",
      "Epoch [24645/50000], Train Loss: 43.9605, Test Loss: 48.3651\n",
      "Epoch [24650/50000], Train Loss: 29.3947, Test Loss: 45.9314\n",
      "Epoch [24655/50000], Train Loss: 30.9653, Test Loss: 46.2650\n",
      "Epoch [24660/50000], Train Loss: 24.2640, Test Loss: 64.0043\n",
      "Epoch [24665/50000], Train Loss: 20.8785, Test Loss: 46.1489\n",
      "Epoch [24670/50000], Train Loss: 37.2797, Test Loss: 56.0219\n",
      "Epoch [24675/50000], Train Loss: 27.7724, Test Loss: 52.3274\n",
      "Epoch [24680/50000], Train Loss: 31.6443, Test Loss: 46.1727\n",
      "Epoch [24685/50000], Train Loss: 37.1120, Test Loss: 44.6555\n",
      "Epoch [24690/50000], Train Loss: 56.6414, Test Loss: 45.3474\n",
      "Epoch [24695/50000], Train Loss: 27.9988, Test Loss: 47.7116\n",
      "Epoch [24700/50000], Train Loss: 21.0199, Test Loss: 45.6729\n",
      "Epoch [24705/50000], Train Loss: 27.1961, Test Loss: 44.8334\n",
      "Epoch [24710/50000], Train Loss: 35.6909, Test Loss: 45.9738\n",
      "Epoch [24715/50000], Train Loss: 28.7164, Test Loss: 46.2239\n",
      "Epoch [24720/50000], Train Loss: 30.8206, Test Loss: 47.6714\n",
      "Epoch [24725/50000], Train Loss: 33.6501, Test Loss: 45.5484\n",
      "Epoch [24730/50000], Train Loss: 26.4511, Test Loss: 48.0726\n",
      "Epoch [24735/50000], Train Loss: 36.5716, Test Loss: 45.7286\n",
      "Epoch [24740/50000], Train Loss: 27.0422, Test Loss: 52.8708\n",
      "Epoch [24745/50000], Train Loss: 48.3025, Test Loss: 45.1774\n",
      "Epoch [24750/50000], Train Loss: 38.6898, Test Loss: 48.9405\n",
      "Epoch [24755/50000], Train Loss: 26.9805, Test Loss: 48.8110\n",
      "Epoch [24760/50000], Train Loss: 49.7004, Test Loss: 45.0864\n",
      "Epoch [24765/50000], Train Loss: 25.8438, Test Loss: 45.8432\n",
      "Epoch [24770/50000], Train Loss: 28.6741, Test Loss: 50.9953\n",
      "Epoch [24775/50000], Train Loss: 25.8059, Test Loss: 47.9594\n",
      "Epoch [24780/50000], Train Loss: 32.1085, Test Loss: 54.8979\n",
      "Epoch [24785/50000], Train Loss: 32.5122, Test Loss: 46.4466\n",
      "Epoch [24790/50000], Train Loss: 31.1110, Test Loss: 51.3728\n",
      "Epoch [24795/50000], Train Loss: 26.1711, Test Loss: 50.2082\n",
      "Epoch [24800/50000], Train Loss: 28.0432, Test Loss: 49.0214\n",
      "Epoch [24805/50000], Train Loss: 28.1171, Test Loss: 48.9359\n",
      "Epoch [24810/50000], Train Loss: 26.3344, Test Loss: 45.0378\n",
      "Epoch [24815/50000], Train Loss: 27.9143, Test Loss: 47.6826\n",
      "Epoch [24820/50000], Train Loss: 48.4091, Test Loss: 49.8894\n",
      "Epoch [24825/50000], Train Loss: 24.2730, Test Loss: 46.9903\n",
      "Epoch [24830/50000], Train Loss: 31.0122, Test Loss: 49.7585\n",
      "Epoch [24835/50000], Train Loss: 32.5945, Test Loss: 45.9527\n",
      "Epoch [24840/50000], Train Loss: 27.6458, Test Loss: 49.6757\n",
      "Epoch [24845/50000], Train Loss: 32.7937, Test Loss: 48.5943\n",
      "Epoch [24850/50000], Train Loss: 23.6919, Test Loss: 44.0576\n",
      "Epoch [24855/50000], Train Loss: 35.5507, Test Loss: 51.4535\n",
      "Epoch [24860/50000], Train Loss: 29.2377, Test Loss: 54.2509\n",
      "Epoch [24865/50000], Train Loss: 29.3087, Test Loss: 48.1979\n",
      "Epoch [24870/50000], Train Loss: 23.0772, Test Loss: 46.9369\n",
      "Epoch [24875/50000], Train Loss: 30.9184, Test Loss: 50.2601\n",
      "Epoch [24880/50000], Train Loss: 35.6844, Test Loss: 46.3691\n",
      "Epoch [24885/50000], Train Loss: 31.0282, Test Loss: 46.4414\n",
      "Epoch [24890/50000], Train Loss: 27.9705, Test Loss: 47.5614\n",
      "Epoch [24895/50000], Train Loss: 25.6774, Test Loss: 47.9168\n",
      "Epoch [24900/50000], Train Loss: 24.4175, Test Loss: 46.3035\n",
      "Epoch [24905/50000], Train Loss: 32.5341, Test Loss: 46.7698\n",
      "Epoch [24910/50000], Train Loss: 29.3723, Test Loss: 46.2497\n",
      "Epoch [24915/50000], Train Loss: 56.0657, Test Loss: 45.4029\n",
      "Epoch [24920/50000], Train Loss: 31.4309, Test Loss: 47.9339\n",
      "Epoch [24925/50000], Train Loss: 24.5614, Test Loss: 45.3854\n",
      "Epoch [24930/50000], Train Loss: 27.1482, Test Loss: 47.3545\n",
      "Epoch [24935/50000], Train Loss: 26.5358, Test Loss: 46.2846\n",
      "Epoch [24940/50000], Train Loss: 29.5012, Test Loss: 47.4780\n",
      "Epoch [24945/50000], Train Loss: 33.0189, Test Loss: 45.0342\n",
      "Epoch [24950/50000], Train Loss: 29.5957, Test Loss: 48.9944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24955/50000], Train Loss: 34.2956, Test Loss: 47.4028\n",
      "Epoch [24960/50000], Train Loss: 29.4491, Test Loss: 44.3163\n",
      "Epoch [24965/50000], Train Loss: 43.7718, Test Loss: 46.8328\n",
      "Epoch [24970/50000], Train Loss: 29.1798, Test Loss: 52.7981\n",
      "Epoch [24975/50000], Train Loss: 35.6603, Test Loss: 48.6043\n",
      "Epoch [24980/50000], Train Loss: 27.3824, Test Loss: 48.0775\n",
      "Epoch [24985/50000], Train Loss: 26.1396, Test Loss: 45.4444\n",
      "Epoch [24990/50000], Train Loss: 28.2858, Test Loss: 45.0029\n",
      "Epoch [24995/50000], Train Loss: 33.3367, Test Loss: 45.8732\n",
      "Epoch [25000/50000], Train Loss: 30.7857, Test Loss: 50.0678\n",
      "Epoch [25005/50000], Train Loss: 29.3982, Test Loss: 48.7191\n",
      "Epoch [25010/50000], Train Loss: 31.2641, Test Loss: 49.8436\n",
      "Epoch [25015/50000], Train Loss: 22.6780, Test Loss: 47.7473\n",
      "Epoch [25020/50000], Train Loss: 37.0189, Test Loss: 47.5840\n",
      "Epoch [25025/50000], Train Loss: 32.0037, Test Loss: 46.6071\n",
      "Epoch [25030/50000], Train Loss: 32.0156, Test Loss: 52.3432\n",
      "Epoch [25035/50000], Train Loss: 31.9828, Test Loss: 51.8917\n",
      "Epoch [25040/50000], Train Loss: 31.7708, Test Loss: 50.8327\n",
      "Epoch [25045/50000], Train Loss: 30.2086, Test Loss: 45.8849\n",
      "Epoch [25050/50000], Train Loss: 27.9945, Test Loss: 51.2583\n",
      "Epoch [25055/50000], Train Loss: 25.6518, Test Loss: 52.2766\n",
      "Epoch [25060/50000], Train Loss: 25.5147, Test Loss: 50.0819\n",
      "Epoch [25065/50000], Train Loss: 30.9860, Test Loss: 55.7822\n",
      "Epoch [25070/50000], Train Loss: 32.5695, Test Loss: 50.2319\n",
      "Epoch [25075/50000], Train Loss: 52.4541, Test Loss: 46.5241\n",
      "Epoch [25080/50000], Train Loss: 26.5256, Test Loss: 46.5872\n",
      "Epoch [25085/50000], Train Loss: 22.9031, Test Loss: 52.4232\n",
      "Epoch [25090/50000], Train Loss: 79.8276, Test Loss: 46.5310\n",
      "Epoch [25095/50000], Train Loss: 28.4389, Test Loss: 48.1153\n",
      "Epoch [25100/50000], Train Loss: 31.0109, Test Loss: 45.1308\n",
      "Epoch [25105/50000], Train Loss: 31.1604, Test Loss: 60.9914\n",
      "Epoch [25110/50000], Train Loss: 32.0758, Test Loss: 45.5182\n",
      "Epoch [25115/50000], Train Loss: 29.6554, Test Loss: 56.8780\n",
      "Epoch [25120/50000], Train Loss: 32.2299, Test Loss: 49.7820\n",
      "Epoch [25125/50000], Train Loss: 31.1532, Test Loss: 46.2036\n",
      "Epoch [25130/50000], Train Loss: 42.6506, Test Loss: 59.9969\n",
      "Epoch [25135/50000], Train Loss: 32.6669, Test Loss: 49.6233\n",
      "Epoch [25140/50000], Train Loss: 51.2057, Test Loss: 44.5284\n",
      "Epoch [25145/50000], Train Loss: 23.9480, Test Loss: 44.9135\n",
      "Epoch [25150/50000], Train Loss: 26.2977, Test Loss: 45.5552\n",
      "Epoch [25155/50000], Train Loss: 43.7565, Test Loss: 47.2341\n",
      "Epoch [25160/50000], Train Loss: 30.4086, Test Loss: 46.8970\n",
      "Epoch [25165/50000], Train Loss: 28.7667, Test Loss: 46.1490\n",
      "Epoch [25170/50000], Train Loss: 26.2216, Test Loss: 49.6112\n",
      "Epoch [25175/50000], Train Loss: 32.2647, Test Loss: 49.6575\n",
      "Epoch [25180/50000], Train Loss: 27.0096, Test Loss: 47.1392\n",
      "Epoch [25185/50000], Train Loss: 29.6384, Test Loss: 48.6376\n",
      "Epoch [25190/50000], Train Loss: 29.9690, Test Loss: 51.3315\n",
      "Epoch [25195/50000], Train Loss: 33.5116, Test Loss: 45.1202\n",
      "Epoch [25200/50000], Train Loss: 33.1725, Test Loss: 46.1728\n",
      "Epoch [25205/50000], Train Loss: 30.9559, Test Loss: 46.8065\n",
      "Epoch [25210/50000], Train Loss: 72.5577, Test Loss: 47.1196\n",
      "Epoch [25215/50000], Train Loss: 28.5454, Test Loss: 48.8719\n",
      "Epoch [25220/50000], Train Loss: 39.2895, Test Loss: 49.4624\n",
      "Epoch [25225/50000], Train Loss: 60.7944, Test Loss: 44.8036\n",
      "Epoch [25230/50000], Train Loss: 31.4910, Test Loss: 47.3929\n",
      "Epoch [25235/50000], Train Loss: 25.5708, Test Loss: 45.9141\n",
      "Epoch [25240/50000], Train Loss: 25.4573, Test Loss: 47.5197\n",
      "Epoch [25245/50000], Train Loss: 38.8623, Test Loss: 70.5855\n",
      "Epoch [25250/50000], Train Loss: 32.8454, Test Loss: 48.3890\n",
      "Epoch [25255/50000], Train Loss: 30.1333, Test Loss: 46.9470\n",
      "Epoch [25260/50000], Train Loss: 27.4858, Test Loss: 49.2173\n",
      "Epoch [25265/50000], Train Loss: 27.4180, Test Loss: 55.1147\n",
      "Epoch [25270/50000], Train Loss: 38.4847, Test Loss: 53.1060\n",
      "Epoch [25275/50000], Train Loss: 37.1891, Test Loss: 50.4911\n",
      "Epoch [25280/50000], Train Loss: 32.9342, Test Loss: 44.6425\n",
      "Epoch [25285/50000], Train Loss: 24.5910, Test Loss: 45.0187\n",
      "Epoch [25290/50000], Train Loss: 30.7976, Test Loss: 45.8198\n",
      "Epoch [25295/50000], Train Loss: 27.0526, Test Loss: 44.8927\n",
      "Epoch [25300/50000], Train Loss: 30.8032, Test Loss: 45.0810\n",
      "Epoch [25305/50000], Train Loss: 56.2068, Test Loss: 74.9987\n",
      "Epoch [25310/50000], Train Loss: 22.5047, Test Loss: 45.6416\n",
      "Epoch [25315/50000], Train Loss: 27.3661, Test Loss: 46.0927\n",
      "Epoch [25320/50000], Train Loss: 29.7136, Test Loss: 49.2524\n",
      "Epoch [25325/50000], Train Loss: 30.5157, Test Loss: 46.9624\n",
      "Epoch [25330/50000], Train Loss: 31.1786, Test Loss: 50.3423\n",
      "Epoch [25335/50000], Train Loss: 30.5109, Test Loss: 48.0847\n",
      "Epoch [25340/50000], Train Loss: 31.7373, Test Loss: 48.9244\n",
      "Epoch [25345/50000], Train Loss: 24.3634, Test Loss: 45.5284\n",
      "Epoch [25350/50000], Train Loss: 32.2056, Test Loss: 44.7854\n",
      "Epoch [25355/50000], Train Loss: 28.4531, Test Loss: 54.4944\n",
      "Epoch [25360/50000], Train Loss: 28.9860, Test Loss: 45.9333\n",
      "Epoch [25365/50000], Train Loss: 34.7580, Test Loss: 52.3133\n",
      "Epoch [25370/50000], Train Loss: 29.0808, Test Loss: 46.6358\n",
      "Epoch [25375/50000], Train Loss: 32.0396, Test Loss: 45.2771\n",
      "Epoch [25380/50000], Train Loss: 32.1086, Test Loss: 89.0628\n",
      "Epoch [25385/50000], Train Loss: 27.1956, Test Loss: 46.6090\n",
      "Epoch [25390/50000], Train Loss: 42.0110, Test Loss: 45.7630\n",
      "Epoch [25395/50000], Train Loss: 32.0536, Test Loss: 52.3996\n",
      "Epoch [25400/50000], Train Loss: 37.0445, Test Loss: 52.4425\n",
      "Epoch [25405/50000], Train Loss: 24.2581, Test Loss: 45.3410\n",
      "Epoch [25410/50000], Train Loss: 29.3636, Test Loss: 45.4446\n",
      "Epoch [25415/50000], Train Loss: 26.3656, Test Loss: 50.7591\n",
      "Epoch [25420/50000], Train Loss: 23.0320, Test Loss: 46.1638\n",
      "Epoch [25425/50000], Train Loss: 29.9124, Test Loss: 50.4648\n",
      "Epoch [25430/50000], Train Loss: 20.4118, Test Loss: 48.8965\n",
      "Epoch [25435/50000], Train Loss: 31.0316, Test Loss: 51.3430\n",
      "Epoch [25440/50000], Train Loss: 27.4977, Test Loss: 49.5999\n",
      "Epoch [25445/50000], Train Loss: 30.2104, Test Loss: 45.2914\n",
      "Epoch [25450/50000], Train Loss: 43.8013, Test Loss: 77.7302\n",
      "Epoch [25455/50000], Train Loss: 35.2313, Test Loss: 45.6212\n",
      "Epoch [25460/50000], Train Loss: 27.4363, Test Loss: 47.1893\n",
      "Epoch [25465/50000], Train Loss: 34.5043, Test Loss: 48.8537\n",
      "Epoch [25470/50000], Train Loss: 26.9170, Test Loss: 46.4955\n",
      "Epoch [25475/50000], Train Loss: 29.5764, Test Loss: 45.8179\n",
      "Epoch [25480/50000], Train Loss: 26.9998, Test Loss: 51.8772\n",
      "Epoch [25485/50000], Train Loss: 39.4977, Test Loss: 45.4731\n",
      "Epoch [25490/50000], Train Loss: 29.1811, Test Loss: 48.4528\n",
      "Epoch [25495/50000], Train Loss: 32.1891, Test Loss: 46.4476\n",
      "Epoch [25500/50000], Train Loss: 31.3812, Test Loss: 48.2226\n",
      "Epoch [25505/50000], Train Loss: 30.1877, Test Loss: 48.6047\n",
      "Epoch [25510/50000], Train Loss: 24.0034, Test Loss: 44.4746\n",
      "Epoch [25515/50000], Train Loss: 34.7636, Test Loss: 46.8115\n",
      "Epoch [25520/50000], Train Loss: 31.7747, Test Loss: 46.1359\n",
      "Epoch [25525/50000], Train Loss: 18.6827, Test Loss: 46.7103\n",
      "Epoch [25530/50000], Train Loss: 21.5334, Test Loss: 46.4735\n",
      "Epoch [25535/50000], Train Loss: 40.2638, Test Loss: 48.7228\n",
      "Epoch [25540/50000], Train Loss: 23.8088, Test Loss: 45.5493\n",
      "Epoch [25545/50000], Train Loss: 33.6915, Test Loss: 46.2118\n",
      "Epoch [25550/50000], Train Loss: 40.3155, Test Loss: 46.1620\n",
      "Epoch [25555/50000], Train Loss: 28.9651, Test Loss: 48.7271\n",
      "Epoch [25560/50000], Train Loss: 21.7591, Test Loss: 47.7019\n",
      "Epoch [25565/50000], Train Loss: 43.7181, Test Loss: 47.8536\n",
      "Epoch [25570/50000], Train Loss: 30.4593, Test Loss: 45.3855\n",
      "Epoch [25575/50000], Train Loss: 28.9612, Test Loss: 44.6259\n",
      "Epoch [25580/50000], Train Loss: 27.7176, Test Loss: 47.2942\n",
      "Epoch [25585/50000], Train Loss: 32.7111, Test Loss: 44.4291\n",
      "Epoch [25590/50000], Train Loss: 28.8592, Test Loss: 50.4160\n",
      "Epoch [25595/50000], Train Loss: 25.7357, Test Loss: 47.6356\n",
      "Epoch [25600/50000], Train Loss: 31.2871, Test Loss: 49.9528\n",
      "Epoch [25605/50000], Train Loss: 27.1474, Test Loss: 49.3278\n",
      "Epoch [25610/50000], Train Loss: 23.0227, Test Loss: 47.5520\n",
      "Epoch [25615/50000], Train Loss: 30.1717, Test Loss: 46.1687\n",
      "Epoch [25620/50000], Train Loss: 27.5396, Test Loss: 59.4862\n",
      "Epoch [25625/50000], Train Loss: 23.7305, Test Loss: 44.9752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25630/50000], Train Loss: 24.7797, Test Loss: 51.1096\n",
      "Epoch [25635/50000], Train Loss: 29.2665, Test Loss: 46.5630\n",
      "Epoch [25640/50000], Train Loss: 26.9997, Test Loss: 54.4476\n",
      "Epoch [25645/50000], Train Loss: 41.7039, Test Loss: 46.2024\n",
      "Epoch [25650/50000], Train Loss: 26.1408, Test Loss: 47.0107\n",
      "Epoch [25655/50000], Train Loss: 32.3845, Test Loss: 44.1519\n",
      "Epoch [25660/50000], Train Loss: 31.9023, Test Loss: 46.0664\n",
      "Epoch [25665/50000], Train Loss: 20.8297, Test Loss: 46.1030\n",
      "Epoch [25670/50000], Train Loss: 29.6266, Test Loss: 48.2515\n",
      "Epoch [25675/50000], Train Loss: 31.3232, Test Loss: 46.9796\n",
      "Epoch [25680/50000], Train Loss: 25.7545, Test Loss: 45.5092\n",
      "Epoch [25685/50000], Train Loss: 68.4221, Test Loss: 44.8322\n",
      "Epoch [25690/50000], Train Loss: 28.2652, Test Loss: 45.9135\n",
      "Epoch [25695/50000], Train Loss: 25.4684, Test Loss: 64.3056\n",
      "Epoch [25700/50000], Train Loss: 27.2096, Test Loss: 45.9957\n",
      "Epoch [25705/50000], Train Loss: 22.9520, Test Loss: 44.4301\n",
      "Epoch [25710/50000], Train Loss: 28.1630, Test Loss: 46.7183\n",
      "Epoch [25715/50000], Train Loss: 66.5067, Test Loss: 74.6382\n",
      "Epoch [25720/50000], Train Loss: 34.3631, Test Loss: 48.8138\n",
      "Epoch [25725/50000], Train Loss: 29.6059, Test Loss: 44.4385\n",
      "Epoch [25730/50000], Train Loss: 25.7040, Test Loss: 45.9516\n",
      "Epoch [25735/50000], Train Loss: 24.5538, Test Loss: 47.0300\n",
      "Epoch [25740/50000], Train Loss: 28.3550, Test Loss: 48.5853\n",
      "Epoch [25745/50000], Train Loss: 32.1033, Test Loss: 54.6062\n",
      "Epoch [25750/50000], Train Loss: 31.7468, Test Loss: 44.7889\n",
      "Epoch [25755/50000], Train Loss: 34.2292, Test Loss: 49.4708\n",
      "Epoch [25760/50000], Train Loss: 36.3231, Test Loss: 47.9217\n",
      "Epoch [25765/50000], Train Loss: 28.1918, Test Loss: 50.8149\n",
      "Epoch [25770/50000], Train Loss: 32.0172, Test Loss: 50.2815\n",
      "Epoch [25775/50000], Train Loss: 29.7748, Test Loss: 48.6094\n",
      "Epoch [25780/50000], Train Loss: 29.9958, Test Loss: 46.3514\n",
      "Epoch [25785/50000], Train Loss: 31.2428, Test Loss: 45.5623\n",
      "Epoch [25790/50000], Train Loss: 38.2701, Test Loss: 45.7405\n",
      "Epoch [25795/50000], Train Loss: 43.4172, Test Loss: 46.5790\n",
      "Epoch [25800/50000], Train Loss: 150.9556, Test Loss: 44.5305\n",
      "Epoch [25805/50000], Train Loss: 27.8314, Test Loss: 48.9636\n",
      "Epoch [25810/50000], Train Loss: 27.9640, Test Loss: 48.5120\n",
      "Epoch [25815/50000], Train Loss: 25.9810, Test Loss: 47.9583\n",
      "Epoch [25820/50000], Train Loss: 33.7160, Test Loss: 53.5840\n",
      "Epoch [25825/50000], Train Loss: 25.1671, Test Loss: 48.4240\n",
      "Epoch [25830/50000], Train Loss: 36.2729, Test Loss: 47.5709\n",
      "Epoch [25835/50000], Train Loss: 23.2601, Test Loss: 55.7204\n",
      "Epoch [25840/50000], Train Loss: 28.0156, Test Loss: 61.4510\n",
      "Epoch [25845/50000], Train Loss: 42.7560, Test Loss: 48.4254\n",
      "Epoch [25850/50000], Train Loss: 23.1358, Test Loss: 45.7218\n",
      "Epoch [25855/50000], Train Loss: 33.0115, Test Loss: 79.0404\n",
      "Epoch [25860/50000], Train Loss: 26.4982, Test Loss: 50.3638\n",
      "Epoch [25865/50000], Train Loss: 36.4501, Test Loss: 45.2700\n",
      "Epoch [25870/50000], Train Loss: 48.6446, Test Loss: 43.7551\n",
      "Epoch [25875/50000], Train Loss: 37.2439, Test Loss: 45.4045\n",
      "Epoch [25880/50000], Train Loss: 29.6440, Test Loss: 44.5303\n",
      "Epoch [25885/50000], Train Loss: 30.3922, Test Loss: 49.2070\n",
      "Epoch [25890/50000], Train Loss: 37.5252, Test Loss: 44.7463\n",
      "Epoch [25895/50000], Train Loss: 28.7804, Test Loss: 45.7119\n",
      "Epoch [25900/50000], Train Loss: 31.1400, Test Loss: 44.9717\n",
      "Epoch [25905/50000], Train Loss: 30.3144, Test Loss: 44.8901\n",
      "Epoch [25910/50000], Train Loss: 27.7050, Test Loss: 50.3982\n",
      "Epoch [25915/50000], Train Loss: 32.1286, Test Loss: 51.3361\n",
      "Epoch [25920/50000], Train Loss: 30.0122, Test Loss: 49.3892\n",
      "Epoch [25925/50000], Train Loss: 29.4086, Test Loss: 46.0177\n",
      "Epoch [25930/50000], Train Loss: 27.4130, Test Loss: 45.0090\n",
      "Epoch [25935/50000], Train Loss: 28.3163, Test Loss: 45.3857\n",
      "Epoch [25940/50000], Train Loss: 24.8805, Test Loss: 46.6980\n",
      "Epoch [25945/50000], Train Loss: 28.8480, Test Loss: 44.9289\n",
      "Epoch [25950/50000], Train Loss: 35.4310, Test Loss: 47.8318\n",
      "Epoch [25955/50000], Train Loss: 50.2108, Test Loss: 43.8793\n",
      "Epoch [25960/50000], Train Loss: 34.5609, Test Loss: 46.0290\n",
      "Epoch [25965/50000], Train Loss: 33.5324, Test Loss: 77.4310\n",
      "Epoch [25970/50000], Train Loss: 30.0555, Test Loss: 46.9333\n",
      "Epoch [25975/50000], Train Loss: 40.2759, Test Loss: 44.5430\n",
      "Epoch [25980/50000], Train Loss: 26.2362, Test Loss: 45.4548\n",
      "Epoch [25985/50000], Train Loss: 29.6189, Test Loss: 50.3908\n",
      "Epoch [25990/50000], Train Loss: 26.1824, Test Loss: 48.2217\n",
      "Epoch [25995/50000], Train Loss: 33.3360, Test Loss: 48.6145\n",
      "Epoch [26000/50000], Train Loss: 23.2985, Test Loss: 46.8537\n",
      "Epoch [26005/50000], Train Loss: 32.5133, Test Loss: 51.1286\n",
      "Epoch [26010/50000], Train Loss: 98.1907, Test Loss: 44.7129\n",
      "Epoch [26015/50000], Train Loss: 44.2401, Test Loss: 47.3268\n",
      "Epoch [26020/50000], Train Loss: 27.8444, Test Loss: 46.7867\n",
      "Epoch [26025/50000], Train Loss: 25.5869, Test Loss: 49.3106\n",
      "Epoch [26030/50000], Train Loss: 27.5033, Test Loss: 48.1558\n",
      "Epoch [26035/50000], Train Loss: 27.6090, Test Loss: 50.1239\n",
      "Epoch [26040/50000], Train Loss: 49.4131, Test Loss: 49.6778\n",
      "Epoch [26045/50000], Train Loss: 27.7029, Test Loss: 45.5711\n",
      "Epoch [26050/50000], Train Loss: 36.4192, Test Loss: 50.1334\n",
      "Epoch [26055/50000], Train Loss: 29.3177, Test Loss: 45.4776\n",
      "Epoch [26060/50000], Train Loss: 25.9256, Test Loss: 44.7340\n",
      "Epoch [26065/50000], Train Loss: 31.0696, Test Loss: 49.6016\n",
      "Epoch [26070/50000], Train Loss: 26.5142, Test Loss: 47.1064\n",
      "Epoch [26075/50000], Train Loss: 21.7303, Test Loss: 45.0186\n",
      "Epoch [26080/50000], Train Loss: 32.0158, Test Loss: 46.6150\n",
      "Epoch [26085/50000], Train Loss: 37.2008, Test Loss: 48.2395\n",
      "Epoch [26090/50000], Train Loss: 25.9825, Test Loss: 48.7847\n",
      "Epoch [26095/50000], Train Loss: 27.8559, Test Loss: 63.9918\n",
      "Epoch [26100/50000], Train Loss: 31.7175, Test Loss: 45.5896\n",
      "Epoch [26105/50000], Train Loss: 27.4188, Test Loss: 55.3319\n",
      "Epoch [26110/50000], Train Loss: 33.5058, Test Loss: 50.3362\n",
      "Epoch [26115/50000], Train Loss: 28.7172, Test Loss: 45.9109\n",
      "Epoch [26120/50000], Train Loss: 29.9440, Test Loss: 46.3345\n",
      "Epoch [26125/50000], Train Loss: 29.8628, Test Loss: 51.1477\n",
      "Epoch [26130/50000], Train Loss: 24.7893, Test Loss: 47.3709\n",
      "Epoch [26135/50000], Train Loss: 34.6344, Test Loss: 47.3113\n",
      "Epoch [26140/50000], Train Loss: 29.0861, Test Loss: 45.5073\n",
      "Epoch [26145/50000], Train Loss: 34.5902, Test Loss: 51.4726\n",
      "Epoch [26150/50000], Train Loss: 30.1749, Test Loss: 51.7609\n",
      "Epoch [26155/50000], Train Loss: 31.2107, Test Loss: 45.1703\n",
      "Epoch [26160/50000], Train Loss: 27.1189, Test Loss: 48.3407\n",
      "Epoch [26165/50000], Train Loss: 27.5570, Test Loss: 48.6029\n",
      "Epoch [26170/50000], Train Loss: 24.2378, Test Loss: 46.0578\n",
      "Epoch [26175/50000], Train Loss: 23.7950, Test Loss: 44.9263\n",
      "Epoch [26180/50000], Train Loss: 30.0498, Test Loss: 51.7483\n",
      "Epoch [26185/50000], Train Loss: 36.3483, Test Loss: 60.6173\n",
      "Epoch [26190/50000], Train Loss: 28.4358, Test Loss: 50.0062\n",
      "Epoch [26195/50000], Train Loss: 27.8453, Test Loss: 46.3701\n",
      "Epoch [26200/50000], Train Loss: 32.6003, Test Loss: 46.5074\n",
      "Epoch [26205/50000], Train Loss: 30.1533, Test Loss: 44.9987\n",
      "Epoch [26210/50000], Train Loss: 27.8197, Test Loss: 51.1813\n",
      "Epoch [26215/50000], Train Loss: 26.6934, Test Loss: 45.8191\n",
      "Epoch [26220/50000], Train Loss: 29.1844, Test Loss: 45.5030\n",
      "Epoch [26225/50000], Train Loss: 22.3560, Test Loss: 43.8116\n",
      "Epoch [26230/50000], Train Loss: 30.4175, Test Loss: 45.9321\n",
      "Epoch [26235/50000], Train Loss: 24.8758, Test Loss: 47.0077\n",
      "Epoch [26240/50000], Train Loss: 31.2645, Test Loss: 48.1447\n",
      "Epoch [26245/50000], Train Loss: 21.1700, Test Loss: 45.3060\n",
      "Epoch [26250/50000], Train Loss: 23.1522, Test Loss: 51.2685\n",
      "Epoch [26255/50000], Train Loss: 27.2012, Test Loss: 49.8362\n",
      "Epoch [26260/50000], Train Loss: 38.4422, Test Loss: 52.0212\n",
      "Epoch [26265/50000], Train Loss: 27.5274, Test Loss: 45.8996\n",
      "Epoch [26270/50000], Train Loss: 28.7274, Test Loss: 51.4522\n",
      "Epoch [26275/50000], Train Loss: 27.9850, Test Loss: 46.4948\n",
      "Epoch [26280/50000], Train Loss: 28.5367, Test Loss: 46.9744\n",
      "Epoch [26285/50000], Train Loss: 25.8935, Test Loss: 46.8555\n",
      "Epoch [26290/50000], Train Loss: 33.3446, Test Loss: 45.4463\n",
      "Epoch [26295/50000], Train Loss: 25.2347, Test Loss: 44.2021\n",
      "Epoch [26300/50000], Train Loss: 27.7015, Test Loss: 49.4453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26305/50000], Train Loss: 19.0711, Test Loss: 46.2380\n",
      "Epoch [26310/50000], Train Loss: 25.8493, Test Loss: 47.5802\n",
      "Epoch [26315/50000], Train Loss: 34.0417, Test Loss: 45.7973\n",
      "Epoch [26320/50000], Train Loss: 28.5855, Test Loss: 48.6437\n",
      "Epoch [26325/50000], Train Loss: 30.1644, Test Loss: 47.0382\n",
      "Epoch [26330/50000], Train Loss: 58.9774, Test Loss: 45.4518\n",
      "Epoch [26335/50000], Train Loss: 29.0409, Test Loss: 46.9319\n",
      "Epoch [26340/50000], Train Loss: 24.9969, Test Loss: 45.1325\n",
      "Epoch [26345/50000], Train Loss: 33.1337, Test Loss: 44.8614\n",
      "Epoch [26350/50000], Train Loss: 24.5109, Test Loss: 45.9143\n",
      "Epoch [26355/50000], Train Loss: 31.1309, Test Loss: 53.1412\n",
      "Epoch [26360/50000], Train Loss: 28.9500, Test Loss: 45.8459\n",
      "Epoch [26365/50000], Train Loss: 33.3077, Test Loss: 49.5868\n",
      "Epoch [26370/50000], Train Loss: 40.7558, Test Loss: 48.8461\n",
      "Epoch [26375/50000], Train Loss: 27.5828, Test Loss: 49.2430\n",
      "Epoch [26380/50000], Train Loss: 59.3240, Test Loss: 64.3863\n",
      "Epoch [26385/50000], Train Loss: 31.0814, Test Loss: 45.9750\n",
      "Epoch [26390/50000], Train Loss: 28.0074, Test Loss: 45.1525\n",
      "Epoch [26395/50000], Train Loss: 30.9499, Test Loss: 46.2347\n",
      "Epoch [26400/50000], Train Loss: 54.8876, Test Loss: 44.8878\n",
      "Epoch [26405/50000], Train Loss: 25.5329, Test Loss: 47.0276\n",
      "Epoch [26410/50000], Train Loss: 31.2959, Test Loss: 46.7910\n",
      "Epoch [26415/50000], Train Loss: 28.3130, Test Loss: 43.8420\n",
      "Epoch [26420/50000], Train Loss: 36.5855, Test Loss: 45.6306\n",
      "Epoch [26425/50000], Train Loss: 28.6108, Test Loss: 44.9786\n",
      "Epoch [26430/50000], Train Loss: 29.4199, Test Loss: 48.5357\n",
      "Epoch [26435/50000], Train Loss: 47.2240, Test Loss: 49.3351\n",
      "Epoch [26440/50000], Train Loss: 25.0134, Test Loss: 47.4819\n",
      "Epoch [26445/50000], Train Loss: 26.0420, Test Loss: 47.4862\n",
      "Epoch [26450/50000], Train Loss: 26.4924, Test Loss: 49.1482\n",
      "Epoch [26455/50000], Train Loss: 29.0622, Test Loss: 45.6316\n",
      "Epoch [26460/50000], Train Loss: 27.9412, Test Loss: 49.8313\n",
      "Epoch [26465/50000], Train Loss: 28.9935, Test Loss: 46.4641\n",
      "Epoch [26470/50000], Train Loss: 26.8341, Test Loss: 46.1787\n",
      "Epoch [26475/50000], Train Loss: 37.4149, Test Loss: 61.1698\n",
      "Epoch [26480/50000], Train Loss: 29.5918, Test Loss: 47.6576\n",
      "Epoch [26485/50000], Train Loss: 53.5510, Test Loss: 46.3990\n",
      "Epoch [26490/50000], Train Loss: 27.8344, Test Loss: 46.7590\n",
      "Epoch [26495/50000], Train Loss: 52.6182, Test Loss: 71.0068\n",
      "Epoch [26500/50000], Train Loss: 25.0085, Test Loss: 44.0351\n",
      "Epoch [26505/50000], Train Loss: 32.9788, Test Loss: 58.8989\n",
      "Epoch [26510/50000], Train Loss: 32.9123, Test Loss: 48.2881\n",
      "Epoch [26515/50000], Train Loss: 34.9492, Test Loss: 45.0796\n",
      "Epoch [26520/50000], Train Loss: 22.4034, Test Loss: 46.8046\n",
      "Epoch [26525/50000], Train Loss: 29.6978, Test Loss: 46.3299\n",
      "Epoch [26530/50000], Train Loss: 29.8057, Test Loss: 45.7959\n",
      "Epoch [26535/50000], Train Loss: 26.6081, Test Loss: 47.3912\n",
      "Epoch [26540/50000], Train Loss: 27.9644, Test Loss: 45.7941\n",
      "Epoch [26545/50000], Train Loss: 21.8359, Test Loss: 52.5396\n",
      "Epoch [26550/50000], Train Loss: 26.4315, Test Loss: 49.6262\n",
      "Epoch [26555/50000], Train Loss: 26.9962, Test Loss: 45.9482\n",
      "Epoch [26560/50000], Train Loss: 26.9231, Test Loss: 45.4144\n",
      "Epoch [26565/50000], Train Loss: 27.9498, Test Loss: 47.9192\n",
      "Epoch [26570/50000], Train Loss: 24.5246, Test Loss: 44.7380\n",
      "Epoch [26575/50000], Train Loss: 28.7406, Test Loss: 48.2380\n",
      "Epoch [26580/50000], Train Loss: 23.5109, Test Loss: 45.5483\n",
      "Epoch [26585/50000], Train Loss: 27.0440, Test Loss: 47.0219\n",
      "Epoch [26590/50000], Train Loss: 26.6587, Test Loss: 45.2795\n",
      "Epoch [26595/50000], Train Loss: 26.1748, Test Loss: 53.3229\n",
      "Epoch [26600/50000], Train Loss: 72.0840, Test Loss: 44.3529\n",
      "Epoch [26605/50000], Train Loss: 31.4232, Test Loss: 52.2913\n",
      "Epoch [26610/50000], Train Loss: 27.7928, Test Loss: 48.7099\n",
      "Epoch [26615/50000], Train Loss: 28.8574, Test Loss: 44.6631\n",
      "Epoch [26620/50000], Train Loss: 27.9506, Test Loss: 46.9674\n",
      "Epoch [26625/50000], Train Loss: 26.0211, Test Loss: 54.7877\n",
      "Epoch [26630/50000], Train Loss: 33.5503, Test Loss: 50.5933\n",
      "Epoch [26635/50000], Train Loss: 45.2570, Test Loss: 46.0061\n",
      "Epoch [26640/50000], Train Loss: 34.1828, Test Loss: 45.6892\n",
      "Epoch [26645/50000], Train Loss: 34.9949, Test Loss: 46.4731\n",
      "Epoch [26650/50000], Train Loss: 33.3260, Test Loss: 47.2753\n",
      "Epoch [26655/50000], Train Loss: 29.1383, Test Loss: 46.0149\n",
      "Epoch [26660/50000], Train Loss: 26.2738, Test Loss: 44.9523\n",
      "Epoch [26665/50000], Train Loss: 41.0744, Test Loss: 45.8330\n",
      "Epoch [26670/50000], Train Loss: 42.6369, Test Loss: 46.7301\n",
      "Epoch [26675/50000], Train Loss: 29.2185, Test Loss: 50.0398\n",
      "Epoch [26680/50000], Train Loss: 21.4478, Test Loss: 44.9430\n",
      "Epoch [26685/50000], Train Loss: 31.7731, Test Loss: 48.9525\n",
      "Epoch [26690/50000], Train Loss: 26.8558, Test Loss: 45.0993\n",
      "Epoch [26695/50000], Train Loss: 37.6728, Test Loss: 49.8226\n",
      "Epoch [26700/50000], Train Loss: 30.3587, Test Loss: 48.8003\n",
      "Epoch [26705/50000], Train Loss: 28.3428, Test Loss: 54.8858\n",
      "Epoch [26710/50000], Train Loss: 27.5513, Test Loss: 45.0517\n",
      "Epoch [26715/50000], Train Loss: 28.4454, Test Loss: 45.4941\n",
      "Epoch [26720/50000], Train Loss: 28.1551, Test Loss: 50.1521\n",
      "Epoch [26725/50000], Train Loss: 27.1916, Test Loss: 47.5816\n",
      "Epoch [26730/50000], Train Loss: 30.0328, Test Loss: 45.4761\n",
      "Epoch [26735/50000], Train Loss: 27.0802, Test Loss: 45.3446\n",
      "Epoch [26740/50000], Train Loss: 20.9995, Test Loss: 47.9281\n",
      "Epoch [26745/50000], Train Loss: 23.8764, Test Loss: 45.7543\n",
      "Epoch [26750/50000], Train Loss: 25.1459, Test Loss: 46.0469\n",
      "Epoch [26755/50000], Train Loss: 57.6692, Test Loss: 45.9404\n",
      "Epoch [26760/50000], Train Loss: 22.8562, Test Loss: 47.5938\n",
      "Epoch [26765/50000], Train Loss: 27.9210, Test Loss: 45.9548\n",
      "Epoch [26770/50000], Train Loss: 27.2456, Test Loss: 51.3845\n",
      "Epoch [26775/50000], Train Loss: 24.2229, Test Loss: 48.5073\n",
      "Epoch [26780/50000], Train Loss: 43.0554, Test Loss: 44.9691\n",
      "Epoch [26785/50000], Train Loss: 25.5014, Test Loss: 46.4030\n",
      "Epoch [26790/50000], Train Loss: 42.0952, Test Loss: 44.0082\n",
      "Epoch [26795/50000], Train Loss: 24.2839, Test Loss: 44.2180\n",
      "Epoch [26800/50000], Train Loss: 25.7046, Test Loss: 51.1257\n",
      "Epoch [26805/50000], Train Loss: 28.5198, Test Loss: 45.3227\n",
      "Epoch [26810/50000], Train Loss: 29.5865, Test Loss: 47.4991\n",
      "Epoch [26815/50000], Train Loss: 25.7164, Test Loss: 45.9389\n",
      "Epoch [26820/50000], Train Loss: 20.2315, Test Loss: 45.3545\n",
      "Epoch [26825/50000], Train Loss: 27.1596, Test Loss: 48.1239\n",
      "Epoch [26830/50000], Train Loss: 29.9355, Test Loss: 48.4533\n",
      "Epoch [26835/50000], Train Loss: 22.7250, Test Loss: 52.6436\n",
      "Epoch [26840/50000], Train Loss: 25.8026, Test Loss: 46.6961\n",
      "Epoch [26845/50000], Train Loss: 108.7817, Test Loss: 44.2292\n",
      "Epoch [26850/50000], Train Loss: 29.5973, Test Loss: 48.1875\n",
      "Epoch [26855/50000], Train Loss: 33.7317, Test Loss: 46.2411\n",
      "Epoch [26860/50000], Train Loss: 28.2719, Test Loss: 45.8448\n",
      "Epoch [26865/50000], Train Loss: 26.1732, Test Loss: 46.6804\n",
      "Epoch [26870/50000], Train Loss: 30.3176, Test Loss: 48.0312\n",
      "Epoch [26875/50000], Train Loss: 26.9444, Test Loss: 46.0939\n",
      "Epoch [26880/50000], Train Loss: 25.1487, Test Loss: 49.6542\n",
      "Epoch [26885/50000], Train Loss: 78.1868, Test Loss: 45.2162\n",
      "Epoch [26890/50000], Train Loss: 32.1669, Test Loss: 45.6397\n",
      "Epoch [26895/50000], Train Loss: 33.6441, Test Loss: 45.6478\n",
      "Epoch [26900/50000], Train Loss: 23.3233, Test Loss: 45.1763\n",
      "Epoch [26905/50000], Train Loss: 29.7021, Test Loss: 44.1858\n",
      "Epoch [26910/50000], Train Loss: 21.1462, Test Loss: 44.5021\n",
      "Epoch [26915/50000], Train Loss: 29.8678, Test Loss: 46.8274\n",
      "Epoch [26920/50000], Train Loss: 24.4712, Test Loss: 46.1215\n",
      "Epoch [26925/50000], Train Loss: 29.3613, Test Loss: 53.9658\n",
      "Epoch [26930/50000], Train Loss: 27.6639, Test Loss: 44.7679\n",
      "Epoch [26935/50000], Train Loss: 25.8819, Test Loss: 53.2165\n",
      "Epoch [26940/50000], Train Loss: 21.0469, Test Loss: 45.2511\n",
      "Epoch [26945/50000], Train Loss: 47.8142, Test Loss: 49.4643\n",
      "Epoch [26950/50000], Train Loss: 25.7348, Test Loss: 52.0844\n",
      "Epoch [26955/50000], Train Loss: 27.2366, Test Loss: 46.7615\n",
      "Epoch [26960/50000], Train Loss: 38.4941, Test Loss: 48.6693\n",
      "Epoch [26965/50000], Train Loss: 29.4157, Test Loss: 46.9873\n",
      "Epoch [26970/50000], Train Loss: 59.0430, Test Loss: 52.0979\n",
      "Epoch [26975/50000], Train Loss: 26.7311, Test Loss: 45.2190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26980/50000], Train Loss: 39.5227, Test Loss: 53.7394\n",
      "Epoch [26985/50000], Train Loss: 31.3140, Test Loss: 49.7574\n",
      "Epoch [26990/50000], Train Loss: 25.4106, Test Loss: 44.9777\n",
      "Epoch [26995/50000], Train Loss: 25.0712, Test Loss: 46.7749\n",
      "Epoch [27000/50000], Train Loss: 39.3791, Test Loss: 47.3707\n",
      "Epoch [27005/50000], Train Loss: 25.8949, Test Loss: 44.6133\n",
      "Epoch [27010/50000], Train Loss: 26.6118, Test Loss: 45.1990\n",
      "Epoch [27015/50000], Train Loss: 29.5359, Test Loss: 46.4831\n",
      "Epoch [27020/50000], Train Loss: 26.5891, Test Loss: 45.6465\n",
      "Epoch [27025/50000], Train Loss: 27.0512, Test Loss: 47.5590\n",
      "Epoch [27030/50000], Train Loss: 33.4931, Test Loss: 46.9022\n",
      "Epoch [27035/50000], Train Loss: 19.6232, Test Loss: 43.9433\n",
      "Epoch [27040/50000], Train Loss: 32.5212, Test Loss: 48.6804\n",
      "Epoch [27045/50000], Train Loss: 25.6220, Test Loss: 46.6165\n",
      "Epoch [27050/50000], Train Loss: 41.8489, Test Loss: 45.1543\n",
      "Epoch [27055/50000], Train Loss: 33.0256, Test Loss: 49.5323\n",
      "Epoch [27060/50000], Train Loss: 31.4606, Test Loss: 46.2770\n",
      "Epoch [27065/50000], Train Loss: 26.3891, Test Loss: 52.0815\n",
      "Epoch [27070/50000], Train Loss: 27.3823, Test Loss: 46.0800\n",
      "Epoch [27075/50000], Train Loss: 27.8555, Test Loss: 45.0681\n",
      "Epoch [27080/50000], Train Loss: 30.9461, Test Loss: 49.0406\n",
      "Epoch [27085/50000], Train Loss: 25.2683, Test Loss: 44.4906\n",
      "Epoch [27090/50000], Train Loss: 26.4419, Test Loss: 45.3687\n",
      "Epoch [27095/50000], Train Loss: 26.3546, Test Loss: 48.1939\n",
      "Epoch [27100/50000], Train Loss: 25.6638, Test Loss: 46.8590\n",
      "Epoch [27105/50000], Train Loss: 27.6294, Test Loss: 49.8047\n",
      "Epoch [27110/50000], Train Loss: 36.1774, Test Loss: 46.2328\n",
      "Epoch [27115/50000], Train Loss: 27.8914, Test Loss: 45.0188\n",
      "Epoch [27120/50000], Train Loss: 37.8517, Test Loss: 47.5431\n",
      "Epoch [27125/50000], Train Loss: 34.4185, Test Loss: 51.0916\n",
      "Epoch [27130/50000], Train Loss: 27.6843, Test Loss: 49.1182\n",
      "Epoch [27135/50000], Train Loss: 29.3719, Test Loss: 46.6274\n",
      "Epoch [27140/50000], Train Loss: 27.2333, Test Loss: 44.0306\n",
      "Epoch [27145/50000], Train Loss: 21.7685, Test Loss: 45.1212\n",
      "Epoch [27150/50000], Train Loss: 36.3382, Test Loss: 59.2206\n",
      "Epoch [27155/50000], Train Loss: 27.7637, Test Loss: 45.4049\n",
      "Epoch [27160/50000], Train Loss: 25.8402, Test Loss: 48.0699\n",
      "Epoch [27165/50000], Train Loss: 33.0229, Test Loss: 54.6412\n",
      "Epoch [27170/50000], Train Loss: 26.8438, Test Loss: 53.0193\n",
      "Epoch [27175/50000], Train Loss: 34.4425, Test Loss: 45.1365\n",
      "Epoch [27180/50000], Train Loss: 33.8543, Test Loss: 57.7571\n",
      "Epoch [27185/50000], Train Loss: 63.6390, Test Loss: 45.4945\n",
      "Epoch [27190/50000], Train Loss: 25.9037, Test Loss: 49.6786\n",
      "Epoch [27195/50000], Train Loss: 25.4813, Test Loss: 47.5241\n",
      "Epoch [27200/50000], Train Loss: 30.4583, Test Loss: 46.4982\n",
      "Epoch [27205/50000], Train Loss: 24.5478, Test Loss: 47.5317\n",
      "Epoch [27210/50000], Train Loss: 26.6310, Test Loss: 51.1939\n",
      "Epoch [27215/50000], Train Loss: 24.2289, Test Loss: 45.7544\n",
      "Epoch [27220/50000], Train Loss: 33.4203, Test Loss: 50.2323\n",
      "Epoch [27225/50000], Train Loss: 28.8806, Test Loss: 52.1699\n",
      "Epoch [27230/50000], Train Loss: 25.9775, Test Loss: 50.9854\n",
      "Epoch [27235/50000], Train Loss: 33.0295, Test Loss: 52.4309\n",
      "Epoch [27240/50000], Train Loss: 23.6210, Test Loss: 50.8247\n",
      "Epoch [27245/50000], Train Loss: 26.0345, Test Loss: 54.4797\n",
      "Epoch [27250/50000], Train Loss: 47.4875, Test Loss: 46.3366\n",
      "Epoch [27255/50000], Train Loss: 27.6382, Test Loss: 44.5068\n",
      "Epoch [27260/50000], Train Loss: 22.1480, Test Loss: 50.1231\n",
      "Epoch [27265/50000], Train Loss: 52.9379, Test Loss: 44.4034\n",
      "Epoch [27270/50000], Train Loss: 23.9482, Test Loss: 47.5840\n",
      "Epoch [27275/50000], Train Loss: 29.9188, Test Loss: 44.4051\n",
      "Epoch [27280/50000], Train Loss: 30.5463, Test Loss: 44.9903\n",
      "Epoch [27285/50000], Train Loss: 25.5424, Test Loss: 46.6083\n",
      "Epoch [27290/50000], Train Loss: 23.9526, Test Loss: 49.9678\n",
      "Epoch [27295/50000], Train Loss: 27.0023, Test Loss: 49.7618\n",
      "Epoch [27300/50000], Train Loss: 27.9377, Test Loss: 45.0062\n",
      "Epoch [27305/50000], Train Loss: 26.5906, Test Loss: 47.2048\n",
      "Epoch [27310/50000], Train Loss: 23.3039, Test Loss: 47.5127\n",
      "Epoch [27315/50000], Train Loss: 42.5647, Test Loss: 43.9190\n",
      "Epoch [27320/50000], Train Loss: 24.4169, Test Loss: 45.4145\n",
      "Epoch [27325/50000], Train Loss: 27.9175, Test Loss: 45.5737\n",
      "Epoch [27330/50000], Train Loss: 22.8428, Test Loss: 58.9185\n",
      "Epoch [27335/50000], Train Loss: 30.7735, Test Loss: 44.7389\n",
      "Epoch [27340/50000], Train Loss: 26.6939, Test Loss: 47.8730\n",
      "Epoch [27345/50000], Train Loss: 34.9187, Test Loss: 45.8125\n",
      "Epoch [27350/50000], Train Loss: 25.8974, Test Loss: 44.9776\n",
      "Epoch [27355/50000], Train Loss: 29.8570, Test Loss: 46.5116\n",
      "Epoch [27360/50000], Train Loss: 20.6210, Test Loss: 45.2397\n",
      "Epoch [27365/50000], Train Loss: 32.4229, Test Loss: 46.8905\n",
      "Epoch [27370/50000], Train Loss: 26.0192, Test Loss: 50.0852\n",
      "Epoch [27375/50000], Train Loss: 24.3950, Test Loss: 44.2988\n",
      "Epoch [27380/50000], Train Loss: 25.7489, Test Loss: 45.2678\n",
      "Epoch [27385/50000], Train Loss: 27.1887, Test Loss: 45.1667\n",
      "Epoch [27390/50000], Train Loss: 28.5430, Test Loss: 48.8448\n",
      "Epoch [27395/50000], Train Loss: 23.3940, Test Loss: 43.7072\n",
      "Epoch [27400/50000], Train Loss: 28.2257, Test Loss: 49.3583\n",
      "Epoch [27405/50000], Train Loss: 29.5225, Test Loss: 45.4403\n",
      "Epoch [27410/50000], Train Loss: 19.9816, Test Loss: 48.5552\n",
      "Epoch [27415/50000], Train Loss: 18.8563, Test Loss: 44.7560\n",
      "Epoch [27420/50000], Train Loss: 24.0836, Test Loss: 45.6935\n",
      "Epoch [27425/50000], Train Loss: 28.4554, Test Loss: 45.8348\n",
      "Epoch [27430/50000], Train Loss: 27.1860, Test Loss: 45.7084\n",
      "Epoch [27435/50000], Train Loss: 30.7230, Test Loss: 46.1191\n",
      "Epoch [27440/50000], Train Loss: 25.8403, Test Loss: 44.2573\n",
      "Epoch [27445/50000], Train Loss: 33.1321, Test Loss: 45.9342\n",
      "Epoch [27450/50000], Train Loss: 21.6764, Test Loss: 46.4964\n",
      "Epoch [27455/50000], Train Loss: 27.2844, Test Loss: 46.8025\n",
      "Epoch [27460/50000], Train Loss: 23.0697, Test Loss: 44.8901\n",
      "Epoch [27465/50000], Train Loss: 29.7319, Test Loss: 45.0268\n",
      "Epoch [27470/50000], Train Loss: 31.6423, Test Loss: 44.9230\n",
      "Epoch [27475/50000], Train Loss: 44.8045, Test Loss: 45.3651\n",
      "Epoch [27480/50000], Train Loss: 21.9309, Test Loss: 48.3243\n",
      "Epoch [27485/50000], Train Loss: 21.6600, Test Loss: 45.5669\n",
      "Epoch [27490/50000], Train Loss: 25.1025, Test Loss: 47.7944\n",
      "Epoch [27495/50000], Train Loss: 22.8975, Test Loss: 45.2155\n",
      "Epoch [27500/50000], Train Loss: 26.1831, Test Loss: 45.5059\n",
      "Epoch [27505/50000], Train Loss: 27.1142, Test Loss: 44.2786\n",
      "Epoch [27510/50000], Train Loss: 26.1950, Test Loss: 45.9946\n",
      "Epoch [27515/50000], Train Loss: 74.3929, Test Loss: 44.6487\n",
      "Epoch [27520/50000], Train Loss: 27.4435, Test Loss: 46.3634\n",
      "Epoch [27525/50000], Train Loss: 31.7276, Test Loss: 46.6496\n",
      "Epoch [27530/50000], Train Loss: 24.7206, Test Loss: 45.4133\n",
      "Epoch [27535/50000], Train Loss: 24.7447, Test Loss: 46.3836\n",
      "Epoch [27540/50000], Train Loss: 72.3815, Test Loss: 51.4733\n",
      "Epoch [27545/50000], Train Loss: 28.3586, Test Loss: 45.6500\n",
      "Epoch [27550/50000], Train Loss: 27.2434, Test Loss: 49.7748\n",
      "Epoch [27555/50000], Train Loss: 26.5953, Test Loss: 47.8478\n",
      "Epoch [27560/50000], Train Loss: 17.2374, Test Loss: 44.7771\n",
      "Epoch [27565/50000], Train Loss: 30.1237, Test Loss: 45.9007\n",
      "Epoch [27570/50000], Train Loss: 21.9760, Test Loss: 50.8067\n",
      "Epoch [27575/50000], Train Loss: 24.6713, Test Loss: 44.3306\n",
      "Epoch [27580/50000], Train Loss: 27.0051, Test Loss: 48.1585\n",
      "Epoch [27585/50000], Train Loss: 27.9785, Test Loss: 54.5431\n",
      "Epoch [27590/50000], Train Loss: 27.8809, Test Loss: 48.1398\n",
      "Epoch [27595/50000], Train Loss: 27.5523, Test Loss: 46.0566\n",
      "Epoch [27600/50000], Train Loss: 26.8866, Test Loss: 47.8459\n",
      "Epoch [27605/50000], Train Loss: 28.2398, Test Loss: 48.2951\n",
      "Epoch [27610/50000], Train Loss: 20.1183, Test Loss: 44.2766\n",
      "Epoch [27615/50000], Train Loss: 32.3369, Test Loss: 48.8297\n",
      "Epoch [27620/50000], Train Loss: 24.2461, Test Loss: 46.1167\n",
      "Epoch [27625/50000], Train Loss: 76.6833, Test Loss: 44.3198\n",
      "Epoch [27630/50000], Train Loss: 27.8769, Test Loss: 47.4595\n",
      "Epoch [27635/50000], Train Loss: 23.8939, Test Loss: 46.2563\n",
      "Epoch [27640/50000], Train Loss: 28.8542, Test Loss: 43.8818\n",
      "Epoch [27645/50000], Train Loss: 29.9477, Test Loss: 48.0239\n",
      "Epoch [27650/50000], Train Loss: 32.9589, Test Loss: 45.1802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27655/50000], Train Loss: 21.3068, Test Loss: 45.3808\n",
      "Epoch [27660/50000], Train Loss: 26.7010, Test Loss: 45.1778\n",
      "Epoch [27665/50000], Train Loss: 25.1972, Test Loss: 57.3286\n",
      "Epoch [27670/50000], Train Loss: 32.8668, Test Loss: 45.4144\n",
      "Epoch [27675/50000], Train Loss: 28.9979, Test Loss: 46.9065\n",
      "Epoch [27680/50000], Train Loss: 24.6848, Test Loss: 46.5399\n",
      "Epoch [27685/50000], Train Loss: 26.2634, Test Loss: 45.0312\n",
      "Epoch [27690/50000], Train Loss: 32.4573, Test Loss: 78.1605\n",
      "Epoch [27695/50000], Train Loss: 30.7095, Test Loss: 47.4496\n",
      "Epoch [27700/50000], Train Loss: 25.8052, Test Loss: 44.1296\n",
      "Epoch [27705/50000], Train Loss: 32.5421, Test Loss: 45.0197\n",
      "Epoch [27710/50000], Train Loss: 19.5209, Test Loss: 45.9624\n",
      "Epoch [27715/50000], Train Loss: 24.4345, Test Loss: 46.8632\n",
      "Epoch [27720/50000], Train Loss: 29.4856, Test Loss: 47.8279\n",
      "Epoch [27725/50000], Train Loss: 25.6425, Test Loss: 46.0409\n",
      "Epoch [27730/50000], Train Loss: 27.2847, Test Loss: 49.1322\n",
      "Epoch [27735/50000], Train Loss: 25.4537, Test Loss: 47.5544\n",
      "Epoch [27740/50000], Train Loss: 22.2420, Test Loss: 47.8052\n",
      "Epoch [27745/50000], Train Loss: 27.3377, Test Loss: 48.1086\n",
      "Epoch [27750/50000], Train Loss: 26.1164, Test Loss: 45.8891\n",
      "Epoch [27755/50000], Train Loss: 25.7847, Test Loss: 45.5037\n",
      "Epoch [27760/50000], Train Loss: 27.0116, Test Loss: 49.9585\n",
      "Epoch [27765/50000], Train Loss: 32.0547, Test Loss: 47.7324\n",
      "Epoch [27770/50000], Train Loss: 31.6849, Test Loss: 47.8381\n",
      "Epoch [27775/50000], Train Loss: 25.5227, Test Loss: 46.6897\n",
      "Epoch [27780/50000], Train Loss: 30.1237, Test Loss: 44.2573\n",
      "Epoch [27785/50000], Train Loss: 24.6710, Test Loss: 58.8824\n",
      "Epoch [27790/50000], Train Loss: 21.3013, Test Loss: 46.1805\n",
      "Epoch [27795/50000], Train Loss: 29.9979, Test Loss: 45.1986\n",
      "Epoch [27800/50000], Train Loss: 27.1759, Test Loss: 56.8326\n",
      "Epoch [27805/50000], Train Loss: 30.0789, Test Loss: 45.4700\n",
      "Epoch [27810/50000], Train Loss: 25.4965, Test Loss: 45.7209\n",
      "Epoch [27815/50000], Train Loss: 73.2769, Test Loss: 63.1166\n",
      "Epoch [27820/50000], Train Loss: 28.3661, Test Loss: 45.5274\n",
      "Epoch [27825/50000], Train Loss: 21.7600, Test Loss: 44.1361\n",
      "Epoch [27830/50000], Train Loss: 28.7394, Test Loss: 45.8202\n",
      "Epoch [27835/50000], Train Loss: 22.5626, Test Loss: 45.6469\n",
      "Epoch [27840/50000], Train Loss: 29.0023, Test Loss: 47.1272\n",
      "Epoch [27845/50000], Train Loss: 21.0346, Test Loss: 44.1389\n",
      "Epoch [27850/50000], Train Loss: 24.7895, Test Loss: 46.8495\n",
      "Epoch [27855/50000], Train Loss: 28.0414, Test Loss: 46.4049\n",
      "Epoch [27860/50000], Train Loss: 24.4540, Test Loss: 44.6676\n",
      "Epoch [27865/50000], Train Loss: 30.7151, Test Loss: 46.4856\n",
      "Epoch [27870/50000], Train Loss: 24.3043, Test Loss: 45.4223\n",
      "Epoch [27875/50000], Train Loss: 29.8996, Test Loss: 47.8538\n",
      "Epoch [27880/50000], Train Loss: 27.5763, Test Loss: 43.7658\n",
      "Epoch [27885/50000], Train Loss: 23.1203, Test Loss: 49.4862\n",
      "Epoch [27890/50000], Train Loss: 25.0102, Test Loss: 47.1116\n",
      "Epoch [27895/50000], Train Loss: 27.9444, Test Loss: 52.1990\n",
      "Epoch [27900/50000], Train Loss: 38.7353, Test Loss: 44.9653\n",
      "Epoch [27905/50000], Train Loss: 27.6190, Test Loss: 48.6598\n",
      "Epoch [27910/50000], Train Loss: 30.9523, Test Loss: 50.8145\n",
      "Epoch [27915/50000], Train Loss: 31.6643, Test Loss: 49.2135\n",
      "Epoch [27920/50000], Train Loss: 17.0219, Test Loss: 43.7336\n",
      "Epoch [27925/50000], Train Loss: 28.7724, Test Loss: 47.4536\n",
      "Epoch [27930/50000], Train Loss: 64.0582, Test Loss: 45.7772\n",
      "Epoch [27935/50000], Train Loss: 22.7766, Test Loss: 45.0937\n",
      "Epoch [27940/50000], Train Loss: 23.1491, Test Loss: 44.2018\n",
      "Epoch [27945/50000], Train Loss: 30.4449, Test Loss: 45.1662\n",
      "Epoch [27950/50000], Train Loss: 26.3512, Test Loss: 54.2936\n",
      "Epoch [27955/50000], Train Loss: 24.6971, Test Loss: 44.4495\n",
      "Epoch [27960/50000], Train Loss: 22.1817, Test Loss: 49.8239\n",
      "Epoch [27965/50000], Train Loss: 34.5431, Test Loss: 53.7502\n",
      "Epoch [27970/50000], Train Loss: 42.6417, Test Loss: 44.8897\n",
      "Epoch [27975/50000], Train Loss: 37.5867, Test Loss: 45.1724\n",
      "Epoch [27980/50000], Train Loss: 19.9806, Test Loss: 45.8612\n",
      "Epoch [27985/50000], Train Loss: 25.3289, Test Loss: 49.2051\n",
      "Epoch [27990/50000], Train Loss: 22.5062, Test Loss: 45.2375\n",
      "Epoch [27995/50000], Train Loss: 26.7795, Test Loss: 46.6088\n",
      "Epoch [28000/50000], Train Loss: 25.6449, Test Loss: 45.3899\n",
      "Epoch [28005/50000], Train Loss: 26.6231, Test Loss: 46.3719\n",
      "Epoch [28010/50000], Train Loss: 26.4954, Test Loss: 45.8840\n",
      "Epoch [28015/50000], Train Loss: 31.2317, Test Loss: 48.3680\n",
      "Epoch [28020/50000], Train Loss: 59.0059, Test Loss: 46.6069\n",
      "Epoch [28025/50000], Train Loss: 30.6521, Test Loss: 49.8268\n",
      "Epoch [28030/50000], Train Loss: 31.1142, Test Loss: 43.7069\n",
      "Epoch [28035/50000], Train Loss: 26.5515, Test Loss: 48.1400\n",
      "Epoch [28040/50000], Train Loss: 26.9068, Test Loss: 48.2025\n",
      "Epoch [28045/50000], Train Loss: 27.7459, Test Loss: 45.1153\n",
      "Epoch [28050/50000], Train Loss: 34.8463, Test Loss: 44.6273\n",
      "Epoch [28055/50000], Train Loss: 26.5685, Test Loss: 45.7895\n",
      "Epoch [28060/50000], Train Loss: 25.2315, Test Loss: 48.6136\n",
      "Epoch [28065/50000], Train Loss: 24.3298, Test Loss: 46.5693\n",
      "Epoch [28070/50000], Train Loss: 33.2136, Test Loss: 47.8201\n",
      "Epoch [28075/50000], Train Loss: 27.7750, Test Loss: 46.9470\n",
      "Epoch [28080/50000], Train Loss: 22.1703, Test Loss: 47.8980\n",
      "Epoch [28085/50000], Train Loss: 30.0270, Test Loss: 51.5497\n",
      "Epoch [28090/50000], Train Loss: 28.1366, Test Loss: 68.1105\n",
      "Epoch [28095/50000], Train Loss: 27.7312, Test Loss: 44.8404\n",
      "Epoch [28100/50000], Train Loss: 55.2727, Test Loss: 45.7020\n",
      "Epoch [28105/50000], Train Loss: 25.0736, Test Loss: 47.6922\n",
      "Epoch [28110/50000], Train Loss: 37.1771, Test Loss: 46.7559\n",
      "Epoch [28115/50000], Train Loss: 27.3458, Test Loss: 44.7873\n",
      "Epoch [28120/50000], Train Loss: 25.1198, Test Loss: 45.5224\n",
      "Epoch [28125/50000], Train Loss: 27.8747, Test Loss: 47.9592\n",
      "Epoch [28130/50000], Train Loss: 25.9242, Test Loss: 46.8219\n",
      "Epoch [28135/50000], Train Loss: 27.9382, Test Loss: 46.2134\n",
      "Epoch [28140/50000], Train Loss: 34.4743, Test Loss: 45.2387\n",
      "Epoch [28145/50000], Train Loss: 22.3053, Test Loss: 44.9201\n",
      "Epoch [28150/50000], Train Loss: 43.4634, Test Loss: 45.0243\n",
      "Epoch [28155/50000], Train Loss: 30.3298, Test Loss: 44.7548\n",
      "Epoch [28160/50000], Train Loss: 26.9886, Test Loss: 48.9992\n",
      "Epoch [28165/50000], Train Loss: 39.3274, Test Loss: 81.1679\n",
      "Epoch [28170/50000], Train Loss: 25.2162, Test Loss: 48.2119\n",
      "Epoch [28175/50000], Train Loss: 39.8984, Test Loss: 46.1952\n",
      "Epoch [28180/50000], Train Loss: 26.7914, Test Loss: 49.0505\n",
      "Epoch [28185/50000], Train Loss: 44.2483, Test Loss: 43.7910\n",
      "Epoch [28190/50000], Train Loss: 26.4651, Test Loss: 44.7586\n",
      "Epoch [28195/50000], Train Loss: 28.4926, Test Loss: 46.4080\n",
      "Epoch [28200/50000], Train Loss: 28.9693, Test Loss: 45.6852\n",
      "Epoch [28205/50000], Train Loss: 24.8816, Test Loss: 45.6353\n",
      "Epoch [28210/50000], Train Loss: 27.2869, Test Loss: 44.8498\n",
      "Epoch [28215/50000], Train Loss: 27.3595, Test Loss: 49.2560\n",
      "Epoch [28220/50000], Train Loss: 21.4334, Test Loss: 44.4685\n",
      "Epoch [28225/50000], Train Loss: 25.0973, Test Loss: 44.7737\n",
      "Epoch [28230/50000], Train Loss: 25.8122, Test Loss: 46.0489\n",
      "Epoch [28235/50000], Train Loss: 29.7465, Test Loss: 46.0262\n",
      "Epoch [28240/50000], Train Loss: 23.9645, Test Loss: 44.2502\n",
      "Epoch [28245/50000], Train Loss: 32.1231, Test Loss: 45.5462\n",
      "Epoch [28250/50000], Train Loss: 22.8758, Test Loss: 47.1923\n",
      "Epoch [28255/50000], Train Loss: 29.2520, Test Loss: 46.3327\n",
      "Epoch [28260/50000], Train Loss: 29.9495, Test Loss: 45.6025\n",
      "Epoch [28265/50000], Train Loss: 24.1693, Test Loss: 45.9896\n",
      "Epoch [28270/50000], Train Loss: 30.2580, Test Loss: 46.0598\n",
      "Epoch [28275/50000], Train Loss: 24.7668, Test Loss: 44.5280\n",
      "Epoch [28280/50000], Train Loss: 27.0244, Test Loss: 44.4367\n",
      "Epoch [28285/50000], Train Loss: 25.5959, Test Loss: 44.9402\n",
      "Epoch [28290/50000], Train Loss: 28.0221, Test Loss: 43.7201\n",
      "Epoch [28295/50000], Train Loss: 27.2350, Test Loss: 49.4778\n",
      "Epoch [28300/50000], Train Loss: 26.9754, Test Loss: 59.7196\n",
      "Epoch [28305/50000], Train Loss: 30.8177, Test Loss: 50.5264\n",
      "Epoch [28310/50000], Train Loss: 68.8944, Test Loss: 43.9152\n",
      "Epoch [28315/50000], Train Loss: 24.5454, Test Loss: 44.1154\n",
      "Epoch [28320/50000], Train Loss: 27.2938, Test Loss: 51.3007\n",
      "Epoch [28325/50000], Train Loss: 32.5206, Test Loss: 45.6078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28330/50000], Train Loss: 30.3362, Test Loss: 46.6664\n",
      "Epoch [28335/50000], Train Loss: 24.3268, Test Loss: 45.6317\n",
      "Epoch [28340/50000], Train Loss: 31.5112, Test Loss: 50.3385\n",
      "Epoch [28345/50000], Train Loss: 30.3585, Test Loss: 48.7919\n",
      "Epoch [28350/50000], Train Loss: 28.9760, Test Loss: 46.5479\n",
      "Epoch [28355/50000], Train Loss: 24.3823, Test Loss: 43.9582\n",
      "Epoch [28360/50000], Train Loss: 17.4592, Test Loss: 45.4438\n",
      "Epoch [28365/50000], Train Loss: 25.0274, Test Loss: 45.4387\n",
      "Epoch [28370/50000], Train Loss: 23.7584, Test Loss: 46.0163\n",
      "Epoch [28375/50000], Train Loss: 24.4479, Test Loss: 44.5393\n",
      "Epoch [28380/50000], Train Loss: 22.2325, Test Loss: 44.4775\n",
      "Epoch [28385/50000], Train Loss: 26.8646, Test Loss: 47.1541\n",
      "Epoch [28390/50000], Train Loss: 28.2666, Test Loss: 44.5004\n",
      "Epoch [28395/50000], Train Loss: 26.7449, Test Loss: 46.0288\n",
      "Epoch [28400/50000], Train Loss: 27.5906, Test Loss: 44.2341\n",
      "Epoch [28405/50000], Train Loss: 35.4914, Test Loss: 46.6652\n",
      "Epoch [28410/50000], Train Loss: 27.4222, Test Loss: 46.1262\n",
      "Epoch [28415/50000], Train Loss: 43.3288, Test Loss: 45.6449\n",
      "Epoch [28420/50000], Train Loss: 29.3546, Test Loss: 45.9659\n",
      "Epoch [28425/50000], Train Loss: 25.1325, Test Loss: 45.2584\n",
      "Epoch [28430/50000], Train Loss: 27.0057, Test Loss: 47.3849\n",
      "Epoch [28435/50000], Train Loss: 25.3868, Test Loss: 46.8418\n",
      "Epoch [28440/50000], Train Loss: 23.8589, Test Loss: 46.0706\n",
      "Epoch [28445/50000], Train Loss: 25.8913, Test Loss: 47.3605\n",
      "Epoch [28450/50000], Train Loss: 18.6518, Test Loss: 45.1823\n",
      "Epoch [28455/50000], Train Loss: 25.5756, Test Loss: 51.3654\n",
      "Epoch [28460/50000], Train Loss: 21.8441, Test Loss: 49.0468\n",
      "Epoch [28465/50000], Train Loss: 28.3083, Test Loss: 49.3251\n",
      "Epoch [28470/50000], Train Loss: 34.5017, Test Loss: 47.8933\n",
      "Epoch [28475/50000], Train Loss: 31.5495, Test Loss: 50.9607\n",
      "Epoch [28480/50000], Train Loss: 28.6160, Test Loss: 46.1577\n",
      "Epoch [28485/50000], Train Loss: 27.0688, Test Loss: 44.8784\n",
      "Epoch [28490/50000], Train Loss: 21.5989, Test Loss: 45.5809\n",
      "Epoch [28495/50000], Train Loss: 48.3622, Test Loss: 44.6740\n",
      "Epoch [28500/50000], Train Loss: 28.4283, Test Loss: 44.5634\n",
      "Epoch [28505/50000], Train Loss: 25.5138, Test Loss: 45.7784\n",
      "Epoch [28510/50000], Train Loss: 29.2243, Test Loss: 48.3069\n",
      "Epoch [28515/50000], Train Loss: 23.0384, Test Loss: 45.7345\n",
      "Epoch [28520/50000], Train Loss: 28.4627, Test Loss: 46.7222\n",
      "Epoch [28525/50000], Train Loss: 24.7144, Test Loss: 44.4221\n",
      "Epoch [28530/50000], Train Loss: 23.6089, Test Loss: 44.2058\n",
      "Epoch [28535/50000], Train Loss: 29.2300, Test Loss: 47.5069\n",
      "Epoch [28540/50000], Train Loss: 34.6638, Test Loss: 50.0494\n",
      "Epoch [28545/50000], Train Loss: 27.5239, Test Loss: 48.2632\n",
      "Epoch [28550/50000], Train Loss: 60.4625, Test Loss: 74.6969\n",
      "Epoch [28555/50000], Train Loss: 21.8955, Test Loss: 46.1751\n",
      "Epoch [28560/50000], Train Loss: 25.3053, Test Loss: 45.0153\n",
      "Epoch [28565/50000], Train Loss: 29.0550, Test Loss: 46.8017\n",
      "Epoch [28570/50000], Train Loss: 29.9306, Test Loss: 48.9632\n",
      "Epoch [28575/50000], Train Loss: 23.7233, Test Loss: 48.6473\n",
      "Epoch [28580/50000], Train Loss: 25.9616, Test Loss: 45.9289\n",
      "Epoch [28585/50000], Train Loss: 30.1481, Test Loss: 48.7727\n",
      "Epoch [28590/50000], Train Loss: 29.5372, Test Loss: 43.6825\n",
      "Epoch [28595/50000], Train Loss: 19.6891, Test Loss: 44.6177\n",
      "Epoch [28600/50000], Train Loss: 24.2944, Test Loss: 46.8749\n",
      "Epoch [28605/50000], Train Loss: 29.1639, Test Loss: 47.6735\n",
      "Epoch [28610/50000], Train Loss: 29.1884, Test Loss: 53.5830\n",
      "Epoch [28615/50000], Train Loss: 24.7395, Test Loss: 43.7800\n",
      "Epoch [28620/50000], Train Loss: 21.7507, Test Loss: 45.5114\n",
      "Epoch [28625/50000], Train Loss: 28.8620, Test Loss: 47.9377\n",
      "Epoch [28630/50000], Train Loss: 26.7039, Test Loss: 46.6328\n",
      "Epoch [28635/50000], Train Loss: 32.7226, Test Loss: 46.7444\n",
      "Epoch [28640/50000], Train Loss: 24.7447, Test Loss: 46.2615\n",
      "Epoch [28645/50000], Train Loss: 17.6867, Test Loss: 43.4068\n",
      "Epoch [28650/50000], Train Loss: 24.6911, Test Loss: 47.7124\n",
      "Epoch [28655/50000], Train Loss: 31.4942, Test Loss: 45.4349\n",
      "Epoch [28660/50000], Train Loss: 26.4966, Test Loss: 48.3199\n",
      "Epoch [28665/50000], Train Loss: 25.8720, Test Loss: 45.8556\n",
      "Epoch [28670/50000], Train Loss: 26.9539, Test Loss: 46.4226\n",
      "Epoch [28675/50000], Train Loss: 52.7943, Test Loss: 48.7402\n",
      "Epoch [28680/50000], Train Loss: 24.6548, Test Loss: 48.2356\n",
      "Epoch [28685/50000], Train Loss: 30.7561, Test Loss: 46.6492\n",
      "Epoch [28690/50000], Train Loss: 26.5626, Test Loss: 44.4406\n",
      "Epoch [28695/50000], Train Loss: 49.6973, Test Loss: 44.2905\n",
      "Epoch [28700/50000], Train Loss: 23.0152, Test Loss: 44.2593\n",
      "Epoch [28705/50000], Train Loss: 48.5613, Test Loss: 47.1289\n",
      "Epoch [28710/50000], Train Loss: 28.7429, Test Loss: 46.3530\n",
      "Epoch [28715/50000], Train Loss: 26.2379, Test Loss: 46.3057\n",
      "Epoch [28720/50000], Train Loss: 21.1954, Test Loss: 46.6974\n",
      "Epoch [28725/50000], Train Loss: 32.1793, Test Loss: 44.8930\n",
      "Epoch [28730/50000], Train Loss: 30.7376, Test Loss: 44.5906\n",
      "Epoch [28735/50000], Train Loss: 27.1618, Test Loss: 51.5538\n",
      "Epoch [28740/50000], Train Loss: 24.4309, Test Loss: 44.1891\n",
      "Epoch [28745/50000], Train Loss: 37.9202, Test Loss: 51.0316\n",
      "Epoch [28750/50000], Train Loss: 25.2057, Test Loss: 44.3796\n",
      "Epoch [28755/50000], Train Loss: 28.2196, Test Loss: 44.8261\n",
      "Epoch [28760/50000], Train Loss: 23.1536, Test Loss: 44.3277\n",
      "Epoch [28765/50000], Train Loss: 27.9679, Test Loss: 43.3461\n",
      "Epoch [28770/50000], Train Loss: 29.5294, Test Loss: 45.6504\n",
      "Epoch [28775/50000], Train Loss: 24.4862, Test Loss: 48.5149\n",
      "Epoch [28780/50000], Train Loss: 27.3533, Test Loss: 44.8991\n",
      "Epoch [28785/50000], Train Loss: 23.8314, Test Loss: 44.3717\n",
      "Epoch [28790/50000], Train Loss: 22.6745, Test Loss: 48.3278\n",
      "Epoch [28795/50000], Train Loss: 26.4778, Test Loss: 46.5669\n",
      "Epoch [28800/50000], Train Loss: 32.6150, Test Loss: 44.8796\n",
      "Epoch [28805/50000], Train Loss: 23.2050, Test Loss: 47.0394\n",
      "Epoch [28810/50000], Train Loss: 53.3910, Test Loss: 63.1407\n",
      "Epoch [28815/50000], Train Loss: 24.2736, Test Loss: 48.7000\n",
      "Epoch [28820/50000], Train Loss: 22.7629, Test Loss: 44.4709\n",
      "Epoch [28825/50000], Train Loss: 24.7805, Test Loss: 44.5669\n",
      "Epoch [28830/50000], Train Loss: 23.3747, Test Loss: 45.1115\n",
      "Epoch [28835/50000], Train Loss: 23.2542, Test Loss: 43.3931\n",
      "Epoch [28840/50000], Train Loss: 32.4015, Test Loss: 49.1207\n",
      "Epoch [28845/50000], Train Loss: 27.0713, Test Loss: 47.8506\n",
      "Epoch [28850/50000], Train Loss: 27.8377, Test Loss: 46.8032\n",
      "Epoch [28855/50000], Train Loss: 31.8946, Test Loss: 48.1910\n",
      "Epoch [28860/50000], Train Loss: 21.0518, Test Loss: 54.6940\n",
      "Epoch [28865/50000], Train Loss: 27.2839, Test Loss: 47.7492\n",
      "Epoch [28870/50000], Train Loss: 24.4643, Test Loss: 45.3618\n",
      "Epoch [28875/50000], Train Loss: 22.8432, Test Loss: 44.6345\n",
      "Epoch [28880/50000], Train Loss: 68.4303, Test Loss: 45.2029\n",
      "Epoch [28885/50000], Train Loss: 29.7647, Test Loss: 43.7156\n",
      "Epoch [28890/50000], Train Loss: 26.4704, Test Loss: 51.9262\n",
      "Epoch [28895/50000], Train Loss: 28.4855, Test Loss: 55.8640\n",
      "Epoch [28900/50000], Train Loss: 24.9500, Test Loss: 48.5207\n",
      "Epoch [28905/50000], Train Loss: 39.7074, Test Loss: 47.5487\n",
      "Epoch [28910/50000], Train Loss: 32.9200, Test Loss: 44.6802\n",
      "Epoch [28915/50000], Train Loss: 28.7510, Test Loss: 46.4995\n",
      "Epoch [28920/50000], Train Loss: 23.6254, Test Loss: 45.8877\n",
      "Epoch [28925/50000], Train Loss: 24.2373, Test Loss: 47.3053\n",
      "Epoch [28930/50000], Train Loss: 32.6816, Test Loss: 54.7539\n",
      "Epoch [28935/50000], Train Loss: 24.4536, Test Loss: 49.7699\n",
      "Epoch [28940/50000], Train Loss: 22.9341, Test Loss: 45.6794\n",
      "Epoch [28945/50000], Train Loss: 29.8488, Test Loss: 45.2581\n",
      "Epoch [28950/50000], Train Loss: 26.7426, Test Loss: 44.5742\n",
      "Epoch [28955/50000], Train Loss: 24.9111, Test Loss: 46.8546\n",
      "Epoch [28960/50000], Train Loss: 23.8705, Test Loss: 43.9967\n",
      "Epoch [28965/50000], Train Loss: 24.9047, Test Loss: 46.1626\n",
      "Epoch [28970/50000], Train Loss: 24.8158, Test Loss: 45.5227\n",
      "Epoch [28975/50000], Train Loss: 26.8023, Test Loss: 45.5091\n",
      "Epoch [28980/50000], Train Loss: 32.0254, Test Loss: 43.5553\n",
      "Epoch [28985/50000], Train Loss: 28.7195, Test Loss: 55.6304\n",
      "Epoch [28990/50000], Train Loss: 28.5004, Test Loss: 49.2318\n",
      "Epoch [28995/50000], Train Loss: 29.6795, Test Loss: 45.3759\n",
      "Epoch [29000/50000], Train Loss: 22.1286, Test Loss: 45.2296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29005/50000], Train Loss: 22.3294, Test Loss: 45.1024\n",
      "Epoch [29010/50000], Train Loss: 22.2702, Test Loss: 44.5409\n",
      "Epoch [29015/50000], Train Loss: 24.6346, Test Loss: 55.0775\n",
      "Epoch [29020/50000], Train Loss: 53.6421, Test Loss: 43.7771\n",
      "Epoch [29025/50000], Train Loss: 30.9865, Test Loss: 48.0213\n",
      "Epoch [29030/50000], Train Loss: 26.4961, Test Loss: 43.9507\n",
      "Epoch [29035/50000], Train Loss: 29.9112, Test Loss: 44.9969\n",
      "Epoch [29040/50000], Train Loss: 25.9014, Test Loss: 45.6482\n",
      "Epoch [29045/50000], Train Loss: 27.5777, Test Loss: 45.8426\n",
      "Epoch [29050/50000], Train Loss: 30.2277, Test Loss: 52.4456\n",
      "Epoch [29055/50000], Train Loss: 30.5709, Test Loss: 44.5664\n",
      "Epoch [29060/50000], Train Loss: 79.4492, Test Loss: 45.0095\n",
      "Epoch [29065/50000], Train Loss: 26.8331, Test Loss: 44.3419\n",
      "Epoch [29070/50000], Train Loss: 24.0353, Test Loss: 44.3297\n",
      "Epoch [29075/50000], Train Loss: 27.0085, Test Loss: 46.8065\n",
      "Epoch [29080/50000], Train Loss: 22.4309, Test Loss: 45.6540\n",
      "Epoch [29085/50000], Train Loss: 24.9946, Test Loss: 45.0179\n",
      "Epoch [29090/50000], Train Loss: 28.4063, Test Loss: 45.4506\n",
      "Epoch [29095/50000], Train Loss: 22.0902, Test Loss: 48.9026\n",
      "Epoch [29100/50000], Train Loss: 33.0985, Test Loss: 49.8241\n",
      "Epoch [29105/50000], Train Loss: 19.6034, Test Loss: 45.1151\n",
      "Epoch [29110/50000], Train Loss: 25.0626, Test Loss: 48.9668\n",
      "Epoch [29115/50000], Train Loss: 34.9666, Test Loss: 45.3424\n",
      "Epoch [29120/50000], Train Loss: 30.1449, Test Loss: 45.8153\n",
      "Epoch [29125/50000], Train Loss: 25.0992, Test Loss: 47.6251\n",
      "Epoch [29130/50000], Train Loss: 26.5722, Test Loss: 47.2665\n",
      "Epoch [29135/50000], Train Loss: 29.9683, Test Loss: 50.4635\n",
      "Epoch [29140/50000], Train Loss: 26.0142, Test Loss: 45.1266\n",
      "Epoch [29145/50000], Train Loss: 26.7178, Test Loss: 45.1327\n",
      "Epoch [29150/50000], Train Loss: 31.1791, Test Loss: 44.3085\n",
      "Epoch [29155/50000], Train Loss: 25.0854, Test Loss: 54.0227\n",
      "Epoch [29160/50000], Train Loss: 26.0441, Test Loss: 43.7059\n",
      "Epoch [29165/50000], Train Loss: 25.8056, Test Loss: 52.3363\n",
      "Epoch [29170/50000], Train Loss: 31.0355, Test Loss: 46.7965\n",
      "Epoch [29175/50000], Train Loss: 24.8152, Test Loss: 49.0998\n",
      "Epoch [29180/50000], Train Loss: 19.4473, Test Loss: 45.3280\n",
      "Epoch [29185/50000], Train Loss: 35.4214, Test Loss: 44.0691\n",
      "Epoch [29190/50000], Train Loss: 23.4693, Test Loss: 51.2146\n",
      "Epoch [29195/50000], Train Loss: 23.2223, Test Loss: 50.1910\n",
      "Epoch [29200/50000], Train Loss: 28.6884, Test Loss: 47.2095\n",
      "Epoch [29205/50000], Train Loss: 24.9536, Test Loss: 47.9433\n",
      "Epoch [29210/50000], Train Loss: 25.6808, Test Loss: 46.2467\n",
      "Epoch [29215/50000], Train Loss: 23.3823, Test Loss: 46.5878\n",
      "Epoch [29220/50000], Train Loss: 49.8901, Test Loss: 44.2556\n",
      "Epoch [29225/50000], Train Loss: 27.3253, Test Loss: 45.0426\n",
      "Epoch [29230/50000], Train Loss: 48.4802, Test Loss: 48.6958\n",
      "Epoch [29235/50000], Train Loss: 23.1756, Test Loss: 46.6148\n",
      "Epoch [29240/50000], Train Loss: 46.3098, Test Loss: 51.0629\n",
      "Epoch [29245/50000], Train Loss: 30.9251, Test Loss: 45.5597\n",
      "Epoch [29250/50000], Train Loss: 28.4560, Test Loss: 44.6797\n",
      "Epoch [29255/50000], Train Loss: 26.7489, Test Loss: 47.5169\n",
      "Epoch [29260/50000], Train Loss: 23.8028, Test Loss: 48.7474\n",
      "Epoch [29265/50000], Train Loss: 23.9611, Test Loss: 46.1757\n",
      "Epoch [29270/50000], Train Loss: 27.0744, Test Loss: 44.7315\n",
      "Epoch [29275/50000], Train Loss: 18.8212, Test Loss: 44.9387\n",
      "Epoch [29280/50000], Train Loss: 26.2197, Test Loss: 46.8386\n",
      "Epoch [29285/50000], Train Loss: 28.5538, Test Loss: 45.7729\n",
      "Epoch [29290/50000], Train Loss: 23.4728, Test Loss: 47.2469\n",
      "Epoch [29295/50000], Train Loss: 68.1574, Test Loss: 45.0573\n",
      "Epoch [29300/50000], Train Loss: 25.5934, Test Loss: 44.7393\n",
      "Epoch [29305/50000], Train Loss: 26.4043, Test Loss: 43.8109\n",
      "Epoch [29310/50000], Train Loss: 35.2678, Test Loss: 54.0011\n",
      "Epoch [29315/50000], Train Loss: 39.9593, Test Loss: 43.9162\n",
      "Epoch [29320/50000], Train Loss: 24.0608, Test Loss: 43.9563\n",
      "Epoch [29325/50000], Train Loss: 39.3418, Test Loss: 44.9106\n",
      "Epoch [29330/50000], Train Loss: 21.9871, Test Loss: 46.4948\n",
      "Epoch [29335/50000], Train Loss: 20.9110, Test Loss: 46.9589\n",
      "Epoch [29340/50000], Train Loss: 29.9008, Test Loss: 44.3653\n",
      "Epoch [29345/50000], Train Loss: 20.5905, Test Loss: 44.3823\n",
      "Epoch [29350/50000], Train Loss: 32.8947, Test Loss: 46.3790\n",
      "Epoch [29355/50000], Train Loss: 25.1803, Test Loss: 49.0221\n",
      "Epoch [29360/50000], Train Loss: 27.0290, Test Loss: 46.3946\n",
      "Epoch [29365/50000], Train Loss: 21.4451, Test Loss: 45.2160\n",
      "Epoch [29370/50000], Train Loss: 24.0473, Test Loss: 44.5374\n",
      "Epoch [29375/50000], Train Loss: 29.6847, Test Loss: 60.2358\n",
      "Epoch [29380/50000], Train Loss: 22.8363, Test Loss: 43.5171\n",
      "Epoch [29385/50000], Train Loss: 18.9965, Test Loss: 45.7617\n",
      "Epoch [29390/50000], Train Loss: 31.9822, Test Loss: 45.8238\n",
      "Epoch [29395/50000], Train Loss: 37.0606, Test Loss: 43.4153\n",
      "Epoch [29400/50000], Train Loss: 24.9297, Test Loss: 48.0158\n",
      "Epoch [29405/50000], Train Loss: 21.4047, Test Loss: 44.8840\n",
      "Epoch [29410/50000], Train Loss: 28.5288, Test Loss: 46.0507\n",
      "Epoch [29415/50000], Train Loss: 35.4136, Test Loss: 44.1658\n",
      "Epoch [29420/50000], Train Loss: 22.3606, Test Loss: 47.1874\n",
      "Epoch [29425/50000], Train Loss: 24.4699, Test Loss: 46.3340\n",
      "Epoch [29430/50000], Train Loss: 23.6461, Test Loss: 44.4853\n",
      "Epoch [29435/50000], Train Loss: 31.1833, Test Loss: 43.9597\n",
      "Epoch [29440/50000], Train Loss: 22.2008, Test Loss: 46.7817\n",
      "Epoch [29445/50000], Train Loss: 25.8225, Test Loss: 50.0206\n",
      "Epoch [29450/50000], Train Loss: 22.4525, Test Loss: 44.7539\n",
      "Epoch [29455/50000], Train Loss: 24.8575, Test Loss: 43.7501\n",
      "Epoch [29460/50000], Train Loss: 25.0675, Test Loss: 44.5584\n",
      "Epoch [29465/50000], Train Loss: 28.8476, Test Loss: 46.5650\n",
      "Epoch [29470/50000], Train Loss: 23.8946, Test Loss: 47.5602\n",
      "Epoch [29475/50000], Train Loss: 25.9787, Test Loss: 45.9855\n",
      "Epoch [29480/50000], Train Loss: 24.9398, Test Loss: 47.9785\n",
      "Epoch [29485/50000], Train Loss: 27.8975, Test Loss: 43.5983\n",
      "Epoch [29490/50000], Train Loss: 34.9893, Test Loss: 44.7462\n",
      "Epoch [29495/50000], Train Loss: 22.7494, Test Loss: 44.7323\n",
      "Epoch [29500/50000], Train Loss: 30.6742, Test Loss: 49.5817\n",
      "Epoch [29505/50000], Train Loss: 30.4853, Test Loss: 45.1622\n",
      "Epoch [29510/50000], Train Loss: 26.2741, Test Loss: 46.1419\n",
      "Epoch [29515/50000], Train Loss: 19.5340, Test Loss: 43.4798\n",
      "Epoch [29520/50000], Train Loss: 28.8224, Test Loss: 53.6782\n",
      "Epoch [29525/50000], Train Loss: 26.4627, Test Loss: 45.8326\n",
      "Epoch [29530/50000], Train Loss: 18.7034, Test Loss: 44.2046\n",
      "Epoch [29535/50000], Train Loss: 38.9818, Test Loss: 45.0962\n",
      "Epoch [29540/50000], Train Loss: 24.3577, Test Loss: 45.4612\n",
      "Epoch [29545/50000], Train Loss: 58.6729, Test Loss: 43.9259\n",
      "Epoch [29550/50000], Train Loss: 40.8348, Test Loss: 46.5551\n",
      "Epoch [29555/50000], Train Loss: 20.7882, Test Loss: 49.1416\n",
      "Epoch [29560/50000], Train Loss: 44.5797, Test Loss: 84.2465\n",
      "Epoch [29565/50000], Train Loss: 31.4254, Test Loss: 44.1113\n",
      "Epoch [29570/50000], Train Loss: 18.1539, Test Loss: 44.7690\n",
      "Epoch [29575/50000], Train Loss: 29.1577, Test Loss: 43.7518\n",
      "Epoch [29580/50000], Train Loss: 21.8703, Test Loss: 47.7954\n",
      "Epoch [29585/50000], Train Loss: 25.7256, Test Loss: 45.0985\n",
      "Epoch [29590/50000], Train Loss: 21.7051, Test Loss: 45.1486\n",
      "Epoch [29595/50000], Train Loss: 29.6595, Test Loss: 80.8409\n",
      "Epoch [29600/50000], Train Loss: 28.6569, Test Loss: 45.4721\n",
      "Epoch [29605/50000], Train Loss: 22.9859, Test Loss: 44.9571\n",
      "Epoch [29610/50000], Train Loss: 22.4936, Test Loss: 43.6561\n",
      "Epoch [29615/50000], Train Loss: 25.7714, Test Loss: 48.8316\n",
      "Epoch [29620/50000], Train Loss: 28.4880, Test Loss: 45.1601\n",
      "Epoch [29625/50000], Train Loss: 28.1543, Test Loss: 57.4658\n",
      "Epoch [29630/50000], Train Loss: 24.7679, Test Loss: 46.5301\n",
      "Epoch [29635/50000], Train Loss: 22.8143, Test Loss: 45.2831\n",
      "Epoch [29640/50000], Train Loss: 29.4244, Test Loss: 45.5676\n",
      "Epoch [29645/50000], Train Loss: 23.6383, Test Loss: 44.8658\n",
      "Epoch [29650/50000], Train Loss: 26.0073, Test Loss: 48.0404\n",
      "Epoch [29655/50000], Train Loss: 34.0895, Test Loss: 45.9138\n",
      "Epoch [29660/50000], Train Loss: 32.0732, Test Loss: 51.4591\n",
      "Epoch [29665/50000], Train Loss: 30.5760, Test Loss: 44.0654\n",
      "Epoch [29670/50000], Train Loss: 25.9108, Test Loss: 45.8090\n",
      "Epoch [29675/50000], Train Loss: 22.6733, Test Loss: 50.7536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29680/50000], Train Loss: 25.3987, Test Loss: 48.5310\n",
      "Epoch [29685/50000], Train Loss: 25.3013, Test Loss: 44.6022\n",
      "Epoch [29690/50000], Train Loss: 37.0503, Test Loss: 45.2441\n",
      "Epoch [29695/50000], Train Loss: 17.7208, Test Loss: 46.3697\n",
      "Epoch [29700/50000], Train Loss: 28.4545, Test Loss: 47.4454\n",
      "Epoch [29705/50000], Train Loss: 30.8907, Test Loss: 45.4481\n",
      "Epoch [29710/50000], Train Loss: 54.5186, Test Loss: 57.1652\n",
      "Epoch [29715/50000], Train Loss: 25.2441, Test Loss: 50.5469\n",
      "Epoch [29720/50000], Train Loss: 25.9030, Test Loss: 46.0521\n",
      "Epoch [29725/50000], Train Loss: 27.8197, Test Loss: 44.8569\n",
      "Epoch [29730/50000], Train Loss: 27.4282, Test Loss: 49.6036\n",
      "Epoch [29735/50000], Train Loss: 24.4559, Test Loss: 45.1863\n",
      "Epoch [29740/50000], Train Loss: 36.0696, Test Loss: 49.1189\n",
      "Epoch [29745/50000], Train Loss: 25.7560, Test Loss: 46.1626\n",
      "Epoch [29750/50000], Train Loss: 30.1591, Test Loss: 45.1536\n",
      "Epoch [29755/50000], Train Loss: 24.0791, Test Loss: 48.1482\n",
      "Epoch [29760/50000], Train Loss: 21.7655, Test Loss: 45.3753\n",
      "Epoch [29765/50000], Train Loss: 28.1467, Test Loss: 48.0822\n",
      "Epoch [29770/50000], Train Loss: 23.4265, Test Loss: 45.6819\n",
      "Epoch [29775/50000], Train Loss: 24.9220, Test Loss: 46.7250\n",
      "Epoch [29780/50000], Train Loss: 44.4539, Test Loss: 42.9427\n",
      "Epoch [29785/50000], Train Loss: 27.0106, Test Loss: 47.7884\n",
      "Epoch [29790/50000], Train Loss: 28.4293, Test Loss: 47.1160\n",
      "Epoch [29795/50000], Train Loss: 21.8160, Test Loss: 49.4161\n",
      "Epoch [29800/50000], Train Loss: 26.4519, Test Loss: 43.7726\n",
      "Epoch [29805/50000], Train Loss: 21.5614, Test Loss: 45.3304\n",
      "Epoch [29810/50000], Train Loss: 27.4978, Test Loss: 45.4706\n",
      "Epoch [29815/50000], Train Loss: 26.0413, Test Loss: 46.5936\n",
      "Epoch [29820/50000], Train Loss: 26.9341, Test Loss: 46.1914\n",
      "Epoch [29825/50000], Train Loss: 31.8956, Test Loss: 51.7721\n",
      "Epoch [29830/50000], Train Loss: 20.5638, Test Loss: 46.7843\n",
      "Epoch [29835/50000], Train Loss: 33.8596, Test Loss: 45.4329\n",
      "Epoch [29840/50000], Train Loss: 28.1695, Test Loss: 48.2295\n",
      "Epoch [29845/50000], Train Loss: 22.5227, Test Loss: 46.1051\n",
      "Epoch [29850/50000], Train Loss: 26.2654, Test Loss: 44.8657\n",
      "Epoch [29855/50000], Train Loss: 23.9056, Test Loss: 45.5326\n",
      "Epoch [29860/50000], Train Loss: 25.3511, Test Loss: 50.1042\n",
      "Epoch [29865/50000], Train Loss: 23.3738, Test Loss: 48.6420\n",
      "Epoch [29870/50000], Train Loss: 27.1769, Test Loss: 45.2286\n",
      "Epoch [29875/50000], Train Loss: 25.6666, Test Loss: 49.2387\n",
      "Epoch [29880/50000], Train Loss: 34.2055, Test Loss: 45.0319\n",
      "Epoch [29885/50000], Train Loss: 26.9835, Test Loss: 56.7507\n",
      "Epoch [29890/50000], Train Loss: 28.8698, Test Loss: 45.5825\n",
      "Epoch [29895/50000], Train Loss: 25.7628, Test Loss: 48.3178\n",
      "Epoch [29900/50000], Train Loss: 28.1270, Test Loss: 51.3841\n",
      "Epoch [29905/50000], Train Loss: 35.0475, Test Loss: 44.4731\n",
      "Epoch [29910/50000], Train Loss: 27.2565, Test Loss: 46.3178\n",
      "Epoch [29915/50000], Train Loss: 22.4054, Test Loss: 44.2213\n",
      "Epoch [29920/50000], Train Loss: 26.5039, Test Loss: 46.5586\n",
      "Epoch [29925/50000], Train Loss: 20.9500, Test Loss: 43.8295\n",
      "Epoch [29930/50000], Train Loss: 26.5296, Test Loss: 46.0645\n",
      "Epoch [29935/50000], Train Loss: 32.4456, Test Loss: 44.9812\n",
      "Epoch [29940/50000], Train Loss: 20.8014, Test Loss: 54.7970\n",
      "Epoch [29945/50000], Train Loss: 24.4874, Test Loss: 46.1683\n",
      "Epoch [29950/50000], Train Loss: 31.2318, Test Loss: 50.6698\n",
      "Epoch [29955/50000], Train Loss: 23.4062, Test Loss: 47.9551\n",
      "Epoch [29960/50000], Train Loss: 26.9195, Test Loss: 44.5648\n",
      "Epoch [29965/50000], Train Loss: 18.5183, Test Loss: 47.3998\n",
      "Epoch [29970/50000], Train Loss: 34.1030, Test Loss: 44.9953\n",
      "Epoch [29975/50000], Train Loss: 25.3722, Test Loss: 46.0103\n",
      "Epoch [29980/50000], Train Loss: 40.5960, Test Loss: 48.5237\n",
      "Epoch [29985/50000], Train Loss: 29.2740, Test Loss: 46.3803\n",
      "Epoch [29990/50000], Train Loss: 23.8953, Test Loss: 44.2577\n",
      "Epoch [29995/50000], Train Loss: 30.9384, Test Loss: 43.9847\n",
      "Epoch [30000/50000], Train Loss: 29.8692, Test Loss: 44.8594\n",
      "Epoch [30005/50000], Train Loss: 21.2968, Test Loss: 48.4863\n",
      "Epoch [30010/50000], Train Loss: 40.6414, Test Loss: 46.0069\n",
      "Epoch [30015/50000], Train Loss: 27.9481, Test Loss: 45.9499\n",
      "Epoch [30020/50000], Train Loss: 24.1848, Test Loss: 48.8915\n",
      "Epoch [30025/50000], Train Loss: 27.7844, Test Loss: 45.2507\n",
      "Epoch [30030/50000], Train Loss: 36.1370, Test Loss: 46.4794\n",
      "Epoch [30035/50000], Train Loss: 23.5134, Test Loss: 47.5069\n",
      "Epoch [30040/50000], Train Loss: 33.5426, Test Loss: 47.8861\n",
      "Epoch [30045/50000], Train Loss: 34.1688, Test Loss: 46.8024\n",
      "Epoch [30050/50000], Train Loss: 33.6259, Test Loss: 45.2281\n",
      "Epoch [30055/50000], Train Loss: 25.9010, Test Loss: 45.6386\n",
      "Epoch [30060/50000], Train Loss: 27.8051, Test Loss: 45.7987\n",
      "Epoch [30065/50000], Train Loss: 29.9706, Test Loss: 45.3519\n",
      "Epoch [30070/50000], Train Loss: 26.8567, Test Loss: 46.5822\n",
      "Epoch [30075/50000], Train Loss: 24.8659, Test Loss: 48.3777\n",
      "Epoch [30080/50000], Train Loss: 41.9909, Test Loss: 46.4415\n",
      "Epoch [30085/50000], Train Loss: 25.9844, Test Loss: 47.0349\n",
      "Epoch [30090/50000], Train Loss: 30.0121, Test Loss: 45.3117\n",
      "Epoch [30095/50000], Train Loss: 25.3509, Test Loss: 47.3166\n",
      "Epoch [30100/50000], Train Loss: 28.9662, Test Loss: 48.4225\n",
      "Epoch [30105/50000], Train Loss: 27.5682, Test Loss: 43.7332\n",
      "Epoch [30110/50000], Train Loss: 43.5176, Test Loss: 45.9802\n",
      "Epoch [30115/50000], Train Loss: 21.5941, Test Loss: 44.0371\n",
      "Epoch [30120/50000], Train Loss: 32.0803, Test Loss: 48.3759\n",
      "Epoch [30125/50000], Train Loss: 27.3789, Test Loss: 48.8300\n",
      "Epoch [30130/50000], Train Loss: 26.5861, Test Loss: 45.0549\n",
      "Epoch [30135/50000], Train Loss: 27.0456, Test Loss: 49.5710\n",
      "Epoch [30140/50000], Train Loss: 23.1221, Test Loss: 45.4499\n",
      "Epoch [30145/50000], Train Loss: 22.4514, Test Loss: 48.4609\n",
      "Epoch [30150/50000], Train Loss: 30.0969, Test Loss: 53.9134\n",
      "Epoch [30155/50000], Train Loss: 24.8506, Test Loss: 46.7435\n",
      "Epoch [30160/50000], Train Loss: 31.8928, Test Loss: 47.2886\n",
      "Epoch [30165/50000], Train Loss: 20.3167, Test Loss: 43.5481\n",
      "Epoch [30170/50000], Train Loss: 28.1227, Test Loss: 46.0576\n",
      "Epoch [30175/50000], Train Loss: 23.8559, Test Loss: 47.0391\n",
      "Epoch [30180/50000], Train Loss: 25.0709, Test Loss: 43.9383\n",
      "Epoch [30185/50000], Train Loss: 26.3334, Test Loss: 45.4986\n",
      "Epoch [30190/50000], Train Loss: 25.2155, Test Loss: 46.1846\n",
      "Epoch [30195/50000], Train Loss: 28.0463, Test Loss: 44.0665\n",
      "Epoch [30200/50000], Train Loss: 17.4425, Test Loss: 43.6278\n",
      "Epoch [30205/50000], Train Loss: 28.5551, Test Loss: 45.5539\n",
      "Epoch [30210/50000], Train Loss: 24.7035, Test Loss: 44.5989\n",
      "Epoch [30215/50000], Train Loss: 19.2935, Test Loss: 44.7482\n",
      "Epoch [30220/50000], Train Loss: 28.1786, Test Loss: 59.6161\n",
      "Epoch [30225/50000], Train Loss: 30.3342, Test Loss: 45.8428\n",
      "Epoch [30230/50000], Train Loss: 22.9495, Test Loss: 44.8313\n",
      "Epoch [30235/50000], Train Loss: 28.3267, Test Loss: 44.3541\n",
      "Epoch [30240/50000], Train Loss: 25.6256, Test Loss: 44.3739\n",
      "Epoch [30245/50000], Train Loss: 35.0246, Test Loss: 44.3261\n",
      "Epoch [30250/50000], Train Loss: 29.6907, Test Loss: 46.4495\n",
      "Epoch [30255/50000], Train Loss: 50.4561, Test Loss: 54.2360\n",
      "Epoch [30260/50000], Train Loss: 29.3538, Test Loss: 43.1869\n",
      "Epoch [30265/50000], Train Loss: 24.4448, Test Loss: 46.8073\n",
      "Epoch [30270/50000], Train Loss: 25.9570, Test Loss: 48.4357\n",
      "Epoch [30275/50000], Train Loss: 23.0953, Test Loss: 51.7001\n",
      "Epoch [30280/50000], Train Loss: 44.1522, Test Loss: 55.8808\n",
      "Epoch [30285/50000], Train Loss: 20.6507, Test Loss: 43.7432\n",
      "Epoch [30290/50000], Train Loss: 38.3250, Test Loss: 44.3972\n",
      "Epoch [30295/50000], Train Loss: 26.6177, Test Loss: 45.5683\n",
      "Epoch [30300/50000], Train Loss: 50.6187, Test Loss: 45.6641\n",
      "Epoch [30305/50000], Train Loss: 30.3758, Test Loss: 43.8936\n",
      "Epoch [30310/50000], Train Loss: 24.9148, Test Loss: 44.4064\n",
      "Epoch [30315/50000], Train Loss: 22.3717, Test Loss: 44.4804\n",
      "Epoch [30320/50000], Train Loss: 22.9166, Test Loss: 47.2878\n",
      "Epoch [30325/50000], Train Loss: 23.1443, Test Loss: 47.4258\n",
      "Epoch [30330/50000], Train Loss: 28.0735, Test Loss: 44.7802\n",
      "Epoch [30335/50000], Train Loss: 27.9388, Test Loss: 45.3174\n",
      "Epoch [30340/50000], Train Loss: 63.2186, Test Loss: 45.2281\n",
      "Epoch [30345/50000], Train Loss: 26.5801, Test Loss: 44.3712\n",
      "Epoch [30350/50000], Train Loss: 29.9126, Test Loss: 45.4201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30355/50000], Train Loss: 23.9791, Test Loss: 47.4052\n",
      "Epoch [30360/50000], Train Loss: 25.6157, Test Loss: 46.2439\n",
      "Epoch [30365/50000], Train Loss: 22.7830, Test Loss: 46.5591\n",
      "Epoch [30370/50000], Train Loss: 26.2625, Test Loss: 49.7852\n",
      "Epoch [30375/50000], Train Loss: 16.7339, Test Loss: 43.8080\n",
      "Epoch [30380/50000], Train Loss: 27.8000, Test Loss: 51.4984\n",
      "Epoch [30385/50000], Train Loss: 26.2190, Test Loss: 43.3868\n",
      "Epoch [30390/50000], Train Loss: 37.2150, Test Loss: 46.4603\n",
      "Epoch [30395/50000], Train Loss: 28.9109, Test Loss: 45.9821\n",
      "Epoch [30400/50000], Train Loss: 25.5864, Test Loss: 44.4743\n",
      "Epoch [30405/50000], Train Loss: 26.2053, Test Loss: 43.6881\n",
      "Epoch [30410/50000], Train Loss: 23.1322, Test Loss: 47.5381\n",
      "Epoch [30415/50000], Train Loss: 23.7458, Test Loss: 44.8365\n",
      "Epoch [30420/50000], Train Loss: 25.8100, Test Loss: 46.2997\n",
      "Epoch [30425/50000], Train Loss: 48.2438, Test Loss: 44.5291\n",
      "Epoch [30430/50000], Train Loss: 25.1520, Test Loss: 46.0742\n",
      "Epoch [30435/50000], Train Loss: 25.5862, Test Loss: 49.6683\n",
      "Epoch [30440/50000], Train Loss: 25.8120, Test Loss: 44.3650\n",
      "Epoch [30445/50000], Train Loss: 28.7788, Test Loss: 45.4177\n",
      "Epoch [30450/50000], Train Loss: 27.1438, Test Loss: 58.1270\n",
      "Epoch [30455/50000], Train Loss: 21.5368, Test Loss: 43.5711\n",
      "Epoch [30460/50000], Train Loss: 25.6462, Test Loss: 46.9279\n",
      "Epoch [30465/50000], Train Loss: 44.3282, Test Loss: 44.9423\n",
      "Epoch [30470/50000], Train Loss: 25.5694, Test Loss: 44.2403\n",
      "Epoch [30475/50000], Train Loss: 33.3999, Test Loss: 45.4641\n",
      "Epoch [30480/50000], Train Loss: 38.5905, Test Loss: 46.2175\n",
      "Epoch [30485/50000], Train Loss: 24.6294, Test Loss: 44.5605\n",
      "Epoch [30490/50000], Train Loss: 26.8629, Test Loss: 50.5793\n",
      "Epoch [30495/50000], Train Loss: 26.6890, Test Loss: 46.6426\n",
      "Epoch [30500/50000], Train Loss: 25.3422, Test Loss: 48.6277\n",
      "Epoch [30505/50000], Train Loss: 39.4356, Test Loss: 44.7116\n",
      "Epoch [30510/50000], Train Loss: 26.1117, Test Loss: 49.3619\n",
      "Epoch [30515/50000], Train Loss: 23.0231, Test Loss: 45.1087\n",
      "Epoch [30520/50000], Train Loss: 29.9813, Test Loss: 43.7651\n",
      "Epoch [30525/50000], Train Loss: 27.1075, Test Loss: 45.5915\n",
      "Epoch [30530/50000], Train Loss: 20.0423, Test Loss: 47.0152\n",
      "Epoch [30535/50000], Train Loss: 27.5386, Test Loss: 45.2655\n",
      "Epoch [30540/50000], Train Loss: 21.1619, Test Loss: 47.2114\n",
      "Epoch [30545/50000], Train Loss: 20.4218, Test Loss: 46.5529\n",
      "Epoch [30550/50000], Train Loss: 26.3957, Test Loss: 46.0760\n",
      "Epoch [30555/50000], Train Loss: 20.0532, Test Loss: 47.4418\n",
      "Epoch [30560/50000], Train Loss: 28.3831, Test Loss: 45.7246\n",
      "Epoch [30565/50000], Train Loss: 16.3523, Test Loss: 44.4719\n",
      "Epoch [30570/50000], Train Loss: 26.5499, Test Loss: 44.8298\n",
      "Epoch [30575/50000], Train Loss: 28.2985, Test Loss: 47.8219\n",
      "Epoch [30580/50000], Train Loss: 24.7758, Test Loss: 46.0049\n",
      "Epoch [30585/50000], Train Loss: 21.6535, Test Loss: 43.8180\n",
      "Epoch [30590/50000], Train Loss: 25.8778, Test Loss: 46.9425\n",
      "Epoch [30595/50000], Train Loss: 26.7987, Test Loss: 45.1871\n",
      "Epoch [30600/50000], Train Loss: 24.3982, Test Loss: 44.7544\n",
      "Epoch [30605/50000], Train Loss: 22.9047, Test Loss: 46.6384\n",
      "Epoch [30610/50000], Train Loss: 42.0650, Test Loss: 42.9877\n",
      "Epoch [30615/50000], Train Loss: 27.7781, Test Loss: 44.4483\n",
      "Epoch [30620/50000], Train Loss: 24.8309, Test Loss: 44.3345\n",
      "Epoch [30625/50000], Train Loss: 25.9154, Test Loss: 44.1756\n",
      "Epoch [30630/50000], Train Loss: 22.8461, Test Loss: 44.6921\n",
      "Epoch [30635/50000], Train Loss: 55.0729, Test Loss: 49.6197\n",
      "Epoch [30640/50000], Train Loss: 23.9238, Test Loss: 46.0784\n",
      "Epoch [30645/50000], Train Loss: 19.7753, Test Loss: 44.0657\n",
      "Epoch [30650/50000], Train Loss: 20.3965, Test Loss: 46.2410\n",
      "Epoch [30655/50000], Train Loss: 23.1756, Test Loss: 46.4434\n",
      "Epoch [30660/50000], Train Loss: 31.9208, Test Loss: 46.0709\n",
      "Epoch [30665/50000], Train Loss: 26.7357, Test Loss: 45.2631\n",
      "Epoch [30670/50000], Train Loss: 27.8204, Test Loss: 47.2980\n",
      "Epoch [30675/50000], Train Loss: 45.2349, Test Loss: 50.1430\n",
      "Epoch [30680/50000], Train Loss: 27.9068, Test Loss: 52.0030\n",
      "Epoch [30685/50000], Train Loss: 24.9030, Test Loss: 50.3209\n",
      "Epoch [30690/50000], Train Loss: 28.0005, Test Loss: 53.2057\n",
      "Epoch [30695/50000], Train Loss: 28.3961, Test Loss: 44.8005\n",
      "Epoch [30700/50000], Train Loss: 27.7913, Test Loss: 48.6455\n",
      "Epoch [30705/50000], Train Loss: 17.2404, Test Loss: 43.9253\n",
      "Epoch [30710/50000], Train Loss: 29.1746, Test Loss: 45.3090\n",
      "Epoch [30715/50000], Train Loss: 26.3526, Test Loss: 45.8291\n",
      "Epoch [30720/50000], Train Loss: 30.2931, Test Loss: 44.4082\n",
      "Epoch [30725/50000], Train Loss: 21.7789, Test Loss: 46.6281\n",
      "Epoch [30730/50000], Train Loss: 37.9199, Test Loss: 63.3709\n",
      "Epoch [30735/50000], Train Loss: 23.6774, Test Loss: 44.9240\n",
      "Epoch [30740/50000], Train Loss: 29.0978, Test Loss: 44.2068\n",
      "Epoch [30745/50000], Train Loss: 20.3220, Test Loss: 44.4383\n",
      "Epoch [30750/50000], Train Loss: 27.4887, Test Loss: 46.3607\n",
      "Epoch [30755/50000], Train Loss: 28.1166, Test Loss: 47.6927\n",
      "Epoch [30760/50000], Train Loss: 23.4709, Test Loss: 44.8381\n",
      "Epoch [30765/50000], Train Loss: 19.1812, Test Loss: 45.1215\n",
      "Epoch [30770/50000], Train Loss: 25.7856, Test Loss: 45.2250\n",
      "Epoch [30775/50000], Train Loss: 26.7075, Test Loss: 47.0655\n",
      "Epoch [30780/50000], Train Loss: 24.5097, Test Loss: 44.3230\n",
      "Epoch [30785/50000], Train Loss: 32.2960, Test Loss: 45.2139\n",
      "Epoch [30790/50000], Train Loss: 28.1345, Test Loss: 44.6098\n",
      "Epoch [30795/50000], Train Loss: 29.0643, Test Loss: 47.3528\n",
      "Epoch [30800/50000], Train Loss: 20.8294, Test Loss: 46.3549\n",
      "Epoch [30805/50000], Train Loss: 26.0182, Test Loss: 44.6160\n",
      "Epoch [30810/50000], Train Loss: 24.2597, Test Loss: 44.6427\n",
      "Epoch [30815/50000], Train Loss: 22.3523, Test Loss: 42.6451\n",
      "Epoch [30820/50000], Train Loss: 24.4523, Test Loss: 45.5875\n",
      "Epoch [30825/50000], Train Loss: 28.9543, Test Loss: 43.8420\n",
      "Epoch [30830/50000], Train Loss: 24.0496, Test Loss: 45.2841\n",
      "Epoch [30835/50000], Train Loss: 23.8285, Test Loss: 43.2535\n",
      "Epoch [30840/50000], Train Loss: 25.8284, Test Loss: 47.6299\n",
      "Epoch [30845/50000], Train Loss: 29.1501, Test Loss: 45.2295\n",
      "Epoch [30850/50000], Train Loss: 47.2290, Test Loss: 42.7379\n",
      "Epoch [30855/50000], Train Loss: 28.6866, Test Loss: 46.5362\n",
      "Epoch [30860/50000], Train Loss: 34.2894, Test Loss: 44.0156\n",
      "Epoch [30865/50000], Train Loss: 23.7387, Test Loss: 48.5630\n",
      "Epoch [30870/50000], Train Loss: 26.4612, Test Loss: 49.5328\n",
      "Epoch [30875/50000], Train Loss: 25.3096, Test Loss: 45.5498\n",
      "Epoch [30880/50000], Train Loss: 26.3694, Test Loss: 55.9830\n",
      "Epoch [30885/50000], Train Loss: 26.8904, Test Loss: 49.7221\n",
      "Epoch [30890/50000], Train Loss: 24.3563, Test Loss: 48.2374\n",
      "Epoch [30895/50000], Train Loss: 33.1843, Test Loss: 48.4927\n",
      "Epoch [30900/50000], Train Loss: 22.1125, Test Loss: 47.0676\n",
      "Epoch [30905/50000], Train Loss: 44.1120, Test Loss: 44.8435\n",
      "Epoch [30910/50000], Train Loss: 25.5092, Test Loss: 46.5091\n",
      "Epoch [30915/50000], Train Loss: 23.2335, Test Loss: 51.8237\n",
      "Epoch [30920/50000], Train Loss: 26.6576, Test Loss: 43.3073\n",
      "Epoch [30925/50000], Train Loss: 23.2141, Test Loss: 45.0703\n",
      "Epoch [30930/50000], Train Loss: 22.5346, Test Loss: 45.4189\n",
      "Epoch [30935/50000], Train Loss: 24.3240, Test Loss: 45.4628\n",
      "Epoch [30940/50000], Train Loss: 22.7551, Test Loss: 44.5964\n",
      "Epoch [30945/50000], Train Loss: 37.6088, Test Loss: 44.3888\n",
      "Epoch [30950/50000], Train Loss: 26.6909, Test Loss: 51.5166\n",
      "Epoch [30955/50000], Train Loss: 28.4142, Test Loss: 44.2522\n",
      "Epoch [30960/50000], Train Loss: 24.7918, Test Loss: 48.3499\n",
      "Epoch [30965/50000], Train Loss: 22.5520, Test Loss: 43.1873\n",
      "Epoch [30970/50000], Train Loss: 24.5534, Test Loss: 45.4780\n",
      "Epoch [30975/50000], Train Loss: 22.1478, Test Loss: 46.8594\n",
      "Epoch [30980/50000], Train Loss: 31.9981, Test Loss: 44.3671\n",
      "Epoch [30985/50000], Train Loss: 24.5053, Test Loss: 44.7866\n",
      "Epoch [30990/50000], Train Loss: 20.0485, Test Loss: 46.8578\n",
      "Epoch [30995/50000], Train Loss: 20.7881, Test Loss: 47.6171\n",
      "Epoch [31000/50000], Train Loss: 24.1725, Test Loss: 54.5850\n",
      "Epoch [31005/50000], Train Loss: 27.0882, Test Loss: 47.3099\n",
      "Epoch [31010/50000], Train Loss: 23.6600, Test Loss: 44.6456\n",
      "Epoch [31015/50000], Train Loss: 33.8321, Test Loss: 57.8369\n",
      "Epoch [31020/50000], Train Loss: 23.2429, Test Loss: 44.5588\n",
      "Epoch [31025/50000], Train Loss: 22.4254, Test Loss: 47.6589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31030/50000], Train Loss: 22.5314, Test Loss: 46.8787\n",
      "Epoch [31035/50000], Train Loss: 24.3387, Test Loss: 46.5515\n",
      "Epoch [31040/50000], Train Loss: 23.3389, Test Loss: 44.6149\n",
      "Epoch [31045/50000], Train Loss: 23.8255, Test Loss: 46.8627\n",
      "Epoch [31050/50000], Train Loss: 25.1448, Test Loss: 45.4864\n",
      "Epoch [31055/50000], Train Loss: 24.0000, Test Loss: 45.0266\n",
      "Epoch [31060/50000], Train Loss: 22.7141, Test Loss: 44.1911\n",
      "Epoch [31065/50000], Train Loss: 23.5348, Test Loss: 45.7035\n",
      "Epoch [31070/50000], Train Loss: 29.2643, Test Loss: 44.1551\n",
      "Epoch [31075/50000], Train Loss: 26.7728, Test Loss: 46.1358\n",
      "Epoch [31080/50000], Train Loss: 24.8618, Test Loss: 51.6893\n",
      "Epoch [31085/50000], Train Loss: 43.8529, Test Loss: 47.9475\n",
      "Epoch [31090/50000], Train Loss: 30.8248, Test Loss: 56.1233\n",
      "Epoch [31095/50000], Train Loss: 42.8150, Test Loss: 46.6242\n",
      "Epoch [31100/50000], Train Loss: 22.6667, Test Loss: 45.6861\n",
      "Epoch [31105/50000], Train Loss: 53.8058, Test Loss: 43.4172\n",
      "Epoch [31110/50000], Train Loss: 22.6872, Test Loss: 45.2318\n",
      "Epoch [31115/50000], Train Loss: 23.0083, Test Loss: 43.7999\n",
      "Epoch [31120/50000], Train Loss: 38.3966, Test Loss: 44.7657\n",
      "Epoch [31125/50000], Train Loss: 41.7356, Test Loss: 43.5407\n",
      "Epoch [31130/50000], Train Loss: 22.2393, Test Loss: 44.2866\n",
      "Epoch [31135/50000], Train Loss: 24.7401, Test Loss: 44.6423\n",
      "Epoch [31140/50000], Train Loss: 30.3612, Test Loss: 46.7602\n",
      "Epoch [31145/50000], Train Loss: 28.3618, Test Loss: 46.7386\n",
      "Epoch [31150/50000], Train Loss: 27.8402, Test Loss: 45.7707\n",
      "Epoch [31155/50000], Train Loss: 25.9290, Test Loss: 51.8844\n",
      "Epoch [31160/50000], Train Loss: 25.1360, Test Loss: 48.1772\n",
      "Epoch [31165/50000], Train Loss: 25.4877, Test Loss: 46.3122\n",
      "Epoch [31170/50000], Train Loss: 22.8103, Test Loss: 44.4156\n",
      "Epoch [31175/50000], Train Loss: 39.0843, Test Loss: 48.5701\n",
      "Epoch [31180/50000], Train Loss: 25.1008, Test Loss: 43.4525\n",
      "Epoch [31185/50000], Train Loss: 26.0879, Test Loss: 46.6641\n",
      "Epoch [31190/50000], Train Loss: 26.5338, Test Loss: 45.6658\n",
      "Epoch [31195/50000], Train Loss: 33.3679, Test Loss: 45.1334\n",
      "Epoch [31200/50000], Train Loss: 25.1207, Test Loss: 45.7764\n",
      "Epoch [31205/50000], Train Loss: 25.5683, Test Loss: 53.6569\n",
      "Epoch [31210/50000], Train Loss: 30.6737, Test Loss: 44.9833\n",
      "Epoch [31215/50000], Train Loss: 23.8013, Test Loss: 43.7729\n",
      "Epoch [31220/50000], Train Loss: 26.4724, Test Loss: 43.9599\n",
      "Epoch [31225/50000], Train Loss: 26.3452, Test Loss: 47.4930\n",
      "Epoch [31230/50000], Train Loss: 28.6242, Test Loss: 43.0622\n",
      "Epoch [31235/50000], Train Loss: 23.3422, Test Loss: 51.5065\n",
      "Epoch [31240/50000], Train Loss: 25.4553, Test Loss: 47.6298\n",
      "Epoch [31245/50000], Train Loss: 25.1591, Test Loss: 46.3760\n",
      "Epoch [31250/50000], Train Loss: 23.8313, Test Loss: 43.2932\n",
      "Epoch [31255/50000], Train Loss: 25.3662, Test Loss: 43.9187\n",
      "Epoch [31260/50000], Train Loss: 24.3053, Test Loss: 45.1267\n",
      "Epoch [31265/50000], Train Loss: 26.1765, Test Loss: 44.6258\n",
      "Epoch [31270/50000], Train Loss: 24.0093, Test Loss: 44.4790\n",
      "Epoch [31275/50000], Train Loss: 25.0192, Test Loss: 48.9957\n",
      "Epoch [31280/50000], Train Loss: 27.3918, Test Loss: 45.0360\n",
      "Epoch [31285/50000], Train Loss: 25.1893, Test Loss: 46.4037\n",
      "Epoch [31290/50000], Train Loss: 17.1416, Test Loss: 48.8996\n",
      "Epoch [31295/50000], Train Loss: 25.2312, Test Loss: 60.6377\n",
      "Epoch [31300/50000], Train Loss: 53.2841, Test Loss: 43.3254\n",
      "Epoch [31305/50000], Train Loss: 26.6950, Test Loss: 46.1277\n",
      "Epoch [31310/50000], Train Loss: 22.3126, Test Loss: 52.5217\n",
      "Epoch [31315/50000], Train Loss: 26.1578, Test Loss: 43.6931\n",
      "Epoch [31320/50000], Train Loss: 31.0736, Test Loss: 45.4504\n",
      "Epoch [31325/50000], Train Loss: 27.6240, Test Loss: 43.7224\n",
      "Epoch [31330/50000], Train Loss: 25.1624, Test Loss: 43.9799\n",
      "Epoch [31335/50000], Train Loss: 26.2776, Test Loss: 46.4147\n",
      "Epoch [31340/50000], Train Loss: 28.5679, Test Loss: 42.9600\n",
      "Epoch [31345/50000], Train Loss: 27.9096, Test Loss: 48.1814\n",
      "Epoch [31350/50000], Train Loss: 22.8019, Test Loss: 46.6255\n",
      "Epoch [31355/50000], Train Loss: 55.9857, Test Loss: 65.6140\n",
      "Epoch [31360/50000], Train Loss: 24.0302, Test Loss: 44.4072\n",
      "Epoch [31365/50000], Train Loss: 20.3876, Test Loss: 46.0069\n",
      "Epoch [31370/50000], Train Loss: 28.0574, Test Loss: 43.8110\n",
      "Epoch [31375/50000], Train Loss: 24.5901, Test Loss: 44.1255\n",
      "Epoch [31380/50000], Train Loss: 26.9345, Test Loss: 44.8704\n",
      "Epoch [31385/50000], Train Loss: 24.3735, Test Loss: 45.0876\n",
      "Epoch [31390/50000], Train Loss: 29.7059, Test Loss: 46.3577\n",
      "Epoch [31395/50000], Train Loss: 18.6975, Test Loss: 43.2550\n",
      "Epoch [31400/50000], Train Loss: 32.6677, Test Loss: 45.4510\n",
      "Epoch [31405/50000], Train Loss: 24.1421, Test Loss: 45.3874\n",
      "Epoch [31410/50000], Train Loss: 26.3953, Test Loss: 44.3614\n",
      "Epoch [31415/50000], Train Loss: 22.3614, Test Loss: 45.9290\n",
      "Epoch [31420/50000], Train Loss: 25.5836, Test Loss: 50.9201\n",
      "Epoch [31425/50000], Train Loss: 22.9875, Test Loss: 45.5296\n",
      "Epoch [31430/50000], Train Loss: 31.0458, Test Loss: 86.0659\n",
      "Epoch [31435/50000], Train Loss: 15.7160, Test Loss: 44.4595\n",
      "Epoch [31440/50000], Train Loss: 23.7804, Test Loss: 45.0929\n",
      "Epoch [31445/50000], Train Loss: 22.1196, Test Loss: 45.9063\n",
      "Epoch [31450/50000], Train Loss: 31.8916, Test Loss: 44.7522\n",
      "Epoch [31455/50000], Train Loss: 21.3236, Test Loss: 42.7702\n",
      "Epoch [31460/50000], Train Loss: 23.6932, Test Loss: 46.8968\n",
      "Epoch [31465/50000], Train Loss: 30.7253, Test Loss: 47.2529\n",
      "Epoch [31470/50000], Train Loss: 15.8370, Test Loss: 43.1559\n",
      "Epoch [31475/50000], Train Loss: 19.5186, Test Loss: 46.8501\n",
      "Epoch [31480/50000], Train Loss: 23.5822, Test Loss: 57.1198\n",
      "Epoch [31485/50000], Train Loss: 22.9090, Test Loss: 50.2594\n",
      "Epoch [31490/50000], Train Loss: 19.4829, Test Loss: 44.6119\n",
      "Epoch [31495/50000], Train Loss: 18.7237, Test Loss: 44.2353\n",
      "Epoch [31500/50000], Train Loss: 27.0016, Test Loss: 52.5364\n",
      "Epoch [31505/50000], Train Loss: 24.2271, Test Loss: 43.3391\n",
      "Epoch [31510/50000], Train Loss: 20.4066, Test Loss: 46.4646\n",
      "Epoch [31515/50000], Train Loss: 25.0622, Test Loss: 45.1728\n",
      "Epoch [31520/50000], Train Loss: 27.2391, Test Loss: 47.7163\n",
      "Epoch [31525/50000], Train Loss: 23.3308, Test Loss: 43.2855\n",
      "Epoch [31530/50000], Train Loss: 28.9020, Test Loss: 44.4320\n",
      "Epoch [31535/50000], Train Loss: 21.0687, Test Loss: 43.4500\n",
      "Epoch [31540/50000], Train Loss: 25.2865, Test Loss: 43.9270\n",
      "Epoch [31545/50000], Train Loss: 28.4792, Test Loss: 49.8110\n",
      "Epoch [31550/50000], Train Loss: 26.9389, Test Loss: 46.2684\n",
      "Epoch [31555/50000], Train Loss: 23.9530, Test Loss: 44.3311\n",
      "Epoch [31560/50000], Train Loss: 20.3783, Test Loss: 44.2246\n",
      "Epoch [31565/50000], Train Loss: 24.8519, Test Loss: 47.0390\n",
      "Epoch [31570/50000], Train Loss: 23.2175, Test Loss: 46.7335\n",
      "Epoch [31575/50000], Train Loss: 27.1776, Test Loss: 43.3079\n",
      "Epoch [31580/50000], Train Loss: 26.9832, Test Loss: 46.3704\n",
      "Epoch [31585/50000], Train Loss: 25.5672, Test Loss: 45.2560\n",
      "Epoch [31590/50000], Train Loss: 24.4591, Test Loss: 44.4860\n",
      "Epoch [31595/50000], Train Loss: 25.1706, Test Loss: 45.0069\n",
      "Epoch [31600/50000], Train Loss: 30.5962, Test Loss: 44.9048\n",
      "Epoch [31605/50000], Train Loss: 26.1733, Test Loss: 42.9903\n",
      "Epoch [31610/50000], Train Loss: 25.8693, Test Loss: 42.9838\n",
      "Epoch [31615/50000], Train Loss: 28.5935, Test Loss: 47.4130\n",
      "Epoch [31620/50000], Train Loss: 27.6070, Test Loss: 47.3918\n",
      "Epoch [31625/50000], Train Loss: 22.1043, Test Loss: 45.0677\n",
      "Epoch [31630/50000], Train Loss: 26.3116, Test Loss: 46.0134\n",
      "Epoch [31635/50000], Train Loss: 27.5728, Test Loss: 45.4319\n",
      "Epoch [31640/50000], Train Loss: 21.9347, Test Loss: 44.3071\n",
      "Epoch [31645/50000], Train Loss: 41.9304, Test Loss: 43.5634\n",
      "Epoch [31650/50000], Train Loss: 25.2545, Test Loss: 43.5074\n",
      "Epoch [31655/50000], Train Loss: 25.6958, Test Loss: 49.6498\n",
      "Epoch [31660/50000], Train Loss: 24.2779, Test Loss: 46.9399\n",
      "Epoch [31665/50000], Train Loss: 32.1741, Test Loss: 48.5511\n",
      "Epoch [31670/50000], Train Loss: 28.2171, Test Loss: 45.3176\n",
      "Epoch [31675/50000], Train Loss: 35.0501, Test Loss: 43.8934\n",
      "Epoch [31680/50000], Train Loss: 25.2918, Test Loss: 48.1905\n",
      "Epoch [31685/50000], Train Loss: 27.7403, Test Loss: 46.6364\n",
      "Epoch [31690/50000], Train Loss: 21.8749, Test Loss: 44.2190\n",
      "Epoch [31695/50000], Train Loss: 31.7528, Test Loss: 45.6900\n",
      "Epoch [31700/50000], Train Loss: 25.2229, Test Loss: 47.9636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31705/50000], Train Loss: 21.6808, Test Loss: 44.0378\n",
      "Epoch [31710/50000], Train Loss: 25.6799, Test Loss: 50.6661\n",
      "Epoch [31715/50000], Train Loss: 25.1601, Test Loss: 46.8715\n",
      "Epoch [31720/50000], Train Loss: 27.7733, Test Loss: 44.1086\n",
      "Epoch [31725/50000], Train Loss: 23.3000, Test Loss: 43.9384\n",
      "Epoch [31730/50000], Train Loss: 19.9226, Test Loss: 43.6382\n",
      "Epoch [31735/50000], Train Loss: 24.5348, Test Loss: 56.2786\n",
      "Epoch [31740/50000], Train Loss: 22.8543, Test Loss: 43.5230\n",
      "Epoch [31745/50000], Train Loss: 22.6114, Test Loss: 57.2210\n",
      "Epoch [31750/50000], Train Loss: 25.7484, Test Loss: 43.2639\n",
      "Epoch [31755/50000], Train Loss: 20.0243, Test Loss: 43.5461\n",
      "Epoch [31760/50000], Train Loss: 24.1278, Test Loss: 46.4184\n",
      "Epoch [31765/50000], Train Loss: 28.9994, Test Loss: 46.7485\n",
      "Epoch [31770/50000], Train Loss: 22.6370, Test Loss: 44.4047\n",
      "Epoch [31775/50000], Train Loss: 22.3621, Test Loss: 48.1384\n",
      "Epoch [31780/50000], Train Loss: 30.5906, Test Loss: 43.2706\n",
      "Epoch [31785/50000], Train Loss: 27.4168, Test Loss: 44.9055\n",
      "Epoch [31790/50000], Train Loss: 16.5113, Test Loss: 45.3441\n",
      "Epoch [31795/50000], Train Loss: 27.0933, Test Loss: 44.2271\n",
      "Epoch [31800/50000], Train Loss: 24.5294, Test Loss: 46.5695\n",
      "Epoch [31805/50000], Train Loss: 31.3128, Test Loss: 45.0827\n",
      "Epoch [31810/50000], Train Loss: 22.2017, Test Loss: 47.4506\n",
      "Epoch [31815/50000], Train Loss: 24.0596, Test Loss: 45.6824\n",
      "Epoch [31820/50000], Train Loss: 27.0373, Test Loss: 45.6917\n",
      "Epoch [31825/50000], Train Loss: 46.9456, Test Loss: 43.7186\n",
      "Epoch [31830/50000], Train Loss: 25.8292, Test Loss: 45.0020\n",
      "Epoch [31835/50000], Train Loss: 43.7126, Test Loss: 44.3251\n",
      "Epoch [31840/50000], Train Loss: 23.6504, Test Loss: 43.1605\n",
      "Epoch [31845/50000], Train Loss: 27.9292, Test Loss: 49.4639\n",
      "Epoch [31850/50000], Train Loss: 27.3355, Test Loss: 45.5413\n",
      "Epoch [31855/50000], Train Loss: 28.8585, Test Loss: 49.8647\n",
      "Epoch [31860/50000], Train Loss: 25.0189, Test Loss: 43.2051\n",
      "Epoch [31865/50000], Train Loss: 23.9447, Test Loss: 42.9183\n",
      "Epoch [31870/50000], Train Loss: 28.3644, Test Loss: 47.6109\n",
      "Epoch [31875/50000], Train Loss: 27.0886, Test Loss: 45.1798\n",
      "Epoch [31880/50000], Train Loss: 26.5124, Test Loss: 63.3594\n",
      "Epoch [31885/50000], Train Loss: 29.0463, Test Loss: 43.5686\n",
      "Epoch [31890/50000], Train Loss: 22.3611, Test Loss: 48.2995\n",
      "Epoch [31895/50000], Train Loss: 24.0454, Test Loss: 43.5620\n",
      "Epoch [31900/50000], Train Loss: 24.9587, Test Loss: 46.9238\n",
      "Epoch [31905/50000], Train Loss: 24.8889, Test Loss: 44.3226\n",
      "Epoch [31910/50000], Train Loss: 17.4845, Test Loss: 44.7293\n",
      "Epoch [31915/50000], Train Loss: 26.0735, Test Loss: 48.4425\n",
      "Epoch [31920/50000], Train Loss: 46.6145, Test Loss: 43.8495\n",
      "Epoch [31925/50000], Train Loss: 26.9973, Test Loss: 46.6869\n",
      "Epoch [31930/50000], Train Loss: 27.2918, Test Loss: 45.2097\n",
      "Epoch [31935/50000], Train Loss: 23.8698, Test Loss: 46.3487\n",
      "Epoch [31940/50000], Train Loss: 32.8953, Test Loss: 47.2649\n",
      "Epoch [31945/50000], Train Loss: 21.4360, Test Loss: 45.8233\n",
      "Epoch [31950/50000], Train Loss: 22.9617, Test Loss: 47.5316\n",
      "Epoch [31955/50000], Train Loss: 25.6506, Test Loss: 45.1375\n",
      "Epoch [31960/50000], Train Loss: 15.2495, Test Loss: 43.0472\n",
      "Epoch [31965/50000], Train Loss: 24.9869, Test Loss: 47.5399\n",
      "Epoch [31970/50000], Train Loss: 25.3004, Test Loss: 44.9426\n",
      "Epoch [31975/50000], Train Loss: 27.2830, Test Loss: 46.6484\n",
      "Epoch [31980/50000], Train Loss: 23.4465, Test Loss: 45.7664\n",
      "Epoch [31985/50000], Train Loss: 26.4449, Test Loss: 44.1712\n",
      "Epoch [31990/50000], Train Loss: 22.9841, Test Loss: 45.7872\n",
      "Epoch [31995/50000], Train Loss: 23.2072, Test Loss: 45.7601\n",
      "Epoch [32000/50000], Train Loss: 23.2060, Test Loss: 44.1946\n",
      "Epoch [32005/50000], Train Loss: 26.6544, Test Loss: 44.3861\n",
      "Epoch [32010/50000], Train Loss: 27.0288, Test Loss: 43.9231\n",
      "Epoch [32015/50000], Train Loss: 24.0924, Test Loss: 46.2692\n",
      "Epoch [32020/50000], Train Loss: 34.3064, Test Loss: 44.8571\n",
      "Epoch [32025/50000], Train Loss: 22.2340, Test Loss: 47.5666\n",
      "Epoch [32030/50000], Train Loss: 26.9247, Test Loss: 47.0585\n",
      "Epoch [32035/50000], Train Loss: 24.4032, Test Loss: 47.5128\n",
      "Epoch [32040/50000], Train Loss: 32.7398, Test Loss: 44.3253\n",
      "Epoch [32045/50000], Train Loss: 25.5069, Test Loss: 44.6150\n",
      "Epoch [32050/50000], Train Loss: 23.2748, Test Loss: 45.7694\n",
      "Epoch [32055/50000], Train Loss: 20.5792, Test Loss: 48.4420\n",
      "Epoch [32060/50000], Train Loss: 49.6157, Test Loss: 45.3945\n",
      "Epoch [32065/50000], Train Loss: 23.5200, Test Loss: 46.0000\n",
      "Epoch [32070/50000], Train Loss: 33.6143, Test Loss: 47.5667\n",
      "Epoch [32075/50000], Train Loss: 31.1998, Test Loss: 45.3107\n",
      "Epoch [32080/50000], Train Loss: 26.9845, Test Loss: 49.0222\n",
      "Epoch [32085/50000], Train Loss: 24.7355, Test Loss: 45.3277\n",
      "Epoch [32090/50000], Train Loss: 18.1340, Test Loss: 43.9348\n",
      "Epoch [32095/50000], Train Loss: 26.5937, Test Loss: 46.7581\n",
      "Epoch [32100/50000], Train Loss: 27.6289, Test Loss: 45.8322\n",
      "Epoch [32105/50000], Train Loss: 22.6813, Test Loss: 45.6560\n",
      "Epoch [32110/50000], Train Loss: 24.8719, Test Loss: 43.6881\n",
      "Epoch [32115/50000], Train Loss: 25.9156, Test Loss: 43.5739\n",
      "Epoch [32120/50000], Train Loss: 20.9342, Test Loss: 47.6562\n",
      "Epoch [32125/50000], Train Loss: 23.1987, Test Loss: 43.6770\n",
      "Epoch [32130/50000], Train Loss: 29.8137, Test Loss: 44.7825\n",
      "Epoch [32135/50000], Train Loss: 26.9262, Test Loss: 43.8907\n",
      "Epoch [32140/50000], Train Loss: 25.2288, Test Loss: 53.0089\n",
      "Epoch [32145/50000], Train Loss: 27.0601, Test Loss: 49.0090\n",
      "Epoch [32150/50000], Train Loss: 39.8721, Test Loss: 43.7543\n",
      "Epoch [32155/50000], Train Loss: 23.2243, Test Loss: 43.3868\n",
      "Epoch [32160/50000], Train Loss: 21.2887, Test Loss: 44.1715\n",
      "Epoch [32165/50000], Train Loss: 22.9142, Test Loss: 44.2672\n",
      "Epoch [32170/50000], Train Loss: 23.6246, Test Loss: 46.3687\n",
      "Epoch [32175/50000], Train Loss: 22.8439, Test Loss: 47.1732\n",
      "Epoch [32180/50000], Train Loss: 39.0358, Test Loss: 44.5757\n",
      "Epoch [32185/50000], Train Loss: 26.2361, Test Loss: 50.7487\n",
      "Epoch [32190/50000], Train Loss: 23.7060, Test Loss: 43.9616\n",
      "Epoch [32195/50000], Train Loss: 25.5394, Test Loss: 44.3572\n",
      "Epoch [32200/50000], Train Loss: 29.2152, Test Loss: 45.8188\n",
      "Epoch [32205/50000], Train Loss: 24.2979, Test Loss: 45.0522\n",
      "Epoch [32210/50000], Train Loss: 24.2646, Test Loss: 46.8460\n",
      "Epoch [32215/50000], Train Loss: 25.0065, Test Loss: 51.7040\n",
      "Epoch [32220/50000], Train Loss: 24.1011, Test Loss: 48.6973\n",
      "Epoch [32225/50000], Train Loss: 21.0813, Test Loss: 45.0675\n",
      "Epoch [32230/50000], Train Loss: 24.4185, Test Loss: 53.8691\n",
      "Epoch [32235/50000], Train Loss: 23.8622, Test Loss: 45.9038\n",
      "Epoch [32240/50000], Train Loss: 23.1702, Test Loss: 44.3415\n",
      "Epoch [32245/50000], Train Loss: 27.8336, Test Loss: 45.1137\n",
      "Epoch [32250/50000], Train Loss: 22.8210, Test Loss: 44.1469\n",
      "Epoch [32255/50000], Train Loss: 22.3841, Test Loss: 44.5673\n",
      "Epoch [32260/50000], Train Loss: 23.1740, Test Loss: 51.7474\n",
      "Epoch [32265/50000], Train Loss: 27.1539, Test Loss: 48.7785\n",
      "Epoch [32270/50000], Train Loss: 25.2858, Test Loss: 47.1719\n",
      "Epoch [32275/50000], Train Loss: 25.5090, Test Loss: 48.5592\n",
      "Epoch [32280/50000], Train Loss: 20.5887, Test Loss: 48.6655\n",
      "Epoch [32285/50000], Train Loss: 20.5756, Test Loss: 45.7233\n",
      "Epoch [32290/50000], Train Loss: 27.4886, Test Loss: 44.4677\n",
      "Epoch [32295/50000], Train Loss: 20.2447, Test Loss: 44.4561\n",
      "Epoch [32300/50000], Train Loss: 25.5266, Test Loss: 43.4884\n",
      "Epoch [32305/50000], Train Loss: 23.7541, Test Loss: 43.7584\n",
      "Epoch [32310/50000], Train Loss: 25.8049, Test Loss: 44.0725\n",
      "Epoch [32315/50000], Train Loss: 23.9213, Test Loss: 46.7816\n",
      "Epoch [32320/50000], Train Loss: 27.4852, Test Loss: 46.8429\n",
      "Epoch [32325/50000], Train Loss: 20.2602, Test Loss: 42.9960\n",
      "Epoch [32330/50000], Train Loss: 20.5483, Test Loss: 43.9999\n",
      "Epoch [32335/50000], Train Loss: 21.6509, Test Loss: 44.9837\n",
      "Epoch [32340/50000], Train Loss: 23.3257, Test Loss: 61.7654\n",
      "Epoch [32345/50000], Train Loss: 26.3212, Test Loss: 45.6104\n",
      "Epoch [32350/50000], Train Loss: 23.9176, Test Loss: 46.6370\n",
      "Epoch [32355/50000], Train Loss: 22.0684, Test Loss: 44.2313\n",
      "Epoch [32360/50000], Train Loss: 22.9463, Test Loss: 48.3483\n",
      "Epoch [32365/50000], Train Loss: 22.5157, Test Loss: 44.4983\n",
      "Epoch [32370/50000], Train Loss: 22.6450, Test Loss: 46.6322\n",
      "Epoch [32375/50000], Train Loss: 22.1370, Test Loss: 46.4086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32380/50000], Train Loss: 17.9245, Test Loss: 43.9154\n",
      "Epoch [32385/50000], Train Loss: 32.3360, Test Loss: 43.7281\n",
      "Epoch [32390/50000], Train Loss: 25.3364, Test Loss: 44.8430\n",
      "Epoch [32395/50000], Train Loss: 24.5207, Test Loss: 44.7820\n",
      "Epoch [32400/50000], Train Loss: 35.4765, Test Loss: 68.9016\n",
      "Epoch [32405/50000], Train Loss: 27.9643, Test Loss: 46.1973\n",
      "Epoch [32410/50000], Train Loss: 43.8775, Test Loss: 47.9308\n",
      "Epoch [32415/50000], Train Loss: 23.4276, Test Loss: 47.0760\n",
      "Epoch [32420/50000], Train Loss: 36.4081, Test Loss: 44.0311\n",
      "Epoch [32425/50000], Train Loss: 86.8638, Test Loss: 43.6751\n",
      "Epoch [32430/50000], Train Loss: 24.1997, Test Loss: 50.9122\n",
      "Epoch [32435/50000], Train Loss: 27.1982, Test Loss: 44.2326\n",
      "Epoch [32440/50000], Train Loss: 29.2310, Test Loss: 47.5660\n",
      "Epoch [32445/50000], Train Loss: 24.0098, Test Loss: 43.9552\n",
      "Epoch [32450/50000], Train Loss: 33.9460, Test Loss: 47.0393\n",
      "Epoch [32455/50000], Train Loss: 24.2080, Test Loss: 44.7511\n",
      "Epoch [32460/50000], Train Loss: 24.9833, Test Loss: 44.2109\n",
      "Epoch [32465/50000], Train Loss: 29.2946, Test Loss: 59.2558\n",
      "Epoch [32470/50000], Train Loss: 21.8498, Test Loss: 43.9419\n",
      "Epoch [32475/50000], Train Loss: 23.4697, Test Loss: 44.6309\n",
      "Epoch [32480/50000], Train Loss: 25.5222, Test Loss: 45.5878\n",
      "Epoch [32485/50000], Train Loss: 20.0933, Test Loss: 47.5665\n",
      "Epoch [32490/50000], Train Loss: 23.5446, Test Loss: 43.7950\n",
      "Epoch [32495/50000], Train Loss: 28.1390, Test Loss: 44.0870\n",
      "Epoch [32500/50000], Train Loss: 30.1325, Test Loss: 43.5508\n",
      "Epoch [32505/50000], Train Loss: 28.6543, Test Loss: 46.0681\n",
      "Epoch [32510/50000], Train Loss: 23.1000, Test Loss: 47.7984\n",
      "Epoch [32515/50000], Train Loss: 25.6844, Test Loss: 52.8905\n",
      "Epoch [32520/50000], Train Loss: 30.2088, Test Loss: 46.5079\n",
      "Epoch [32525/50000], Train Loss: 38.3604, Test Loss: 43.7668\n",
      "Epoch [32530/50000], Train Loss: 28.4961, Test Loss: 44.5194\n",
      "Epoch [32535/50000], Train Loss: 23.2070, Test Loss: 44.8181\n",
      "Epoch [32540/50000], Train Loss: 27.8548, Test Loss: 43.0835\n",
      "Epoch [32545/50000], Train Loss: 20.4311, Test Loss: 45.3512\n",
      "Epoch [32550/50000], Train Loss: 24.2788, Test Loss: 43.5842\n",
      "Epoch [32555/50000], Train Loss: 15.7907, Test Loss: 43.4051\n",
      "Epoch [32560/50000], Train Loss: 26.9656, Test Loss: 44.5674\n",
      "Epoch [32565/50000], Train Loss: 21.2867, Test Loss: 43.2280\n",
      "Epoch [32570/50000], Train Loss: 21.4875, Test Loss: 44.1130\n",
      "Epoch [32575/50000], Train Loss: 22.5774, Test Loss: 48.2020\n",
      "Epoch [32580/50000], Train Loss: 21.6904, Test Loss: 48.0164\n",
      "Epoch [32585/50000], Train Loss: 28.2720, Test Loss: 43.3503\n",
      "Epoch [32590/50000], Train Loss: 16.0496, Test Loss: 43.3496\n",
      "Epoch [32595/50000], Train Loss: 22.5293, Test Loss: 46.7465\n",
      "Epoch [32600/50000], Train Loss: 15.2369, Test Loss: 47.1649\n",
      "Epoch [32605/50000], Train Loss: 24.8011, Test Loss: 43.1672\n",
      "Epoch [32610/50000], Train Loss: 19.6484, Test Loss: 50.3932\n",
      "Epoch [32615/50000], Train Loss: 58.2439, Test Loss: 44.9822\n",
      "Epoch [32620/50000], Train Loss: 23.5953, Test Loss: 43.3600\n",
      "Epoch [32625/50000], Train Loss: 27.9093, Test Loss: 44.6668\n",
      "Epoch [32630/50000], Train Loss: 21.6558, Test Loss: 45.1881\n",
      "Epoch [32635/50000], Train Loss: 26.2689, Test Loss: 43.6374\n",
      "Epoch [32640/50000], Train Loss: 24.9991, Test Loss: 42.7022\n",
      "Epoch [32645/50000], Train Loss: 22.2064, Test Loss: 46.7552\n",
      "Epoch [32650/50000], Train Loss: 25.4837, Test Loss: 43.5708\n",
      "Epoch [32655/50000], Train Loss: 22.1023, Test Loss: 46.1279\n",
      "Epoch [32660/50000], Train Loss: 24.7278, Test Loss: 45.1744\n",
      "Epoch [32665/50000], Train Loss: 25.9336, Test Loss: 48.5636\n",
      "Epoch [32670/50000], Train Loss: 22.7264, Test Loss: 44.9766\n",
      "Epoch [32675/50000], Train Loss: 27.9422, Test Loss: 44.5799\n",
      "Epoch [32680/50000], Train Loss: 29.6938, Test Loss: 43.8444\n",
      "Epoch [32685/50000], Train Loss: 24.1718, Test Loss: 50.1835\n",
      "Epoch [32690/50000], Train Loss: 27.3216, Test Loss: 69.1683\n",
      "Epoch [32695/50000], Train Loss: 37.4858, Test Loss: 68.7002\n",
      "Epoch [32700/50000], Train Loss: 28.1292, Test Loss: 44.1710\n",
      "Epoch [32705/50000], Train Loss: 31.4333, Test Loss: 42.4399\n",
      "Epoch [32710/50000], Train Loss: 32.5192, Test Loss: 44.2298\n",
      "Epoch [32715/50000], Train Loss: 25.0974, Test Loss: 46.7082\n",
      "Epoch [32720/50000], Train Loss: 24.4270, Test Loss: 46.3326\n",
      "Epoch [32725/50000], Train Loss: 21.9949, Test Loss: 46.4502\n",
      "Epoch [32730/50000], Train Loss: 23.4300, Test Loss: 43.1409\n",
      "Epoch [32735/50000], Train Loss: 24.1447, Test Loss: 44.5358\n",
      "Epoch [32740/50000], Train Loss: 20.2439, Test Loss: 47.0425\n",
      "Epoch [32745/50000], Train Loss: 28.3893, Test Loss: 43.1946\n",
      "Epoch [32750/50000], Train Loss: 28.6222, Test Loss: 43.1123\n",
      "Epoch [32755/50000], Train Loss: 25.8676, Test Loss: 46.1070\n",
      "Epoch [32760/50000], Train Loss: 23.3973, Test Loss: 44.6800\n",
      "Epoch [32765/50000], Train Loss: 24.6650, Test Loss: 43.6704\n",
      "Epoch [32770/50000], Train Loss: 25.1847, Test Loss: 58.1386\n",
      "Epoch [32775/50000], Train Loss: 22.0484, Test Loss: 51.4511\n",
      "Epoch [32780/50000], Train Loss: 22.0329, Test Loss: 44.7044\n",
      "Epoch [32785/50000], Train Loss: 23.6023, Test Loss: 47.4968\n",
      "Epoch [32790/50000], Train Loss: 23.1066, Test Loss: 42.8288\n",
      "Epoch [32795/50000], Train Loss: 36.2541, Test Loss: 89.1936\n",
      "Epoch [32800/50000], Train Loss: 33.5928, Test Loss: 43.7356\n",
      "Epoch [32805/50000], Train Loss: 29.5957, Test Loss: 45.3986\n",
      "Epoch [32810/50000], Train Loss: 21.3776, Test Loss: 47.5292\n",
      "Epoch [32815/50000], Train Loss: 25.6237, Test Loss: 47.4357\n",
      "Epoch [32820/50000], Train Loss: 18.9247, Test Loss: 45.7384\n",
      "Epoch [32825/50000], Train Loss: 41.4670, Test Loss: 43.5411\n",
      "Epoch [32830/50000], Train Loss: 20.9182, Test Loss: 49.5122\n",
      "Epoch [32835/50000], Train Loss: 19.2113, Test Loss: 44.5388\n",
      "Epoch [32840/50000], Train Loss: 25.3657, Test Loss: 45.5644\n",
      "Epoch [32845/50000], Train Loss: 25.1528, Test Loss: 46.6052\n",
      "Epoch [32850/50000], Train Loss: 23.8229, Test Loss: 47.0378\n",
      "Epoch [32855/50000], Train Loss: 22.7480, Test Loss: 44.0934\n",
      "Epoch [32860/50000], Train Loss: 21.9012, Test Loss: 44.2582\n",
      "Epoch [32865/50000], Train Loss: 25.5832, Test Loss: 44.1527\n",
      "Epoch [32870/50000], Train Loss: 18.3521, Test Loss: 44.6503\n",
      "Epoch [32875/50000], Train Loss: 24.6349, Test Loss: 47.1786\n",
      "Epoch [32880/50000], Train Loss: 21.9533, Test Loss: 46.2362\n",
      "Epoch [32885/50000], Train Loss: 24.6610, Test Loss: 46.3377\n",
      "Epoch [32890/50000], Train Loss: 20.7107, Test Loss: 48.6584\n",
      "Epoch [32895/50000], Train Loss: 33.2316, Test Loss: 46.0424\n",
      "Epoch [32900/50000], Train Loss: 19.7376, Test Loss: 45.1796\n",
      "Epoch [32905/50000], Train Loss: 25.4215, Test Loss: 46.2855\n",
      "Epoch [32910/50000], Train Loss: 29.0180, Test Loss: 48.4722\n",
      "Epoch [32915/50000], Train Loss: 24.6933, Test Loss: 44.5249\n",
      "Epoch [32920/50000], Train Loss: 21.8533, Test Loss: 44.0360\n",
      "Epoch [32925/50000], Train Loss: 24.7310, Test Loss: 44.4885\n",
      "Epoch [32930/50000], Train Loss: 24.4130, Test Loss: 52.5872\n",
      "Epoch [32935/50000], Train Loss: 26.3513, Test Loss: 50.2077\n",
      "Epoch [32940/50000], Train Loss: 33.6955, Test Loss: 43.3585\n",
      "Epoch [32945/50000], Train Loss: 22.7544, Test Loss: 44.4657\n",
      "Epoch [32950/50000], Train Loss: 22.8292, Test Loss: 46.3077\n",
      "Epoch [32955/50000], Train Loss: 25.1112, Test Loss: 43.9385\n",
      "Epoch [32960/50000], Train Loss: 29.2404, Test Loss: 43.3216\n",
      "Epoch [32965/50000], Train Loss: 24.0582, Test Loss: 44.0304\n",
      "Epoch [32970/50000], Train Loss: 21.6642, Test Loss: 42.5726\n",
      "Epoch [32975/50000], Train Loss: 28.8114, Test Loss: 43.3225\n",
      "Epoch [32980/50000], Train Loss: 24.5133, Test Loss: 45.3865\n",
      "Epoch [32985/50000], Train Loss: 26.7689, Test Loss: 47.8390\n",
      "Epoch [32990/50000], Train Loss: 20.5393, Test Loss: 44.1105\n",
      "Epoch [32995/50000], Train Loss: 19.7741, Test Loss: 42.9963\n",
      "Epoch [33000/50000], Train Loss: 25.4756, Test Loss: 44.8130\n",
      "Epoch [33005/50000], Train Loss: 29.0821, Test Loss: 48.8415\n",
      "Epoch [33010/50000], Train Loss: 26.2803, Test Loss: 43.5454\n",
      "Epoch [33015/50000], Train Loss: 23.2445, Test Loss: 45.4351\n",
      "Epoch [33020/50000], Train Loss: 22.3446, Test Loss: 42.6854\n",
      "Epoch [33025/50000], Train Loss: 24.6698, Test Loss: 43.9684\n",
      "Epoch [33030/50000], Train Loss: 23.1536, Test Loss: 43.6422\n",
      "Epoch [33035/50000], Train Loss: 23.4499, Test Loss: 43.9558\n",
      "Epoch [33040/50000], Train Loss: 21.8330, Test Loss: 45.5006\n",
      "Epoch [33045/50000], Train Loss: 19.7824, Test Loss: 43.1694\n",
      "Epoch [33050/50000], Train Loss: 22.5284, Test Loss: 44.5325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33055/50000], Train Loss: 24.5337, Test Loss: 47.9204\n",
      "Epoch [33060/50000], Train Loss: 28.4327, Test Loss: 48.7220\n",
      "Epoch [33065/50000], Train Loss: 32.4877, Test Loss: 48.7495\n",
      "Epoch [33070/50000], Train Loss: 39.8277, Test Loss: 46.9437\n",
      "Epoch [33075/50000], Train Loss: 22.4566, Test Loss: 54.2718\n",
      "Epoch [33080/50000], Train Loss: 24.6478, Test Loss: 43.7638\n",
      "Epoch [33085/50000], Train Loss: 22.5730, Test Loss: 43.1789\n",
      "Epoch [33090/50000], Train Loss: 25.2222, Test Loss: 45.1156\n",
      "Epoch [33095/50000], Train Loss: 19.8365, Test Loss: 43.6430\n",
      "Epoch [33100/50000], Train Loss: 26.3253, Test Loss: 54.9179\n",
      "Epoch [33105/50000], Train Loss: 23.0107, Test Loss: 48.7160\n",
      "Epoch [33110/50000], Train Loss: 23.3309, Test Loss: 45.7026\n",
      "Epoch [33115/50000], Train Loss: 21.1678, Test Loss: 43.6464\n",
      "Epoch [33120/50000], Train Loss: 26.3908, Test Loss: 43.2502\n",
      "Epoch [33125/50000], Train Loss: 19.1386, Test Loss: 42.9504\n",
      "Epoch [33130/50000], Train Loss: 20.9978, Test Loss: 56.4327\n",
      "Epoch [33135/50000], Train Loss: 24.2431, Test Loss: 50.3165\n",
      "Epoch [33140/50000], Train Loss: 22.2121, Test Loss: 44.7987\n",
      "Epoch [33145/50000], Train Loss: 23.3458, Test Loss: 56.6312\n",
      "Epoch [33150/50000], Train Loss: 22.4740, Test Loss: 45.3045\n",
      "Epoch [33155/50000], Train Loss: 26.5590, Test Loss: 44.8344\n",
      "Epoch [33160/50000], Train Loss: 18.9926, Test Loss: 45.8372\n",
      "Epoch [33165/50000], Train Loss: 26.0488, Test Loss: 43.7164\n",
      "Epoch [33170/50000], Train Loss: 24.9611, Test Loss: 53.5214\n",
      "Epoch [33175/50000], Train Loss: 36.1564, Test Loss: 45.0723\n",
      "Epoch [33180/50000], Train Loss: 29.4248, Test Loss: 42.9584\n",
      "Epoch [33185/50000], Train Loss: 23.9509, Test Loss: 49.1181\n",
      "Epoch [33190/50000], Train Loss: 20.1856, Test Loss: 45.4558\n",
      "Epoch [33195/50000], Train Loss: 22.4956, Test Loss: 44.3285\n",
      "Epoch [33200/50000], Train Loss: 23.1528, Test Loss: 42.9193\n",
      "Epoch [33205/50000], Train Loss: 26.6508, Test Loss: 43.9510\n",
      "Epoch [33210/50000], Train Loss: 18.1926, Test Loss: 45.6843\n",
      "Epoch [33215/50000], Train Loss: 26.0446, Test Loss: 43.2185\n",
      "Epoch [33220/50000], Train Loss: 41.0231, Test Loss: 44.0181\n",
      "Epoch [33225/50000], Train Loss: 24.0969, Test Loss: 46.2393\n",
      "Epoch [33230/50000], Train Loss: 26.2909, Test Loss: 44.3179\n",
      "Epoch [33235/50000], Train Loss: 22.9864, Test Loss: 45.3934\n",
      "Epoch [33240/50000], Train Loss: 20.7082, Test Loss: 43.8508\n",
      "Epoch [33245/50000], Train Loss: 23.8587, Test Loss: 50.6440\n",
      "Epoch [33250/50000], Train Loss: 26.1875, Test Loss: 44.0904\n",
      "Epoch [33255/50000], Train Loss: 42.3197, Test Loss: 43.5364\n",
      "Epoch [33260/50000], Train Loss: 29.3994, Test Loss: 43.3655\n",
      "Epoch [33265/50000], Train Loss: 29.2406, Test Loss: 43.4990\n",
      "Epoch [33270/50000], Train Loss: 29.9939, Test Loss: 43.8687\n",
      "Epoch [33275/50000], Train Loss: 32.0144, Test Loss: 42.8832\n",
      "Epoch [33280/50000], Train Loss: 32.6821, Test Loss: 43.1643\n",
      "Epoch [33285/50000], Train Loss: 43.5144, Test Loss: 43.6147\n",
      "Epoch [33290/50000], Train Loss: 21.3795, Test Loss: 48.2767\n",
      "Epoch [33295/50000], Train Loss: 32.5804, Test Loss: 44.0287\n",
      "Epoch [33300/50000], Train Loss: 71.8361, Test Loss: 44.1205\n",
      "Epoch [33305/50000], Train Loss: 28.0602, Test Loss: 43.6713\n",
      "Epoch [33310/50000], Train Loss: 16.8683, Test Loss: 43.8235\n",
      "Epoch [33315/50000], Train Loss: 20.7673, Test Loss: 46.1551\n",
      "Epoch [33320/50000], Train Loss: 16.3405, Test Loss: 45.7033\n",
      "Epoch [33325/50000], Train Loss: 24.0702, Test Loss: 43.2468\n",
      "Epoch [33330/50000], Train Loss: 20.5292, Test Loss: 46.6672\n",
      "Epoch [33335/50000], Train Loss: 21.0958, Test Loss: 49.0779\n",
      "Epoch [33340/50000], Train Loss: 28.3958, Test Loss: 45.5650\n",
      "Epoch [33345/50000], Train Loss: 24.4541, Test Loss: 46.4160\n",
      "Epoch [33350/50000], Train Loss: 25.5531, Test Loss: 43.9296\n",
      "Epoch [33355/50000], Train Loss: 21.6311, Test Loss: 43.8352\n",
      "Epoch [33360/50000], Train Loss: 25.3546, Test Loss: 45.0530\n",
      "Epoch [33365/50000], Train Loss: 19.6175, Test Loss: 42.7231\n",
      "Epoch [33370/50000], Train Loss: 48.0619, Test Loss: 50.0798\n",
      "Epoch [33375/50000], Train Loss: 25.0421, Test Loss: 50.9301\n",
      "Epoch [33380/50000], Train Loss: 20.9363, Test Loss: 45.7504\n",
      "Epoch [33385/50000], Train Loss: 22.9442, Test Loss: 45.8008\n",
      "Epoch [33390/50000], Train Loss: 29.6121, Test Loss: 44.3978\n",
      "Epoch [33395/50000], Train Loss: 23.5456, Test Loss: 44.9974\n",
      "Epoch [33400/50000], Train Loss: 24.6745, Test Loss: 44.5288\n",
      "Epoch [33405/50000], Train Loss: 21.4703, Test Loss: 43.0354\n",
      "Epoch [33410/50000], Train Loss: 23.0582, Test Loss: 43.3116\n",
      "Epoch [33415/50000], Train Loss: 22.3014, Test Loss: 48.9664\n",
      "Epoch [33420/50000], Train Loss: 24.3975, Test Loss: 51.6521\n",
      "Epoch [33425/50000], Train Loss: 33.8688, Test Loss: 43.5610\n",
      "Epoch [33430/50000], Train Loss: 19.3491, Test Loss: 43.3166\n",
      "Epoch [33435/50000], Train Loss: 25.1647, Test Loss: 44.9600\n",
      "Epoch [33440/50000], Train Loss: 21.8991, Test Loss: 47.4142\n",
      "Epoch [33445/50000], Train Loss: 22.8222, Test Loss: 45.6895\n",
      "Epoch [33450/50000], Train Loss: 20.0707, Test Loss: 43.4083\n",
      "Epoch [33455/50000], Train Loss: 21.1131, Test Loss: 47.1685\n",
      "Epoch [33460/50000], Train Loss: 24.6537, Test Loss: 45.2055\n",
      "Epoch [33465/50000], Train Loss: 22.8437, Test Loss: 45.5242\n",
      "Epoch [33470/50000], Train Loss: 23.8932, Test Loss: 44.4645\n",
      "Epoch [33475/50000], Train Loss: 32.1502, Test Loss: 57.7531\n",
      "Epoch [33480/50000], Train Loss: 32.9193, Test Loss: 47.9037\n",
      "Epoch [33485/50000], Train Loss: 22.4333, Test Loss: 46.2260\n",
      "Epoch [33490/50000], Train Loss: 25.1504, Test Loss: 46.3550\n",
      "Epoch [33495/50000], Train Loss: 25.8010, Test Loss: 43.8033\n",
      "Epoch [33500/50000], Train Loss: 30.2018, Test Loss: 43.0096\n",
      "Epoch [33505/50000], Train Loss: 23.8349, Test Loss: 49.2257\n",
      "Epoch [33510/50000], Train Loss: 23.6528, Test Loss: 51.1802\n",
      "Epoch [33515/50000], Train Loss: 23.3182, Test Loss: 44.6011\n",
      "Epoch [33520/50000], Train Loss: 19.0821, Test Loss: 45.0224\n",
      "Epoch [33525/50000], Train Loss: 27.9134, Test Loss: 44.1607\n",
      "Epoch [33530/50000], Train Loss: 29.7039, Test Loss: 43.1610\n",
      "Epoch [33535/50000], Train Loss: 21.7253, Test Loss: 48.3574\n",
      "Epoch [33540/50000], Train Loss: 24.2649, Test Loss: 45.5276\n",
      "Epoch [33545/50000], Train Loss: 27.6055, Test Loss: 53.9638\n",
      "Epoch [33550/50000], Train Loss: 27.8240, Test Loss: 44.3960\n",
      "Epoch [33555/50000], Train Loss: 23.1884, Test Loss: 42.8598\n",
      "Epoch [33560/50000], Train Loss: 28.7297, Test Loss: 44.6403\n",
      "Epoch [33565/50000], Train Loss: 38.1083, Test Loss: 46.7762\n",
      "Epoch [33570/50000], Train Loss: 24.1530, Test Loss: 50.1939\n",
      "Epoch [33575/50000], Train Loss: 45.7217, Test Loss: 44.0359\n",
      "Epoch [33580/50000], Train Loss: 22.6312, Test Loss: 44.6460\n",
      "Epoch [33585/50000], Train Loss: 28.1689, Test Loss: 49.0049\n",
      "Epoch [33590/50000], Train Loss: 25.9522, Test Loss: 54.8825\n",
      "Epoch [33595/50000], Train Loss: 21.2307, Test Loss: 46.8074\n",
      "Epoch [33600/50000], Train Loss: 22.3876, Test Loss: 49.5307\n",
      "Epoch [33605/50000], Train Loss: 20.2636, Test Loss: 46.7428\n",
      "Epoch [33610/50000], Train Loss: 25.5899, Test Loss: 43.4306\n",
      "Epoch [33615/50000], Train Loss: 22.8716, Test Loss: 42.8695\n",
      "Epoch [33620/50000], Train Loss: 54.8013, Test Loss: 43.3042\n",
      "Epoch [33625/50000], Train Loss: 37.6380, Test Loss: 42.4993\n",
      "Epoch [33630/50000], Train Loss: 24.6500, Test Loss: 46.0168\n",
      "Epoch [33635/50000], Train Loss: 26.5808, Test Loss: 44.1425\n",
      "Epoch [33640/50000], Train Loss: 24.8977, Test Loss: 44.3245\n",
      "Epoch [33645/50000], Train Loss: 22.0719, Test Loss: 44.3779\n",
      "Epoch [33650/50000], Train Loss: 23.4793, Test Loss: 43.8663\n",
      "Epoch [33655/50000], Train Loss: 19.4194, Test Loss: 44.1304\n",
      "Epoch [33660/50000], Train Loss: 24.3611, Test Loss: 46.0342\n",
      "Epoch [33665/50000], Train Loss: 27.0075, Test Loss: 49.6697\n",
      "Epoch [33670/50000], Train Loss: 26.0118, Test Loss: 45.5350\n",
      "Epoch [33675/50000], Train Loss: 20.7045, Test Loss: 44.2333\n",
      "Epoch [33680/50000], Train Loss: 21.2246, Test Loss: 51.0328\n",
      "Epoch [33685/50000], Train Loss: 19.1990, Test Loss: 42.7593\n",
      "Epoch [33690/50000], Train Loss: 22.8679, Test Loss: 43.7092\n",
      "Epoch [33695/50000], Train Loss: 26.2112, Test Loss: 44.7355\n",
      "Epoch [33700/50000], Train Loss: 42.1114, Test Loss: 43.6059\n",
      "Epoch [33705/50000], Train Loss: 23.8385, Test Loss: 43.7575\n",
      "Epoch [33710/50000], Train Loss: 24.9486, Test Loss: 45.3948\n",
      "Epoch [33715/50000], Train Loss: 22.4800, Test Loss: 44.2014\n",
      "Epoch [33720/50000], Train Loss: 24.7487, Test Loss: 43.9893\n",
      "Epoch [33725/50000], Train Loss: 24.3154, Test Loss: 44.9192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33730/50000], Train Loss: 24.7682, Test Loss: 46.3724\n",
      "Epoch [33735/50000], Train Loss: 26.2670, Test Loss: 47.2299\n",
      "Epoch [33740/50000], Train Loss: 29.7765, Test Loss: 43.4028\n",
      "Epoch [33745/50000], Train Loss: 21.3620, Test Loss: 43.6987\n",
      "Epoch [33750/50000], Train Loss: 23.0626, Test Loss: 49.5818\n",
      "Epoch [33755/50000], Train Loss: 21.2244, Test Loss: 44.7373\n",
      "Epoch [33760/50000], Train Loss: 33.0451, Test Loss: 43.9835\n",
      "Epoch [33765/50000], Train Loss: 22.7317, Test Loss: 45.3162\n",
      "Epoch [33770/50000], Train Loss: 23.6349, Test Loss: 43.7688\n",
      "Epoch [33775/50000], Train Loss: 22.2097, Test Loss: 42.6459\n",
      "Epoch [33780/50000], Train Loss: 18.4658, Test Loss: 44.6798\n",
      "Epoch [33785/50000], Train Loss: 25.9112, Test Loss: 43.6660\n",
      "Epoch [33790/50000], Train Loss: 24.9216, Test Loss: 44.5976\n",
      "Epoch [33795/50000], Train Loss: 27.0747, Test Loss: 67.8965\n",
      "Epoch [33800/50000], Train Loss: 27.8096, Test Loss: 57.4704\n",
      "Epoch [33805/50000], Train Loss: 25.2885, Test Loss: 47.7165\n",
      "Epoch [33810/50000], Train Loss: 28.9006, Test Loss: 43.5497\n",
      "Epoch [33815/50000], Train Loss: 36.0224, Test Loss: 42.9314\n",
      "Epoch [33820/50000], Train Loss: 35.9478, Test Loss: 43.1480\n",
      "Epoch [33825/50000], Train Loss: 29.6726, Test Loss: 44.8961\n",
      "Epoch [33830/50000], Train Loss: 21.6238, Test Loss: 44.4377\n",
      "Epoch [33835/50000], Train Loss: 22.4966, Test Loss: 43.5805\n",
      "Epoch [33840/50000], Train Loss: 26.2405, Test Loss: 43.1818\n",
      "Epoch [33845/50000], Train Loss: 26.2451, Test Loss: 42.7568\n",
      "Epoch [33850/50000], Train Loss: 27.0325, Test Loss: 44.0902\n",
      "Epoch [33855/50000], Train Loss: 17.7461, Test Loss: 43.3296\n",
      "Epoch [33860/50000], Train Loss: 21.1029, Test Loss: 45.6101\n",
      "Epoch [33865/50000], Train Loss: 37.7556, Test Loss: 42.2851\n",
      "Epoch [33870/50000], Train Loss: 23.8097, Test Loss: 50.8289\n",
      "Epoch [33875/50000], Train Loss: 30.1857, Test Loss: 42.8963\n",
      "Epoch [33880/50000], Train Loss: 47.7903, Test Loss: 44.4550\n",
      "Epoch [33885/50000], Train Loss: 23.7053, Test Loss: 44.5665\n",
      "Epoch [33890/50000], Train Loss: 27.2292, Test Loss: 76.1957\n",
      "Epoch [33895/50000], Train Loss: 26.4168, Test Loss: 44.2108\n",
      "Epoch [33900/50000], Train Loss: 21.6795, Test Loss: 43.5704\n",
      "Epoch [33905/50000], Train Loss: 21.2219, Test Loss: 44.2054\n",
      "Epoch [33910/50000], Train Loss: 26.4941, Test Loss: 43.2067\n",
      "Epoch [33915/50000], Train Loss: 23.9308, Test Loss: 44.2189\n",
      "Epoch [33920/50000], Train Loss: 27.2732, Test Loss: 45.7355\n",
      "Epoch [33925/50000], Train Loss: 20.2550, Test Loss: 42.8631\n",
      "Epoch [33930/50000], Train Loss: 24.7698, Test Loss: 43.1233\n",
      "Epoch [33935/50000], Train Loss: 47.0099, Test Loss: 45.7258\n",
      "Epoch [33940/50000], Train Loss: 24.8369, Test Loss: 42.4996\n",
      "Epoch [33945/50000], Train Loss: 22.4712, Test Loss: 57.6467\n",
      "Epoch [33950/50000], Train Loss: 31.9318, Test Loss: 44.2855\n",
      "Epoch [33955/50000], Train Loss: 23.7350, Test Loss: 47.3044\n",
      "Epoch [33960/50000], Train Loss: 19.1769, Test Loss: 42.5132\n",
      "Epoch [33965/50000], Train Loss: 27.1148, Test Loss: 43.9114\n",
      "Epoch [33970/50000], Train Loss: 23.7394, Test Loss: 45.9772\n",
      "Epoch [33975/50000], Train Loss: 24.0192, Test Loss: 43.0429\n",
      "Epoch [33980/50000], Train Loss: 32.6823, Test Loss: 47.6194\n",
      "Epoch [33985/50000], Train Loss: 26.0709, Test Loss: 45.4450\n",
      "Epoch [33990/50000], Train Loss: 17.3147, Test Loss: 42.3213\n",
      "Epoch [33995/50000], Train Loss: 22.1463, Test Loss: 43.6379\n",
      "Epoch [34000/50000], Train Loss: 25.6781, Test Loss: 45.0173\n",
      "Epoch [34005/50000], Train Loss: 18.9346, Test Loss: 47.7312\n",
      "Epoch [34010/50000], Train Loss: 29.0683, Test Loss: 46.3280\n",
      "Epoch [34015/50000], Train Loss: 26.9582, Test Loss: 46.1078\n",
      "Epoch [34020/50000], Train Loss: 22.5479, Test Loss: 43.6696\n",
      "Epoch [34025/50000], Train Loss: 25.8564, Test Loss: 47.3623\n",
      "Epoch [34030/50000], Train Loss: 32.0868, Test Loss: 43.3152\n",
      "Epoch [34035/50000], Train Loss: 29.4918, Test Loss: 43.0165\n",
      "Epoch [34040/50000], Train Loss: 25.4448, Test Loss: 45.2229\n",
      "Epoch [34045/50000], Train Loss: 23.6847, Test Loss: 45.9744\n",
      "Epoch [34050/50000], Train Loss: 20.4890, Test Loss: 43.7351\n",
      "Epoch [34055/50000], Train Loss: 22.3153, Test Loss: 45.1773\n",
      "Epoch [34060/50000], Train Loss: 18.8710, Test Loss: 43.2721\n",
      "Epoch [34065/50000], Train Loss: 21.4036, Test Loss: 43.3837\n",
      "Epoch [34070/50000], Train Loss: 26.6933, Test Loss: 43.7332\n",
      "Epoch [34075/50000], Train Loss: 21.8707, Test Loss: 43.9199\n",
      "Epoch [34080/50000], Train Loss: 26.2100, Test Loss: 46.2969\n",
      "Epoch [34085/50000], Train Loss: 35.1024, Test Loss: 42.8113\n",
      "Epoch [34090/50000], Train Loss: 26.6270, Test Loss: 43.1130\n",
      "Epoch [34095/50000], Train Loss: 28.5209, Test Loss: 44.0663\n",
      "Epoch [34100/50000], Train Loss: 33.8658, Test Loss: 44.9270\n",
      "Epoch [34105/50000], Train Loss: 26.2686, Test Loss: 45.5471\n",
      "Epoch [34110/50000], Train Loss: 20.2495, Test Loss: 43.7962\n",
      "Epoch [34115/50000], Train Loss: 26.8237, Test Loss: 50.9195\n",
      "Epoch [34120/50000], Train Loss: 24.1075, Test Loss: 44.5571\n",
      "Epoch [34125/50000], Train Loss: 22.2191, Test Loss: 44.1970\n",
      "Epoch [34130/50000], Train Loss: 24.2632, Test Loss: 43.5389\n",
      "Epoch [34135/50000], Train Loss: 21.8529, Test Loss: 45.4719\n",
      "Epoch [34140/50000], Train Loss: 26.4669, Test Loss: 43.8403\n",
      "Epoch [34145/50000], Train Loss: 22.4112, Test Loss: 45.1271\n",
      "Epoch [34150/50000], Train Loss: 28.9699, Test Loss: 43.4072\n",
      "Epoch [34155/50000], Train Loss: 26.1997, Test Loss: 44.7560\n",
      "Epoch [34160/50000], Train Loss: 65.2543, Test Loss: 42.8541\n",
      "Epoch [34165/50000], Train Loss: 25.0602, Test Loss: 44.0887\n",
      "Epoch [34170/50000], Train Loss: 19.5197, Test Loss: 44.9349\n",
      "Epoch [34175/50000], Train Loss: 27.7961, Test Loss: 44.6857\n",
      "Epoch [34180/50000], Train Loss: 23.0459, Test Loss: 42.3930\n",
      "Epoch [34185/50000], Train Loss: 21.2931, Test Loss: 45.1188\n",
      "Epoch [34190/50000], Train Loss: 24.4261, Test Loss: 47.3404\n",
      "Epoch [34195/50000], Train Loss: 25.9716, Test Loss: 43.5643\n",
      "Epoch [34200/50000], Train Loss: 23.0803, Test Loss: 43.6977\n",
      "Epoch [34205/50000], Train Loss: 25.1730, Test Loss: 43.5392\n",
      "Epoch [34210/50000], Train Loss: 23.5697, Test Loss: 46.5238\n",
      "Epoch [34215/50000], Train Loss: 24.4203, Test Loss: 44.3167\n",
      "Epoch [34220/50000], Train Loss: 20.0558, Test Loss: 42.6273\n",
      "Epoch [34225/50000], Train Loss: 25.9870, Test Loss: 43.4575\n",
      "Epoch [34230/50000], Train Loss: 24.9689, Test Loss: 49.0307\n",
      "Epoch [34235/50000], Train Loss: 23.4656, Test Loss: 44.7691\n",
      "Epoch [34240/50000], Train Loss: 28.4868, Test Loss: 45.5356\n",
      "Epoch [34245/50000], Train Loss: 21.7263, Test Loss: 47.5878\n",
      "Epoch [34250/50000], Train Loss: 19.9045, Test Loss: 43.6421\n",
      "Epoch [34255/50000], Train Loss: 27.0154, Test Loss: 44.6139\n",
      "Epoch [34260/50000], Train Loss: 28.8330, Test Loss: 48.1077\n",
      "Epoch [34265/50000], Train Loss: 22.5376, Test Loss: 44.4962\n",
      "Epoch [34270/50000], Train Loss: 21.6260, Test Loss: 43.1824\n",
      "Epoch [34275/50000], Train Loss: 22.8967, Test Loss: 45.2566\n",
      "Epoch [34280/50000], Train Loss: 23.0550, Test Loss: 45.2322\n",
      "Epoch [34285/50000], Train Loss: 24.3733, Test Loss: 42.9019\n",
      "Epoch [34290/50000], Train Loss: 27.5473, Test Loss: 43.4948\n",
      "Epoch [34295/50000], Train Loss: 25.5041, Test Loss: 53.8300\n",
      "Epoch [34300/50000], Train Loss: 26.1017, Test Loss: 46.3700\n",
      "Epoch [34305/50000], Train Loss: 21.5347, Test Loss: 43.9250\n",
      "Epoch [34310/50000], Train Loss: 26.1761, Test Loss: 45.3897\n",
      "Epoch [34315/50000], Train Loss: 28.1687, Test Loss: 42.7595\n",
      "Epoch [34320/50000], Train Loss: 35.6847, Test Loss: 97.2242\n",
      "Epoch [34325/50000], Train Loss: 22.3028, Test Loss: 46.6189\n",
      "Epoch [34330/50000], Train Loss: 26.0179, Test Loss: 48.0306\n",
      "Epoch [34335/50000], Train Loss: 21.1543, Test Loss: 42.9933\n",
      "Epoch [34340/50000], Train Loss: 23.0463, Test Loss: 42.6173\n",
      "Epoch [34345/50000], Train Loss: 23.4544, Test Loss: 43.6273\n",
      "Epoch [34350/50000], Train Loss: 23.1890, Test Loss: 45.2499\n",
      "Epoch [34355/50000], Train Loss: 24.3424, Test Loss: 45.0785\n",
      "Epoch [34360/50000], Train Loss: 25.7875, Test Loss: 46.4627\n",
      "Epoch [34365/50000], Train Loss: 24.6491, Test Loss: 45.1977\n",
      "Epoch [34370/50000], Train Loss: 20.3914, Test Loss: 46.0926\n",
      "Epoch [34375/50000], Train Loss: 23.8257, Test Loss: 48.6910\n",
      "Epoch [34380/50000], Train Loss: 23.0064, Test Loss: 44.4561\n",
      "Epoch [34385/50000], Train Loss: 17.1867, Test Loss: 43.6876\n",
      "Epoch [34390/50000], Train Loss: 22.0120, Test Loss: 46.5771\n",
      "Epoch [34395/50000], Train Loss: 24.8446, Test Loss: 47.7679\n",
      "Epoch [34400/50000], Train Loss: 18.5837, Test Loss: 44.1546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34405/50000], Train Loss: 23.6883, Test Loss: 47.6951\n",
      "Epoch [34410/50000], Train Loss: 19.6177, Test Loss: 44.6637\n",
      "Epoch [34415/50000], Train Loss: 28.5100, Test Loss: 44.8892\n",
      "Epoch [34420/50000], Train Loss: 24.4348, Test Loss: 45.7742\n",
      "Epoch [34425/50000], Train Loss: 35.5413, Test Loss: 43.0035\n",
      "Epoch [34430/50000], Train Loss: 26.8164, Test Loss: 51.6909\n",
      "Epoch [34435/50000], Train Loss: 20.4371, Test Loss: 43.9845\n",
      "Epoch [34440/50000], Train Loss: 25.2753, Test Loss: 47.2223\n",
      "Epoch [34445/50000], Train Loss: 42.9130, Test Loss: 45.7883\n",
      "Epoch [34450/50000], Train Loss: 25.5633, Test Loss: 44.5321\n",
      "Epoch [34455/50000], Train Loss: 26.9078, Test Loss: 43.0386\n",
      "Epoch [34460/50000], Train Loss: 22.9369, Test Loss: 42.8719\n",
      "Epoch [34465/50000], Train Loss: 20.6124, Test Loss: 50.0551\n",
      "Epoch [34470/50000], Train Loss: 25.6943, Test Loss: 45.3912\n",
      "Epoch [34475/50000], Train Loss: 22.8442, Test Loss: 42.8593\n",
      "Epoch [34480/50000], Train Loss: 27.2167, Test Loss: 44.6553\n",
      "Epoch [34485/50000], Train Loss: 23.6901, Test Loss: 43.3807\n",
      "Epoch [34490/50000], Train Loss: 33.1797, Test Loss: 68.4373\n",
      "Epoch [34495/50000], Train Loss: 22.7123, Test Loss: 47.0436\n",
      "Epoch [34500/50000], Train Loss: 26.5089, Test Loss: 42.5087\n",
      "Epoch [34505/50000], Train Loss: 27.6603, Test Loss: 44.0512\n",
      "Epoch [34510/50000], Train Loss: 49.2899, Test Loss: 43.7894\n",
      "Epoch [34515/50000], Train Loss: 26.6003, Test Loss: 45.2451\n",
      "Epoch [34520/50000], Train Loss: 22.1667, Test Loss: 48.8656\n",
      "Epoch [34525/50000], Train Loss: 20.7461, Test Loss: 42.6362\n",
      "Epoch [34530/50000], Train Loss: 27.0211, Test Loss: 45.0885\n",
      "Epoch [34535/50000], Train Loss: 22.7180, Test Loss: 43.4411\n",
      "Epoch [34540/50000], Train Loss: 24.9182, Test Loss: 43.5573\n",
      "Epoch [34545/50000], Train Loss: 20.6511, Test Loss: 44.0249\n",
      "Epoch [34550/50000], Train Loss: 21.4219, Test Loss: 46.5759\n",
      "Epoch [34555/50000], Train Loss: 18.7394, Test Loss: 48.2189\n",
      "Epoch [34560/50000], Train Loss: 23.0419, Test Loss: 50.4065\n",
      "Epoch [34565/50000], Train Loss: 21.9264, Test Loss: 44.6757\n",
      "Epoch [34570/50000], Train Loss: 23.6221, Test Loss: 47.4917\n",
      "Epoch [34575/50000], Train Loss: 26.2987, Test Loss: 49.9994\n",
      "Epoch [34580/50000], Train Loss: 23.8658, Test Loss: 44.8275\n",
      "Epoch [34585/50000], Train Loss: 24.5396, Test Loss: 45.6220\n",
      "Epoch [34590/50000], Train Loss: 18.5825, Test Loss: 44.9552\n",
      "Epoch [34595/50000], Train Loss: 25.5208, Test Loss: 48.0451\n",
      "Epoch [34600/50000], Train Loss: 22.5834, Test Loss: 43.0817\n",
      "Epoch [34605/50000], Train Loss: 19.1119, Test Loss: 43.9600\n",
      "Epoch [34610/50000], Train Loss: 27.6728, Test Loss: 44.0545\n",
      "Epoch [34615/50000], Train Loss: 23.6967, Test Loss: 43.5213\n",
      "Epoch [34620/50000], Train Loss: 23.4557, Test Loss: 46.3050\n",
      "Epoch [34625/50000], Train Loss: 26.5432, Test Loss: 45.3438\n",
      "Epoch [34630/50000], Train Loss: 27.1030, Test Loss: 45.7836\n",
      "Epoch [34635/50000], Train Loss: 24.1054, Test Loss: 44.5442\n",
      "Epoch [34640/50000], Train Loss: 22.0643, Test Loss: 45.7898\n",
      "Epoch [34645/50000], Train Loss: 24.4207, Test Loss: 43.1652\n",
      "Epoch [34650/50000], Train Loss: 22.5704, Test Loss: 44.3532\n",
      "Epoch [34655/50000], Train Loss: 20.7731, Test Loss: 42.6233\n",
      "Epoch [34660/50000], Train Loss: 22.3368, Test Loss: 43.8001\n",
      "Epoch [34665/50000], Train Loss: 35.0641, Test Loss: 43.7969\n",
      "Epoch [34670/50000], Train Loss: 24.2609, Test Loss: 42.9024\n",
      "Epoch [34675/50000], Train Loss: 31.7581, Test Loss: 44.5872\n",
      "Epoch [34680/50000], Train Loss: 22.8623, Test Loss: 44.1567\n",
      "Epoch [34685/50000], Train Loss: 31.4662, Test Loss: 47.2775\n",
      "Epoch [34690/50000], Train Loss: 20.9892, Test Loss: 45.2070\n",
      "Epoch [34695/50000], Train Loss: 25.5571, Test Loss: 44.9561\n",
      "Epoch [34700/50000], Train Loss: 19.1062, Test Loss: 46.1687\n",
      "Epoch [34705/50000], Train Loss: 20.9338, Test Loss: 43.7700\n",
      "Epoch [34710/50000], Train Loss: 25.9768, Test Loss: 48.7104\n",
      "Epoch [34715/50000], Train Loss: 56.4112, Test Loss: 42.9324\n",
      "Epoch [34720/50000], Train Loss: 25.7130, Test Loss: 44.4056\n",
      "Epoch [34725/50000], Train Loss: 24.9944, Test Loss: 45.4499\n",
      "Epoch [34730/50000], Train Loss: 23.4487, Test Loss: 43.5931\n",
      "Epoch [34735/50000], Train Loss: 20.8930, Test Loss: 44.8215\n",
      "Epoch [34740/50000], Train Loss: 54.5372, Test Loss: 43.2522\n",
      "Epoch [34745/50000], Train Loss: 89.6969, Test Loss: 42.7987\n",
      "Epoch [34750/50000], Train Loss: 21.2699, Test Loss: 46.5472\n",
      "Epoch [34755/50000], Train Loss: 73.7808, Test Loss: 51.3955\n",
      "Epoch [34760/50000], Train Loss: 23.0947, Test Loss: 46.0782\n",
      "Epoch [34765/50000], Train Loss: 21.2159, Test Loss: 45.2187\n",
      "Epoch [34770/50000], Train Loss: 21.4037, Test Loss: 46.0521\n",
      "Epoch [34775/50000], Train Loss: 44.9077, Test Loss: 43.3392\n",
      "Epoch [34780/50000], Train Loss: 23.6387, Test Loss: 43.9712\n",
      "Epoch [34785/50000], Train Loss: 23.4281, Test Loss: 49.6141\n",
      "Epoch [34790/50000], Train Loss: 17.8491, Test Loss: 43.2370\n",
      "Epoch [34795/50000], Train Loss: 23.1019, Test Loss: 44.3345\n",
      "Epoch [34800/50000], Train Loss: 24.1260, Test Loss: 44.9937\n",
      "Epoch [34805/50000], Train Loss: 34.0199, Test Loss: 127.6629\n",
      "Epoch [34810/50000], Train Loss: 25.0995, Test Loss: 45.8749\n",
      "Epoch [34815/50000], Train Loss: 22.6586, Test Loss: 46.6438\n",
      "Epoch [34820/50000], Train Loss: 24.6706, Test Loss: 44.7313\n",
      "Epoch [34825/50000], Train Loss: 24.9882, Test Loss: 42.5944\n",
      "Epoch [34830/50000], Train Loss: 21.5449, Test Loss: 48.3044\n",
      "Epoch [34835/50000], Train Loss: 27.0955, Test Loss: 43.1721\n",
      "Epoch [34840/50000], Train Loss: 18.3117, Test Loss: 42.8080\n",
      "Epoch [34845/50000], Train Loss: 25.1951, Test Loss: 45.0177\n",
      "Epoch [34850/50000], Train Loss: 19.2087, Test Loss: 44.3394\n",
      "Epoch [34855/50000], Train Loss: 21.4103, Test Loss: 46.1931\n",
      "Epoch [34860/50000], Train Loss: 39.4994, Test Loss: 43.7346\n",
      "Epoch [34865/50000], Train Loss: 25.8884, Test Loss: 42.2093\n",
      "Epoch [34870/50000], Train Loss: 20.0311, Test Loss: 43.8942\n",
      "Epoch [34875/50000], Train Loss: 22.4647, Test Loss: 44.5784\n",
      "Epoch [34880/50000], Train Loss: 24.2333, Test Loss: 44.0774\n",
      "Epoch [34885/50000], Train Loss: 22.9707, Test Loss: 43.0954\n",
      "Epoch [34890/50000], Train Loss: 51.9062, Test Loss: 42.8995\n",
      "Epoch [34895/50000], Train Loss: 21.1709, Test Loss: 43.5627\n",
      "Epoch [34900/50000], Train Loss: 20.9804, Test Loss: 43.2916\n",
      "Epoch [34905/50000], Train Loss: 30.1791, Test Loss: 45.1958\n",
      "Epoch [34910/50000], Train Loss: 26.1429, Test Loss: 44.8386\n",
      "Epoch [34915/50000], Train Loss: 25.1323, Test Loss: 43.5441\n",
      "Epoch [34920/50000], Train Loss: 25.0043, Test Loss: 44.9417\n",
      "Epoch [34925/50000], Train Loss: 26.7369, Test Loss: 45.4958\n",
      "Epoch [34930/50000], Train Loss: 22.7930, Test Loss: 47.8207\n",
      "Epoch [34935/50000], Train Loss: 15.9475, Test Loss: 43.2079\n",
      "Epoch [34940/50000], Train Loss: 25.0998, Test Loss: 44.1124\n",
      "Epoch [34945/50000], Train Loss: 21.6315, Test Loss: 43.6747\n",
      "Epoch [34950/50000], Train Loss: 22.1422, Test Loss: 42.7666\n",
      "Epoch [34955/50000], Train Loss: 21.9102, Test Loss: 46.5004\n",
      "Epoch [34960/50000], Train Loss: 22.1837, Test Loss: 44.5336\n",
      "Epoch [34965/50000], Train Loss: 14.2217, Test Loss: 42.5516\n",
      "Epoch [34970/50000], Train Loss: 22.3825, Test Loss: 43.2103\n",
      "Epoch [34975/50000], Train Loss: 19.2093, Test Loss: 51.8870\n",
      "Epoch [34980/50000], Train Loss: 17.7052, Test Loss: 47.1645\n",
      "Epoch [34985/50000], Train Loss: 19.7959, Test Loss: 46.3355\n",
      "Epoch [34990/50000], Train Loss: 21.9846, Test Loss: 43.6055\n",
      "Epoch [34995/50000], Train Loss: 22.5209, Test Loss: 43.4228\n",
      "Epoch [35000/50000], Train Loss: 22.3278, Test Loss: 42.1391\n",
      "Epoch [35005/50000], Train Loss: 23.3141, Test Loss: 47.2261\n",
      "Epoch [35010/50000], Train Loss: 21.3057, Test Loss: 48.7561\n",
      "Epoch [35015/50000], Train Loss: 20.4496, Test Loss: 43.5132\n",
      "Epoch [35020/50000], Train Loss: 24.0501, Test Loss: 47.2126\n",
      "Epoch [35025/50000], Train Loss: 28.4562, Test Loss: 42.7641\n",
      "Epoch [35030/50000], Train Loss: 22.4569, Test Loss: 42.5945\n",
      "Epoch [35035/50000], Train Loss: 20.8549, Test Loss: 43.7749\n",
      "Epoch [35040/50000], Train Loss: 19.7224, Test Loss: 43.0328\n",
      "Epoch [35045/50000], Train Loss: 22.4026, Test Loss: 46.3785\n",
      "Epoch [35050/50000], Train Loss: 25.8029, Test Loss: 45.1822\n",
      "Epoch [35055/50000], Train Loss: 23.7485, Test Loss: 44.3644\n",
      "Epoch [35060/50000], Train Loss: 28.1325, Test Loss: 42.3121\n",
      "Epoch [35065/50000], Train Loss: 22.4299, Test Loss: 45.4081\n",
      "Epoch [35070/50000], Train Loss: 22.3033, Test Loss: 46.5769\n",
      "Epoch [35075/50000], Train Loss: 23.9642, Test Loss: 44.1014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35080/50000], Train Loss: 24.4231, Test Loss: 44.1605\n",
      "Epoch [35085/50000], Train Loss: 23.5790, Test Loss: 44.7080\n",
      "Epoch [35090/50000], Train Loss: 21.1219, Test Loss: 47.2235\n",
      "Epoch [35095/50000], Train Loss: 23.5469, Test Loss: 43.2475\n",
      "Epoch [35100/50000], Train Loss: 27.4639, Test Loss: 46.1839\n",
      "Epoch [35105/50000], Train Loss: 26.1390, Test Loss: 43.8364\n",
      "Epoch [35110/50000], Train Loss: 18.0476, Test Loss: 41.7890\n",
      "Epoch [35115/50000], Train Loss: 22.1456, Test Loss: 44.5609\n",
      "Epoch [35120/50000], Train Loss: 24.5163, Test Loss: 42.4739\n",
      "Epoch [35125/50000], Train Loss: 26.8977, Test Loss: 43.4472\n",
      "Epoch [35130/50000], Train Loss: 32.3381, Test Loss: 42.5484\n",
      "Epoch [35135/50000], Train Loss: 22.2841, Test Loss: 43.6877\n",
      "Epoch [35140/50000], Train Loss: 23.1654, Test Loss: 43.2360\n",
      "Epoch [35145/50000], Train Loss: 22.8352, Test Loss: 43.3830\n",
      "Epoch [35150/50000], Train Loss: 35.5183, Test Loss: 43.8322\n",
      "Epoch [35155/50000], Train Loss: 21.7987, Test Loss: 45.5291\n",
      "Epoch [35160/50000], Train Loss: 22.2895, Test Loss: 42.8540\n",
      "Epoch [35165/50000], Train Loss: 22.1760, Test Loss: 43.9330\n",
      "Epoch [35170/50000], Train Loss: 23.3133, Test Loss: 42.7485\n",
      "Epoch [35175/50000], Train Loss: 26.3845, Test Loss: 43.7796\n",
      "Epoch [35180/50000], Train Loss: 22.0309, Test Loss: 42.0316\n",
      "Epoch [35185/50000], Train Loss: 22.6935, Test Loss: 44.7274\n",
      "Epoch [35190/50000], Train Loss: 24.9735, Test Loss: 53.2239\n",
      "Epoch [35195/50000], Train Loss: 25.9584, Test Loss: 44.0012\n",
      "Epoch [35200/50000], Train Loss: 19.4059, Test Loss: 43.5193\n",
      "Epoch [35205/50000], Train Loss: 23.0643, Test Loss: 51.4424\n",
      "Epoch [35210/50000], Train Loss: 15.7172, Test Loss: 41.8937\n",
      "Epoch [35215/50000], Train Loss: 22.7323, Test Loss: 48.8559\n",
      "Epoch [35220/50000], Train Loss: 27.7363, Test Loss: 43.1286\n",
      "Epoch [35225/50000], Train Loss: 24.2724, Test Loss: 45.4954\n",
      "Epoch [35230/50000], Train Loss: 22.4880, Test Loss: 43.1287\n",
      "Epoch [35235/50000], Train Loss: 17.8002, Test Loss: 44.0805\n",
      "Epoch [35240/50000], Train Loss: 19.0127, Test Loss: 44.1809\n",
      "Epoch [35245/50000], Train Loss: 27.2973, Test Loss: 46.3494\n",
      "Epoch [35250/50000], Train Loss: 25.4357, Test Loss: 47.1135\n",
      "Epoch [35255/50000], Train Loss: 21.6092, Test Loss: 43.7135\n",
      "Epoch [35260/50000], Train Loss: 22.6157, Test Loss: 45.6867\n",
      "Epoch [35265/50000], Train Loss: 23.7171, Test Loss: 41.8359\n",
      "Epoch [35270/50000], Train Loss: 26.0760, Test Loss: 43.5326\n",
      "Epoch [35275/50000], Train Loss: 21.0808, Test Loss: 42.2107\n",
      "Epoch [35280/50000], Train Loss: 23.7141, Test Loss: 45.2745\n",
      "Epoch [35285/50000], Train Loss: 18.7723, Test Loss: 42.9687\n",
      "Epoch [35290/50000], Train Loss: 20.3951, Test Loss: 42.8458\n",
      "Epoch [35295/50000], Train Loss: 22.3875, Test Loss: 44.1857\n",
      "Epoch [35300/50000], Train Loss: 21.7250, Test Loss: 44.8897\n",
      "Epoch [35305/50000], Train Loss: 22.0407, Test Loss: 46.0441\n",
      "Epoch [35310/50000], Train Loss: 22.9755, Test Loss: 43.5961\n",
      "Epoch [35315/50000], Train Loss: 45.5509, Test Loss: 42.9199\n",
      "Epoch [35320/50000], Train Loss: 22.0684, Test Loss: 43.3490\n",
      "Epoch [35325/50000], Train Loss: 21.2485, Test Loss: 50.7045\n",
      "Epoch [35330/50000], Train Loss: 20.9160, Test Loss: 41.6801\n",
      "Epoch [35335/50000], Train Loss: 22.8410, Test Loss: 45.0868\n",
      "Epoch [35340/50000], Train Loss: 24.8013, Test Loss: 46.0871\n",
      "Epoch [35345/50000], Train Loss: 25.6634, Test Loss: 45.2909\n",
      "Epoch [35350/50000], Train Loss: 46.3731, Test Loss: 43.7223\n",
      "Epoch [35355/50000], Train Loss: 25.0370, Test Loss: 44.1671\n",
      "Epoch [35360/50000], Train Loss: 22.5047, Test Loss: 43.7401\n",
      "Epoch [35365/50000], Train Loss: 25.5859, Test Loss: 43.7632\n",
      "Epoch [35370/50000], Train Loss: 22.6251, Test Loss: 43.9562\n",
      "Epoch [35375/50000], Train Loss: 20.5306, Test Loss: 42.1461\n",
      "Epoch [35380/50000], Train Loss: 32.0617, Test Loss: 45.0308\n",
      "Epoch [35385/50000], Train Loss: 22.1451, Test Loss: 48.6147\n",
      "Epoch [35390/50000], Train Loss: 27.2866, Test Loss: 44.6321\n",
      "Epoch [35395/50000], Train Loss: 23.9709, Test Loss: 49.5320\n",
      "Epoch [35400/50000], Train Loss: 20.9480, Test Loss: 43.4705\n",
      "Epoch [35405/50000], Train Loss: 27.9882, Test Loss: 47.8893\n",
      "Epoch [35410/50000], Train Loss: 20.8417, Test Loss: 45.1530\n",
      "Epoch [35415/50000], Train Loss: 23.9490, Test Loss: 45.7624\n",
      "Epoch [35420/50000], Train Loss: 22.7694, Test Loss: 46.1288\n",
      "Epoch [35425/50000], Train Loss: 22.7183, Test Loss: 43.9537\n",
      "Epoch [35430/50000], Train Loss: 24.7344, Test Loss: 47.9141\n",
      "Epoch [35435/50000], Train Loss: 26.4032, Test Loss: 45.5114\n",
      "Epoch [35440/50000], Train Loss: 21.8597, Test Loss: 47.3048\n",
      "Epoch [35445/50000], Train Loss: 23.4346, Test Loss: 43.3773\n",
      "Epoch [35450/50000], Train Loss: 16.5838, Test Loss: 43.6585\n",
      "Epoch [35455/50000], Train Loss: 20.2639, Test Loss: 42.2060\n",
      "Epoch [35460/50000], Train Loss: 23.7438, Test Loss: 44.3785\n",
      "Epoch [35465/50000], Train Loss: 42.5123, Test Loss: 46.5803\n",
      "Epoch [35470/50000], Train Loss: 21.1455, Test Loss: 45.9494\n",
      "Epoch [35475/50000], Train Loss: 21.1612, Test Loss: 42.4145\n",
      "Epoch [35480/50000], Train Loss: 20.4138, Test Loss: 43.6278\n",
      "Epoch [35485/50000], Train Loss: 19.4762, Test Loss: 43.3503\n",
      "Epoch [35490/50000], Train Loss: 32.8012, Test Loss: 43.6612\n",
      "Epoch [35495/50000], Train Loss: 32.5116, Test Loss: 43.3386\n",
      "Epoch [35500/50000], Train Loss: 34.0092, Test Loss: 43.9755\n",
      "Epoch [35505/50000], Train Loss: 23.9115, Test Loss: 47.6650\n",
      "Epoch [35510/50000], Train Loss: 25.7044, Test Loss: 46.1984\n",
      "Epoch [35515/50000], Train Loss: 32.0171, Test Loss: 44.2698\n",
      "Epoch [35520/50000], Train Loss: 22.6693, Test Loss: 48.0888\n",
      "Epoch [35525/50000], Train Loss: 20.3579, Test Loss: 43.9729\n",
      "Epoch [35530/50000], Train Loss: 22.8775, Test Loss: 43.7317\n",
      "Epoch [35535/50000], Train Loss: 20.9942, Test Loss: 44.5898\n",
      "Epoch [35540/50000], Train Loss: 20.2465, Test Loss: 44.3136\n",
      "Epoch [35545/50000], Train Loss: 27.8696, Test Loss: 54.2891\n",
      "Epoch [35550/50000], Train Loss: 20.7814, Test Loss: 44.6573\n",
      "Epoch [35555/50000], Train Loss: 32.0219, Test Loss: 53.0229\n",
      "Epoch [35560/50000], Train Loss: 21.8286, Test Loss: 43.5773\n",
      "Epoch [35565/50000], Train Loss: 20.8992, Test Loss: 43.5575\n",
      "Epoch [35570/50000], Train Loss: 23.6777, Test Loss: 44.4702\n",
      "Epoch [35575/50000], Train Loss: 25.1191, Test Loss: 42.3582\n",
      "Epoch [35580/50000], Train Loss: 21.5305, Test Loss: 43.7431\n",
      "Epoch [35585/50000], Train Loss: 25.9738, Test Loss: 43.1030\n",
      "Epoch [35590/50000], Train Loss: 31.2888, Test Loss: 43.8879\n",
      "Epoch [35595/50000], Train Loss: 22.5377, Test Loss: 43.4287\n",
      "Epoch [35600/50000], Train Loss: 21.2398, Test Loss: 43.7164\n",
      "Epoch [35605/50000], Train Loss: 21.4243, Test Loss: 49.5574\n",
      "Epoch [35610/50000], Train Loss: 26.8713, Test Loss: 45.7326\n",
      "Epoch [35615/50000], Train Loss: 23.6693, Test Loss: 43.2187\n",
      "Epoch [35620/50000], Train Loss: 24.6773, Test Loss: 47.7221\n",
      "Epoch [35625/50000], Train Loss: 17.9080, Test Loss: 46.2082\n",
      "Epoch [35630/50000], Train Loss: 22.0895, Test Loss: 46.4926\n",
      "Epoch [35635/50000], Train Loss: 21.9447, Test Loss: 43.1565\n",
      "Epoch [35640/50000], Train Loss: 25.4148, Test Loss: 54.7279\n",
      "Epoch [35645/50000], Train Loss: 35.7289, Test Loss: 43.5721\n",
      "Epoch [35650/50000], Train Loss: 29.3594, Test Loss: 42.5666\n",
      "Epoch [35655/50000], Train Loss: 23.4611, Test Loss: 42.4770\n",
      "Epoch [35660/50000], Train Loss: 24.6423, Test Loss: 44.2978\n",
      "Epoch [35665/50000], Train Loss: 27.7711, Test Loss: 44.2781\n",
      "Epoch [35670/50000], Train Loss: 21.0287, Test Loss: 45.0816\n",
      "Epoch [35675/50000], Train Loss: 22.0742, Test Loss: 44.3475\n",
      "Epoch [35680/50000], Train Loss: 23.0915, Test Loss: 46.7824\n",
      "Epoch [35685/50000], Train Loss: 17.1787, Test Loss: 43.6405\n",
      "Epoch [35690/50000], Train Loss: 24.7772, Test Loss: 44.1105\n",
      "Epoch [35695/50000], Train Loss: 24.1041, Test Loss: 47.2557\n",
      "Epoch [35700/50000], Train Loss: 25.9724, Test Loss: 43.4523\n",
      "Epoch [35705/50000], Train Loss: 19.4531, Test Loss: 46.8340\n",
      "Epoch [35710/50000], Train Loss: 37.0430, Test Loss: 43.3416\n",
      "Epoch [35715/50000], Train Loss: 23.2036, Test Loss: 43.2752\n",
      "Epoch [35720/50000], Train Loss: 23.8252, Test Loss: 44.4436\n",
      "Epoch [35725/50000], Train Loss: 18.1032, Test Loss: 47.1555\n",
      "Epoch [35730/50000], Train Loss: 23.3696, Test Loss: 43.9006\n",
      "Epoch [35735/50000], Train Loss: 22.5228, Test Loss: 42.9843\n",
      "Epoch [35740/50000], Train Loss: 22.4456, Test Loss: 45.6047\n",
      "Epoch [35745/50000], Train Loss: 22.5403, Test Loss: 45.2063\n",
      "Epoch [35750/50000], Train Loss: 31.4197, Test Loss: 43.2625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35755/50000], Train Loss: 19.1824, Test Loss: 44.4622\n",
      "Epoch [35760/50000], Train Loss: 21.4729, Test Loss: 42.7399\n",
      "Epoch [35765/50000], Train Loss: 25.7143, Test Loss: 42.6064\n",
      "Epoch [35770/50000], Train Loss: 26.3725, Test Loss: 44.0650\n",
      "Epoch [35775/50000], Train Loss: 14.8963, Test Loss: 41.4469\n",
      "Epoch [35780/50000], Train Loss: 24.1274, Test Loss: 44.4595\n",
      "Epoch [35785/50000], Train Loss: 29.5594, Test Loss: 46.4878\n",
      "Epoch [35790/50000], Train Loss: 24.7062, Test Loss: 42.9240\n",
      "Epoch [35795/50000], Train Loss: 19.8373, Test Loss: 43.4384\n",
      "Epoch [35800/50000], Train Loss: 20.4386, Test Loss: 44.7382\n",
      "Epoch [35805/50000], Train Loss: 21.4762, Test Loss: 44.3520\n",
      "Epoch [35810/50000], Train Loss: 18.9734, Test Loss: 43.2320\n",
      "Epoch [35815/50000], Train Loss: 21.7081, Test Loss: 44.3727\n",
      "Epoch [35820/50000], Train Loss: 31.3896, Test Loss: 43.3116\n",
      "Epoch [35825/50000], Train Loss: 21.2344, Test Loss: 43.2093\n",
      "Epoch [35830/50000], Train Loss: 23.6456, Test Loss: 44.5647\n",
      "Epoch [35835/50000], Train Loss: 18.3550, Test Loss: 43.8466\n",
      "Epoch [35840/50000], Train Loss: 18.0189, Test Loss: 44.7514\n",
      "Epoch [35845/50000], Train Loss: 22.1850, Test Loss: 43.7095\n",
      "Epoch [35850/50000], Train Loss: 29.5092, Test Loss: 44.1853\n",
      "Epoch [35855/50000], Train Loss: 23.4274, Test Loss: 41.6926\n",
      "Epoch [35860/50000], Train Loss: 20.2802, Test Loss: 43.9930\n",
      "Epoch [35865/50000], Train Loss: 23.0512, Test Loss: 42.6975\n",
      "Epoch [35870/50000], Train Loss: 23.2137, Test Loss: 43.8661\n",
      "Epoch [35875/50000], Train Loss: 25.8777, Test Loss: 44.1842\n",
      "Epoch [35880/50000], Train Loss: 18.0756, Test Loss: 43.6963\n",
      "Epoch [35885/50000], Train Loss: 19.0836, Test Loss: 43.6167\n",
      "Epoch [35890/50000], Train Loss: 27.0616, Test Loss: 46.8092\n",
      "Epoch [35895/50000], Train Loss: 28.7739, Test Loss: 43.0270\n",
      "Epoch [35900/50000], Train Loss: 24.9173, Test Loss: 43.1359\n",
      "Epoch [35905/50000], Train Loss: 22.7630, Test Loss: 45.3601\n",
      "Epoch [35910/50000], Train Loss: 23.7720, Test Loss: 44.1231\n",
      "Epoch [35915/50000], Train Loss: 23.8916, Test Loss: 43.5607\n",
      "Epoch [35920/50000], Train Loss: 26.2603, Test Loss: 45.9191\n",
      "Epoch [35925/50000], Train Loss: 20.7122, Test Loss: 42.6191\n",
      "Epoch [35930/50000], Train Loss: 20.7273, Test Loss: 44.1767\n",
      "Epoch [35935/50000], Train Loss: 21.5619, Test Loss: 43.9837\n",
      "Epoch [35940/50000], Train Loss: 18.8818, Test Loss: 42.9421\n",
      "Epoch [35945/50000], Train Loss: 22.7648, Test Loss: 45.2446\n",
      "Epoch [35950/50000], Train Loss: 32.9624, Test Loss: 48.1088\n",
      "Epoch [35955/50000], Train Loss: 18.4612, Test Loss: 42.0323\n",
      "Epoch [35960/50000], Train Loss: 23.8839, Test Loss: 44.0449\n",
      "Epoch [35965/50000], Train Loss: 17.6375, Test Loss: 43.2381\n",
      "Epoch [35970/50000], Train Loss: 27.4093, Test Loss: 42.6765\n",
      "Epoch [35975/50000], Train Loss: 33.8066, Test Loss: 41.6915\n",
      "Epoch [35980/50000], Train Loss: 23.8132, Test Loss: 49.1769\n",
      "Epoch [35985/50000], Train Loss: 25.0885, Test Loss: 49.0525\n",
      "Epoch [35990/50000], Train Loss: 23.5636, Test Loss: 42.3772\n",
      "Epoch [35995/50000], Train Loss: 23.3759, Test Loss: 47.1603\n",
      "Epoch [36000/50000], Train Loss: 24.7660, Test Loss: 51.4377\n",
      "Epoch [36005/50000], Train Loss: 28.9021, Test Loss: 42.4401\n",
      "Epoch [36010/50000], Train Loss: 21.5766, Test Loss: 46.4350\n",
      "Epoch [36015/50000], Train Loss: 21.9755, Test Loss: 45.8487\n",
      "Epoch [36020/50000], Train Loss: 26.8602, Test Loss: 43.0893\n",
      "Epoch [36025/50000], Train Loss: 27.0511, Test Loss: 53.9541\n",
      "Epoch [36030/50000], Train Loss: 25.5268, Test Loss: 43.0333\n",
      "Epoch [36035/50000], Train Loss: 23.4188, Test Loss: 44.0779\n",
      "Epoch [36040/50000], Train Loss: 25.0353, Test Loss: 46.6184\n",
      "Epoch [36045/50000], Train Loss: 18.2833, Test Loss: 41.5675\n",
      "Epoch [36050/50000], Train Loss: 22.3903, Test Loss: 44.8828\n",
      "Epoch [36055/50000], Train Loss: 27.4259, Test Loss: 45.2320\n",
      "Epoch [36060/50000], Train Loss: 27.0842, Test Loss: 43.8006\n",
      "Epoch [36065/50000], Train Loss: 23.4485, Test Loss: 42.7605\n",
      "Epoch [36070/50000], Train Loss: 26.2072, Test Loss: 45.0985\n",
      "Epoch [36075/50000], Train Loss: 24.7267, Test Loss: 47.0452\n",
      "Epoch [36080/50000], Train Loss: 26.8228, Test Loss: 45.2220\n",
      "Epoch [36085/50000], Train Loss: 21.0977, Test Loss: 42.2726\n",
      "Epoch [36090/50000], Train Loss: 29.4201, Test Loss: 42.1849\n",
      "Epoch [36095/50000], Train Loss: 24.5373, Test Loss: 44.3989\n",
      "Epoch [36100/50000], Train Loss: 27.5092, Test Loss: 45.9865\n",
      "Epoch [36105/50000], Train Loss: 21.7053, Test Loss: 42.5419\n",
      "Epoch [36110/50000], Train Loss: 41.7459, Test Loss: 42.8458\n",
      "Epoch [36115/50000], Train Loss: 29.4987, Test Loss: 43.4190\n",
      "Epoch [36120/50000], Train Loss: 19.0555, Test Loss: 44.6156\n",
      "Epoch [36125/50000], Train Loss: 23.8998, Test Loss: 42.5227\n",
      "Epoch [36130/50000], Train Loss: 25.1327, Test Loss: 43.3365\n",
      "Epoch [36135/50000], Train Loss: 19.4953, Test Loss: 42.4626\n",
      "Epoch [36140/50000], Train Loss: 21.2270, Test Loss: 42.8556\n",
      "Epoch [36145/50000], Train Loss: 26.3336, Test Loss: 44.3842\n",
      "Epoch [36150/50000], Train Loss: 22.4747, Test Loss: 49.3469\n",
      "Epoch [36155/50000], Train Loss: 25.0393, Test Loss: 43.9424\n",
      "Epoch [36160/50000], Train Loss: 20.6907, Test Loss: 43.0715\n",
      "Epoch [36165/50000], Train Loss: 22.3798, Test Loss: 44.7664\n",
      "Epoch [36170/50000], Train Loss: 20.3502, Test Loss: 46.3055\n",
      "Epoch [36175/50000], Train Loss: 21.4771, Test Loss: 42.8540\n",
      "Epoch [36180/50000], Train Loss: 15.6206, Test Loss: 45.1545\n",
      "Epoch [36185/50000], Train Loss: 19.9991, Test Loss: 48.9483\n",
      "Epoch [36190/50000], Train Loss: 23.8227, Test Loss: 49.3367\n",
      "Epoch [36195/50000], Train Loss: 21.7754, Test Loss: 44.5957\n",
      "Epoch [36200/50000], Train Loss: 23.0627, Test Loss: 44.9286\n",
      "Epoch [36205/50000], Train Loss: 24.0442, Test Loss: 45.3728\n",
      "Epoch [36210/50000], Train Loss: 23.5067, Test Loss: 55.3502\n",
      "Epoch [36215/50000], Train Loss: 24.8454, Test Loss: 43.5767\n",
      "Epoch [36220/50000], Train Loss: 24.4050, Test Loss: 45.7900\n",
      "Epoch [36225/50000], Train Loss: 23.9773, Test Loss: 43.3295\n",
      "Epoch [36230/50000], Train Loss: 18.9306, Test Loss: 42.3167\n",
      "Epoch [36235/50000], Train Loss: 25.1540, Test Loss: 43.5838\n",
      "Epoch [36240/50000], Train Loss: 26.3840, Test Loss: 44.6228\n",
      "Epoch [36245/50000], Train Loss: 19.9466, Test Loss: 43.1414\n",
      "Epoch [36250/50000], Train Loss: 21.9258, Test Loss: 45.7495\n",
      "Epoch [36255/50000], Train Loss: 21.1049, Test Loss: 42.0524\n",
      "Epoch [36260/50000], Train Loss: 24.8056, Test Loss: 42.0712\n",
      "Epoch [36265/50000], Train Loss: 28.8258, Test Loss: 45.3355\n",
      "Epoch [36270/50000], Train Loss: 25.1283, Test Loss: 47.6532\n",
      "Epoch [36275/50000], Train Loss: 21.6900, Test Loss: 46.9254\n",
      "Epoch [36280/50000], Train Loss: 23.0995, Test Loss: 46.4193\n",
      "Epoch [36285/50000], Train Loss: 21.9217, Test Loss: 44.6930\n",
      "Epoch [36290/50000], Train Loss: 24.2894, Test Loss: 46.7264\n",
      "Epoch [36295/50000], Train Loss: 19.2178, Test Loss: 44.6101\n",
      "Epoch [36300/50000], Train Loss: 21.3072, Test Loss: 45.2593\n",
      "Epoch [36305/50000], Train Loss: 29.6467, Test Loss: 50.3000\n",
      "Epoch [36310/50000], Train Loss: 24.8360, Test Loss: 45.7100\n",
      "Epoch [36315/50000], Train Loss: 18.7253, Test Loss: 41.9093\n",
      "Epoch [36320/50000], Train Loss: 21.7907, Test Loss: 47.0300\n",
      "Epoch [36325/50000], Train Loss: 22.7542, Test Loss: 44.1632\n",
      "Epoch [36330/50000], Train Loss: 21.3554, Test Loss: 43.3923\n",
      "Epoch [36335/50000], Train Loss: 24.4453, Test Loss: 60.8021\n",
      "Epoch [36340/50000], Train Loss: 20.7176, Test Loss: 41.8698\n",
      "Epoch [36345/50000], Train Loss: 18.9770, Test Loss: 42.8748\n",
      "Epoch [36350/50000], Train Loss: 26.9684, Test Loss: 43.4348\n",
      "Epoch [36355/50000], Train Loss: 22.7025, Test Loss: 42.6584\n",
      "Epoch [36360/50000], Train Loss: 20.8204, Test Loss: 49.3244\n",
      "Epoch [36365/50000], Train Loss: 14.7446, Test Loss: 42.0436\n",
      "Epoch [36370/50000], Train Loss: 23.8674, Test Loss: 44.4010\n",
      "Epoch [36375/50000], Train Loss: 30.3258, Test Loss: 42.1935\n",
      "Epoch [36380/50000], Train Loss: 21.8055, Test Loss: 43.6369\n",
      "Epoch [36385/50000], Train Loss: 22.9551, Test Loss: 44.4401\n",
      "Epoch [36390/50000], Train Loss: 21.6576, Test Loss: 43.4615\n",
      "Epoch [36395/50000], Train Loss: 20.1271, Test Loss: 43.4117\n",
      "Epoch [36400/50000], Train Loss: 25.7303, Test Loss: 51.6835\n",
      "Epoch [36405/50000], Train Loss: 25.6082, Test Loss: 46.0159\n",
      "Epoch [36410/50000], Train Loss: 23.8524, Test Loss: 42.8799\n",
      "Epoch [36415/50000], Train Loss: 22.7525, Test Loss: 43.1154\n",
      "Epoch [36420/50000], Train Loss: 23.5915, Test Loss: 44.9200\n",
      "Epoch [36425/50000], Train Loss: 19.8573, Test Loss: 43.0033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36430/50000], Train Loss: 22.2837, Test Loss: 44.7951\n",
      "Epoch [36435/50000], Train Loss: 23.9698, Test Loss: 42.3557\n",
      "Epoch [36440/50000], Train Loss: 23.1047, Test Loss: 45.7680\n",
      "Epoch [36445/50000], Train Loss: 28.3526, Test Loss: 43.5387\n",
      "Epoch [36450/50000], Train Loss: 21.9953, Test Loss: 43.2332\n",
      "Epoch [36455/50000], Train Loss: 21.3541, Test Loss: 47.7306\n",
      "Epoch [36460/50000], Train Loss: 20.5582, Test Loss: 42.7654\n",
      "Epoch [36465/50000], Train Loss: 26.7109, Test Loss: 43.0007\n",
      "Epoch [36470/50000], Train Loss: 20.1615, Test Loss: 47.1483\n",
      "Epoch [36475/50000], Train Loss: 23.3247, Test Loss: 42.0953\n",
      "Epoch [36480/50000], Train Loss: 18.4920, Test Loss: 45.4863\n",
      "Epoch [36485/50000], Train Loss: 22.8868, Test Loss: 42.5359\n",
      "Epoch [36490/50000], Train Loss: 20.8729, Test Loss: 42.5484\n",
      "Epoch [36495/50000], Train Loss: 21.2979, Test Loss: 43.4013\n",
      "Epoch [36500/50000], Train Loss: 18.4284, Test Loss: 41.5695\n",
      "Epoch [36505/50000], Train Loss: 28.3363, Test Loss: 47.5461\n",
      "Epoch [36510/50000], Train Loss: 24.4540, Test Loss: 46.9422\n",
      "Epoch [36515/50000], Train Loss: 26.8633, Test Loss: 42.8210\n",
      "Epoch [36520/50000], Train Loss: 22.9914, Test Loss: 44.4999\n",
      "Epoch [36525/50000], Train Loss: 20.8757, Test Loss: 44.1220\n",
      "Epoch [36530/50000], Train Loss: 25.7951, Test Loss: 42.7268\n",
      "Epoch [36535/50000], Train Loss: 21.6917, Test Loss: 42.5628\n",
      "Epoch [36540/50000], Train Loss: 19.8411, Test Loss: 43.8611\n",
      "Epoch [36545/50000], Train Loss: 26.1642, Test Loss: 42.8211\n",
      "Epoch [36550/50000], Train Loss: 23.7770, Test Loss: 44.3715\n",
      "Epoch [36555/50000], Train Loss: 20.9422, Test Loss: 42.4679\n",
      "Epoch [36560/50000], Train Loss: 22.4729, Test Loss: 44.1700\n",
      "Epoch [36565/50000], Train Loss: 25.7916, Test Loss: 46.3046\n",
      "Epoch [36570/50000], Train Loss: 20.7592, Test Loss: 44.9467\n",
      "Epoch [36575/50000], Train Loss: 19.2801, Test Loss: 43.9768\n",
      "Epoch [36580/50000], Train Loss: 18.7463, Test Loss: 42.4813\n",
      "Epoch [36585/50000], Train Loss: 16.3882, Test Loss: 44.0967\n",
      "Epoch [36590/50000], Train Loss: 23.3423, Test Loss: 43.1488\n",
      "Epoch [36595/50000], Train Loss: 21.2129, Test Loss: 44.0021\n",
      "Epoch [36600/50000], Train Loss: 29.1210, Test Loss: 59.2138\n",
      "Epoch [36605/50000], Train Loss: 23.5334, Test Loss: 43.5070\n",
      "Epoch [36610/50000], Train Loss: 21.5581, Test Loss: 46.1006\n",
      "Epoch [36615/50000], Train Loss: 22.3106, Test Loss: 45.0655\n",
      "Epoch [36620/50000], Train Loss: 25.5033, Test Loss: 47.4377\n",
      "Epoch [36625/50000], Train Loss: 19.0944, Test Loss: 42.8182\n",
      "Epoch [36630/50000], Train Loss: 31.1949, Test Loss: 44.3982\n",
      "Epoch [36635/50000], Train Loss: 21.9050, Test Loss: 46.9173\n",
      "Epoch [36640/50000], Train Loss: 23.1043, Test Loss: 46.2630\n",
      "Epoch [36645/50000], Train Loss: 23.6343, Test Loss: 44.3519\n",
      "Epoch [36650/50000], Train Loss: 20.2844, Test Loss: 43.5870\n",
      "Epoch [36655/50000], Train Loss: 20.4649, Test Loss: 47.9269\n",
      "Epoch [36660/50000], Train Loss: 22.8479, Test Loss: 42.5066\n",
      "Epoch [36665/50000], Train Loss: 24.4653, Test Loss: 42.4528\n",
      "Epoch [36670/50000], Train Loss: 34.5821, Test Loss: 43.8711\n",
      "Epoch [36675/50000], Train Loss: 30.5182, Test Loss: 41.6418\n",
      "Epoch [36680/50000], Train Loss: 58.6570, Test Loss: 43.0084\n",
      "Epoch [36685/50000], Train Loss: 23.2805, Test Loss: 42.8987\n",
      "Epoch [36690/50000], Train Loss: 24.8828, Test Loss: 45.1590\n",
      "Epoch [36695/50000], Train Loss: 21.0098, Test Loss: 42.8695\n",
      "Epoch [36700/50000], Train Loss: 29.1570, Test Loss: 50.9128\n",
      "Epoch [36705/50000], Train Loss: 17.7516, Test Loss: 44.2278\n",
      "Epoch [36710/50000], Train Loss: 22.5722, Test Loss: 44.9001\n",
      "Epoch [36715/50000], Train Loss: 22.1617, Test Loss: 44.2863\n",
      "Epoch [36720/50000], Train Loss: 20.5260, Test Loss: 43.8646\n",
      "Epoch [36725/50000], Train Loss: 22.2914, Test Loss: 43.4985\n",
      "Epoch [36730/50000], Train Loss: 50.6374, Test Loss: 42.9236\n",
      "Epoch [36735/50000], Train Loss: 23.5292, Test Loss: 44.9922\n",
      "Epoch [36740/50000], Train Loss: 39.8098, Test Loss: 45.5427\n",
      "Epoch [36745/50000], Train Loss: 24.8763, Test Loss: 42.4959\n",
      "Epoch [36750/50000], Train Loss: 27.5833, Test Loss: 44.3881\n",
      "Epoch [36755/50000], Train Loss: 44.2439, Test Loss: 41.8063\n",
      "Epoch [36760/50000], Train Loss: 20.5972, Test Loss: 46.5715\n",
      "Epoch [36765/50000], Train Loss: 20.0225, Test Loss: 43.1857\n",
      "Epoch [36770/50000], Train Loss: 19.4203, Test Loss: 42.6264\n",
      "Epoch [36775/50000], Train Loss: 23.2162, Test Loss: 49.9140\n",
      "Epoch [36780/50000], Train Loss: 28.7603, Test Loss: 46.4225\n",
      "Epoch [36785/50000], Train Loss: 21.9724, Test Loss: 45.1028\n",
      "Epoch [36790/50000], Train Loss: 22.6964, Test Loss: 44.4416\n",
      "Epoch [36795/50000], Train Loss: 19.3993, Test Loss: 45.6456\n",
      "Epoch [36800/50000], Train Loss: 22.2599, Test Loss: 44.5497\n",
      "Epoch [36805/50000], Train Loss: 26.8096, Test Loss: 42.1746\n",
      "Epoch [36810/50000], Train Loss: 22.6125, Test Loss: 48.1527\n",
      "Epoch [36815/50000], Train Loss: 27.5452, Test Loss: 47.0120\n",
      "Epoch [36820/50000], Train Loss: 21.6641, Test Loss: 44.2595\n",
      "Epoch [36825/50000], Train Loss: 18.8799, Test Loss: 50.7756\n",
      "Epoch [36830/50000], Train Loss: 16.8050, Test Loss: 43.2672\n",
      "Epoch [36835/50000], Train Loss: 25.2143, Test Loss: 43.1453\n",
      "Epoch [36840/50000], Train Loss: 21.2661, Test Loss: 45.9784\n",
      "Epoch [36845/50000], Train Loss: 22.1793, Test Loss: 46.4839\n",
      "Epoch [36850/50000], Train Loss: 19.6199, Test Loss: 42.6039\n",
      "Epoch [36855/50000], Train Loss: 19.1013, Test Loss: 49.7060\n",
      "Epoch [36860/50000], Train Loss: 22.0533, Test Loss: 41.3386\n",
      "Epoch [36865/50000], Train Loss: 19.4235, Test Loss: 43.2112\n",
      "Epoch [36870/50000], Train Loss: 19.6451, Test Loss: 43.9761\n",
      "Epoch [36875/50000], Train Loss: 21.7169, Test Loss: 43.5216\n",
      "Epoch [36880/50000], Train Loss: 23.0712, Test Loss: 42.4886\n",
      "Epoch [36885/50000], Train Loss: 22.3548, Test Loss: 43.8205\n",
      "Epoch [36890/50000], Train Loss: 31.8243, Test Loss: 42.9854\n",
      "Epoch [36895/50000], Train Loss: 28.4848, Test Loss: 42.5066\n",
      "Epoch [36900/50000], Train Loss: 25.8876, Test Loss: 44.5751\n",
      "Epoch [36905/50000], Train Loss: 22.2962, Test Loss: 43.2648\n",
      "Epoch [36910/50000], Train Loss: 25.8971, Test Loss: 46.1116\n",
      "Epoch [36915/50000], Train Loss: 19.4389, Test Loss: 42.2006\n",
      "Epoch [36920/50000], Train Loss: 33.4434, Test Loss: 42.4559\n",
      "Epoch [36925/50000], Train Loss: 21.7918, Test Loss: 46.5471\n",
      "Epoch [36930/50000], Train Loss: 23.5290, Test Loss: 46.1342\n",
      "Epoch [36935/50000], Train Loss: 24.2276, Test Loss: 42.5071\n",
      "Epoch [36940/50000], Train Loss: 21.0499, Test Loss: 42.9509\n",
      "Epoch [36945/50000], Train Loss: 23.0839, Test Loss: 42.6512\n",
      "Epoch [36950/50000], Train Loss: 24.3103, Test Loss: 43.5917\n",
      "Epoch [36955/50000], Train Loss: 26.3639, Test Loss: 43.8907\n",
      "Epoch [36960/50000], Train Loss: 26.7901, Test Loss: 45.2672\n",
      "Epoch [36965/50000], Train Loss: 22.8933, Test Loss: 43.9397\n",
      "Epoch [36970/50000], Train Loss: 34.6531, Test Loss: 45.3770\n",
      "Epoch [36975/50000], Train Loss: 22.0142, Test Loss: 43.3502\n",
      "Epoch [36980/50000], Train Loss: 21.4833, Test Loss: 43.2536\n",
      "Epoch [36985/50000], Train Loss: 26.5608, Test Loss: 44.1589\n",
      "Epoch [36990/50000], Train Loss: 21.9965, Test Loss: 48.3953\n",
      "Epoch [36995/50000], Train Loss: 36.4216, Test Loss: 43.1875\n",
      "Epoch [37000/50000], Train Loss: 18.4063, Test Loss: 43.7542\n",
      "Epoch [37005/50000], Train Loss: 23.2757, Test Loss: 46.8771\n",
      "Epoch [37010/50000], Train Loss: 29.7679, Test Loss: 53.5112\n",
      "Epoch [37015/50000], Train Loss: 22.3484, Test Loss: 41.6270\n",
      "Epoch [37020/50000], Train Loss: 23.4538, Test Loss: 45.5329\n",
      "Epoch [37025/50000], Train Loss: 19.4658, Test Loss: 44.2671\n",
      "Epoch [37030/50000], Train Loss: 21.2959, Test Loss: 43.6443\n",
      "Epoch [37035/50000], Train Loss: 20.4664, Test Loss: 44.7720\n",
      "Epoch [37040/50000], Train Loss: 27.3682, Test Loss: 43.6189\n",
      "Epoch [37045/50000], Train Loss: 24.7031, Test Loss: 46.0474\n",
      "Epoch [37050/50000], Train Loss: 30.0940, Test Loss: 42.0948\n",
      "Epoch [37055/50000], Train Loss: 25.3929, Test Loss: 46.4397\n",
      "Epoch [37060/50000], Train Loss: 18.5313, Test Loss: 42.4103\n",
      "Epoch [37065/50000], Train Loss: 19.5539, Test Loss: 43.8107\n",
      "Epoch [37070/50000], Train Loss: 22.6139, Test Loss: 45.8029\n",
      "Epoch [37075/50000], Train Loss: 19.4150, Test Loss: 71.0671\n",
      "Epoch [37080/50000], Train Loss: 35.3884, Test Loss: 42.8220\n",
      "Epoch [37085/50000], Train Loss: 28.7363, Test Loss: 52.3087\n",
      "Epoch [37090/50000], Train Loss: 21.3678, Test Loss: 41.8087\n",
      "Epoch [37095/50000], Train Loss: 19.2771, Test Loss: 42.4048\n",
      "Epoch [37100/50000], Train Loss: 22.2457, Test Loss: 43.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37105/50000], Train Loss: 21.6468, Test Loss: 43.4042\n",
      "Epoch [37110/50000], Train Loss: 29.5865, Test Loss: 44.9438\n",
      "Epoch [37115/50000], Train Loss: 25.1205, Test Loss: 42.5699\n",
      "Epoch [37120/50000], Train Loss: 24.4415, Test Loss: 43.9358\n",
      "Epoch [37125/50000], Train Loss: 19.4142, Test Loss: 43.4419\n",
      "Epoch [37130/50000], Train Loss: 25.9295, Test Loss: 42.6143\n",
      "Epoch [37135/50000], Train Loss: 21.8727, Test Loss: 45.5483\n",
      "Epoch [37140/50000], Train Loss: 25.4883, Test Loss: 47.0105\n",
      "Epoch [37145/50000], Train Loss: 20.4133, Test Loss: 43.5598\n",
      "Epoch [37150/50000], Train Loss: 24.9087, Test Loss: 44.4586\n",
      "Epoch [37155/50000], Train Loss: 12.8753, Test Loss: 41.2387\n",
      "Epoch [37160/50000], Train Loss: 19.6301, Test Loss: 42.9326\n",
      "Epoch [37165/50000], Train Loss: 25.9958, Test Loss: 43.1702\n",
      "Epoch [37170/50000], Train Loss: 17.1257, Test Loss: 42.9894\n",
      "Epoch [37175/50000], Train Loss: 23.1067, Test Loss: 46.1824\n",
      "Epoch [37180/50000], Train Loss: 23.4873, Test Loss: 46.1000\n",
      "Epoch [37185/50000], Train Loss: 18.4111, Test Loss: 43.0929\n",
      "Epoch [37190/50000], Train Loss: 20.1705, Test Loss: 46.1350\n",
      "Epoch [37195/50000], Train Loss: 19.8507, Test Loss: 44.6414\n",
      "Epoch [37200/50000], Train Loss: 22.5282, Test Loss: 42.2572\n",
      "Epoch [37205/50000], Train Loss: 22.9332, Test Loss: 41.4101\n",
      "Epoch [37210/50000], Train Loss: 23.0534, Test Loss: 42.1118\n",
      "Epoch [37215/50000], Train Loss: 57.4884, Test Loss: 59.2193\n",
      "Epoch [37220/50000], Train Loss: 19.8517, Test Loss: 58.1572\n",
      "Epoch [37225/50000], Train Loss: 20.7916, Test Loss: 41.3161\n",
      "Epoch [37230/50000], Train Loss: 25.7108, Test Loss: 46.2423\n",
      "Epoch [37235/50000], Train Loss: 18.0972, Test Loss: 45.0687\n",
      "Epoch [37240/50000], Train Loss: 23.1716, Test Loss: 46.4246\n",
      "Epoch [37245/50000], Train Loss: 20.2141, Test Loss: 42.5998\n",
      "Epoch [37250/50000], Train Loss: 25.7428, Test Loss: 42.0631\n",
      "Epoch [37255/50000], Train Loss: 46.2643, Test Loss: 45.6093\n",
      "Epoch [37260/50000], Train Loss: 46.9096, Test Loss: 45.2473\n",
      "Epoch [37265/50000], Train Loss: 20.2910, Test Loss: 42.6666\n",
      "Epoch [37270/50000], Train Loss: 20.7058, Test Loss: 45.3008\n",
      "Epoch [37275/50000], Train Loss: 23.2711, Test Loss: 43.4220\n",
      "Epoch [37280/50000], Train Loss: 21.8463, Test Loss: 47.1642\n",
      "Epoch [37285/50000], Train Loss: 25.3174, Test Loss: 44.3279\n",
      "Epoch [37290/50000], Train Loss: 23.3532, Test Loss: 43.1633\n",
      "Epoch [37295/50000], Train Loss: 20.0721, Test Loss: 44.5577\n",
      "Epoch [37300/50000], Train Loss: 19.2774, Test Loss: 41.7788\n",
      "Epoch [37305/50000], Train Loss: 19.0973, Test Loss: 43.4615\n",
      "Epoch [37310/50000], Train Loss: 19.3888, Test Loss: 41.8541\n",
      "Epoch [37315/50000], Train Loss: 23.0012, Test Loss: 43.9002\n",
      "Epoch [37320/50000], Train Loss: 22.2005, Test Loss: 43.8121\n",
      "Epoch [37325/50000], Train Loss: 38.4821, Test Loss: 43.9649\n",
      "Epoch [37330/50000], Train Loss: 21.3528, Test Loss: 42.9802\n",
      "Epoch [37335/50000], Train Loss: 22.6868, Test Loss: 61.9254\n",
      "Epoch [37340/50000], Train Loss: 26.4980, Test Loss: 50.7028\n",
      "Epoch [37345/50000], Train Loss: 22.5649, Test Loss: 42.7650\n",
      "Epoch [37350/50000], Train Loss: 25.0023, Test Loss: 44.6573\n",
      "Epoch [37355/50000], Train Loss: 25.0206, Test Loss: 41.8859\n",
      "Epoch [37360/50000], Train Loss: 25.5207, Test Loss: 46.0465\n",
      "Epoch [37365/50000], Train Loss: 23.3716, Test Loss: 42.8412\n",
      "Epoch [37370/50000], Train Loss: 21.1442, Test Loss: 46.7853\n",
      "Epoch [37375/50000], Train Loss: 23.1922, Test Loss: 43.7148\n",
      "Epoch [37380/50000], Train Loss: 20.6400, Test Loss: 46.8915\n",
      "Epoch [37385/50000], Train Loss: 17.3095, Test Loss: 42.5202\n",
      "Epoch [37390/50000], Train Loss: 17.4280, Test Loss: 44.6336\n",
      "Epoch [37395/50000], Train Loss: 25.7845, Test Loss: 43.7520\n",
      "Epoch [37400/50000], Train Loss: 20.9876, Test Loss: 43.5370\n",
      "Epoch [37405/50000], Train Loss: 22.6166, Test Loss: 46.7226\n",
      "Epoch [37410/50000], Train Loss: 19.2372, Test Loss: 45.7003\n",
      "Epoch [37415/50000], Train Loss: 23.6786, Test Loss: 44.6311\n",
      "Epoch [37420/50000], Train Loss: 22.1901, Test Loss: 42.5935\n",
      "Epoch [37425/50000], Train Loss: 20.7984, Test Loss: 43.1435\n",
      "Epoch [37430/50000], Train Loss: 29.2752, Test Loss: 43.3294\n",
      "Epoch [37435/50000], Train Loss: 20.4770, Test Loss: 43.0623\n",
      "Epoch [37440/50000], Train Loss: 32.4430, Test Loss: 44.3145\n",
      "Epoch [37445/50000], Train Loss: 18.3979, Test Loss: 43.5877\n",
      "Epoch [37450/50000], Train Loss: 21.9690, Test Loss: 47.0500\n",
      "Epoch [37455/50000], Train Loss: 22.2800, Test Loss: 44.7815\n",
      "Epoch [37460/50000], Train Loss: 28.8407, Test Loss: 46.8364\n",
      "Epoch [37465/50000], Train Loss: 18.7125, Test Loss: 47.3349\n",
      "Epoch [37470/50000], Train Loss: 19.2451, Test Loss: 50.4912\n",
      "Epoch [37475/50000], Train Loss: 45.2347, Test Loss: 42.0016\n",
      "Epoch [37480/50000], Train Loss: 20.4711, Test Loss: 41.8813\n",
      "Epoch [37485/50000], Train Loss: 18.2981, Test Loss: 45.8741\n",
      "Epoch [37490/50000], Train Loss: 16.5358, Test Loss: 45.1007\n",
      "Epoch [37495/50000], Train Loss: 24.8228, Test Loss: 41.7664\n",
      "Epoch [37500/50000], Train Loss: 27.6051, Test Loss: 46.4073\n",
      "Epoch [37505/50000], Train Loss: 19.1534, Test Loss: 49.9212\n",
      "Epoch [37510/50000], Train Loss: 23.2443, Test Loss: 46.2563\n",
      "Epoch [37515/50000], Train Loss: 24.5698, Test Loss: 44.7907\n",
      "Epoch [37520/50000], Train Loss: 21.0051, Test Loss: 42.7617\n",
      "Epoch [37525/50000], Train Loss: 25.1498, Test Loss: 44.7743\n",
      "Epoch [37530/50000], Train Loss: 23.9304, Test Loss: 44.9321\n",
      "Epoch [37535/50000], Train Loss: 23.6492, Test Loss: 42.7321\n",
      "Epoch [37540/50000], Train Loss: 21.1855, Test Loss: 41.8814\n",
      "Epoch [37545/50000], Train Loss: 16.9581, Test Loss: 42.0857\n",
      "Epoch [37550/50000], Train Loss: 20.8666, Test Loss: 44.7417\n",
      "Epoch [37555/50000], Train Loss: 23.6412, Test Loss: 46.1798\n",
      "Epoch [37560/50000], Train Loss: 19.0481, Test Loss: 41.6741\n",
      "Epoch [37565/50000], Train Loss: 20.6150, Test Loss: 45.8626\n",
      "Epoch [37570/50000], Train Loss: 24.7712, Test Loss: 43.4835\n",
      "Epoch [37575/50000], Train Loss: 25.7104, Test Loss: 43.9608\n",
      "Epoch [37580/50000], Train Loss: 21.4553, Test Loss: 43.6877\n",
      "Epoch [37585/50000], Train Loss: 21.4514, Test Loss: 42.1528\n",
      "Epoch [37590/50000], Train Loss: 17.5527, Test Loss: 49.8747\n",
      "Epoch [37595/50000], Train Loss: 22.0550, Test Loss: 42.3401\n",
      "Epoch [37600/50000], Train Loss: 25.4911, Test Loss: 43.8146\n",
      "Epoch [37605/50000], Train Loss: 24.1375, Test Loss: 56.6788\n",
      "Epoch [37610/50000], Train Loss: 18.9724, Test Loss: 42.8018\n",
      "Epoch [37615/50000], Train Loss: 21.2042, Test Loss: 43.3951\n",
      "Epoch [37620/50000], Train Loss: 25.8306, Test Loss: 52.1654\n",
      "Epoch [37625/50000], Train Loss: 26.5355, Test Loss: 42.6157\n",
      "Epoch [37630/50000], Train Loss: 23.7228, Test Loss: 46.9577\n",
      "Epoch [37635/50000], Train Loss: 31.4093, Test Loss: 44.6202\n",
      "Epoch [37640/50000], Train Loss: 23.4456, Test Loss: 43.7869\n",
      "Epoch [37645/50000], Train Loss: 23.3025, Test Loss: 43.8500\n",
      "Epoch [37650/50000], Train Loss: 23.0269, Test Loss: 44.2704\n",
      "Epoch [37655/50000], Train Loss: 23.3815, Test Loss: 45.8867\n",
      "Epoch [37660/50000], Train Loss: 21.5189, Test Loss: 45.1041\n",
      "Epoch [37665/50000], Train Loss: 17.8120, Test Loss: 41.9512\n",
      "Epoch [37670/50000], Train Loss: 21.9861, Test Loss: 49.9496\n",
      "Epoch [37675/50000], Train Loss: 21.3016, Test Loss: 42.1115\n",
      "Epoch [37680/50000], Train Loss: 20.8090, Test Loss: 43.1599\n",
      "Epoch [37685/50000], Train Loss: 23.8111, Test Loss: 43.2139\n",
      "Epoch [37690/50000], Train Loss: 19.5519, Test Loss: 42.4736\n",
      "Epoch [37695/50000], Train Loss: 23.1942, Test Loss: 47.5939\n",
      "Epoch [37700/50000], Train Loss: 31.0422, Test Loss: 41.0949\n",
      "Epoch [37705/50000], Train Loss: 23.7727, Test Loss: 48.1868\n",
      "Epoch [37710/50000], Train Loss: 18.3397, Test Loss: 44.0396\n",
      "Epoch [37715/50000], Train Loss: 22.6038, Test Loss: 43.9249\n",
      "Epoch [37720/50000], Train Loss: 25.9547, Test Loss: 44.1536\n",
      "Epoch [37725/50000], Train Loss: 26.6755, Test Loss: 56.0093\n",
      "Epoch [37730/50000], Train Loss: 17.3635, Test Loss: 43.3409\n",
      "Epoch [37735/50000], Train Loss: 24.9469, Test Loss: 42.4770\n",
      "Epoch [37740/50000], Train Loss: 20.2030, Test Loss: 43.5546\n",
      "Epoch [37745/50000], Train Loss: 22.1127, Test Loss: 43.4546\n",
      "Epoch [37750/50000], Train Loss: 18.3952, Test Loss: 43.6091\n",
      "Epoch [37755/50000], Train Loss: 19.6603, Test Loss: 43.6540\n",
      "Epoch [37760/50000], Train Loss: 20.2981, Test Loss: 41.8265\n",
      "Epoch [37765/50000], Train Loss: 31.2499, Test Loss: 46.9576\n",
      "Epoch [37770/50000], Train Loss: 23.0474, Test Loss: 45.1261\n",
      "Epoch [37775/50000], Train Loss: 18.4405, Test Loss: 48.1670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37780/50000], Train Loss: 28.8586, Test Loss: 48.3011\n",
      "Epoch [37785/50000], Train Loss: 20.8223, Test Loss: 42.9159\n",
      "Epoch [37790/50000], Train Loss: 21.8813, Test Loss: 41.8144\n",
      "Epoch [37795/50000], Train Loss: 21.5079, Test Loss: 42.5468\n",
      "Epoch [37800/50000], Train Loss: 24.5603, Test Loss: 43.8459\n",
      "Epoch [37805/50000], Train Loss: 16.2088, Test Loss: 41.6054\n",
      "Epoch [37810/50000], Train Loss: 23.9829, Test Loss: 41.7400\n",
      "Epoch [37815/50000], Train Loss: 56.2917, Test Loss: 53.2533\n",
      "Epoch [37820/50000], Train Loss: 21.1981, Test Loss: 45.4001\n",
      "Epoch [37825/50000], Train Loss: 20.9371, Test Loss: 42.1166\n",
      "Epoch [37830/50000], Train Loss: 22.4434, Test Loss: 43.4333\n",
      "Epoch [37835/50000], Train Loss: 25.0414, Test Loss: 43.6506\n",
      "Epoch [37840/50000], Train Loss: 25.4039, Test Loss: 45.0013\n",
      "Epoch [37845/50000], Train Loss: 20.1415, Test Loss: 42.4316\n",
      "Epoch [37850/50000], Train Loss: 20.6191, Test Loss: 43.3283\n",
      "Epoch [37855/50000], Train Loss: 19.8789, Test Loss: 45.1249\n",
      "Epoch [37860/50000], Train Loss: 26.0924, Test Loss: 43.8410\n",
      "Epoch [37865/50000], Train Loss: 23.4044, Test Loss: 42.9636\n",
      "Epoch [37870/50000], Train Loss: 20.1663, Test Loss: 43.2886\n",
      "Epoch [37875/50000], Train Loss: 23.0024, Test Loss: 44.3212\n",
      "Epoch [37880/50000], Train Loss: 22.8948, Test Loss: 44.7921\n",
      "Epoch [37885/50000], Train Loss: 24.4110, Test Loss: 45.6089\n",
      "Epoch [37890/50000], Train Loss: 35.0986, Test Loss: 44.6084\n",
      "Epoch [37895/50000], Train Loss: 20.9002, Test Loss: 45.2869\n",
      "Epoch [37900/50000], Train Loss: 21.3349, Test Loss: 44.1645\n",
      "Epoch [37905/50000], Train Loss: 19.2602, Test Loss: 42.5324\n",
      "Epoch [37910/50000], Train Loss: 22.7575, Test Loss: 41.8759\n",
      "Epoch [37915/50000], Train Loss: 20.5797, Test Loss: 43.7810\n",
      "Epoch [37920/50000], Train Loss: 19.5103, Test Loss: 47.8376\n",
      "Epoch [37925/50000], Train Loss: 19.3164, Test Loss: 42.6973\n",
      "Epoch [37930/50000], Train Loss: 19.3173, Test Loss: 44.0677\n",
      "Epoch [37935/50000], Train Loss: 36.3957, Test Loss: 42.4923\n",
      "Epoch [37940/50000], Train Loss: 23.6193, Test Loss: 42.1981\n",
      "Epoch [37945/50000], Train Loss: 31.7710, Test Loss: 42.2723\n",
      "Epoch [37950/50000], Train Loss: 22.8951, Test Loss: 48.2019\n",
      "Epoch [37955/50000], Train Loss: 24.6711, Test Loss: 51.5400\n",
      "Epoch [37960/50000], Train Loss: 32.4542, Test Loss: 46.5262\n",
      "Epoch [37965/50000], Train Loss: 27.6341, Test Loss: 45.2891\n",
      "Epoch [37970/50000], Train Loss: 21.3733, Test Loss: 44.3918\n",
      "Epoch [37975/50000], Train Loss: 26.2726, Test Loss: 42.1284\n",
      "Epoch [37980/50000], Train Loss: 20.9092, Test Loss: 44.1778\n",
      "Epoch [37985/50000], Train Loss: 29.4370, Test Loss: 42.4163\n",
      "Epoch [37990/50000], Train Loss: 30.6289, Test Loss: 41.9089\n",
      "Epoch [37995/50000], Train Loss: 25.2178, Test Loss: 46.0742\n",
      "Epoch [38000/50000], Train Loss: 20.3519, Test Loss: 46.5030\n",
      "Epoch [38005/50000], Train Loss: 22.0558, Test Loss: 43.6394\n",
      "Epoch [38010/50000], Train Loss: 23.0826, Test Loss: 42.6887\n",
      "Epoch [38015/50000], Train Loss: 23.2661, Test Loss: 41.9221\n",
      "Epoch [38020/50000], Train Loss: 23.0098, Test Loss: 52.5739\n",
      "Epoch [38025/50000], Train Loss: 25.8507, Test Loss: 43.3070\n",
      "Epoch [38030/50000], Train Loss: 21.7225, Test Loss: 44.4864\n",
      "Epoch [38035/50000], Train Loss: 21.0751, Test Loss: 43.1189\n",
      "Epoch [38040/50000], Train Loss: 20.5078, Test Loss: 46.0174\n",
      "Epoch [38045/50000], Train Loss: 22.1342, Test Loss: 45.0877\n",
      "Epoch [38050/50000], Train Loss: 35.7079, Test Loss: 41.7846\n",
      "Epoch [38055/50000], Train Loss: 23.5626, Test Loss: 42.5663\n",
      "Epoch [38060/50000], Train Loss: 17.8655, Test Loss: 42.3099\n",
      "Epoch [38065/50000], Train Loss: 24.1188, Test Loss: 43.9958\n",
      "Epoch [38070/50000], Train Loss: 22.4994, Test Loss: 45.2508\n",
      "Epoch [38075/50000], Train Loss: 20.4325, Test Loss: 42.5008\n",
      "Epoch [38080/50000], Train Loss: 19.1484, Test Loss: 41.6016\n",
      "Epoch [38085/50000], Train Loss: 23.9557, Test Loss: 42.8716\n",
      "Epoch [38090/50000], Train Loss: 17.9919, Test Loss: 44.4846\n",
      "Epoch [38095/50000], Train Loss: 22.2698, Test Loss: 47.8701\n",
      "Epoch [38100/50000], Train Loss: 24.4250, Test Loss: 43.5890\n",
      "Epoch [38105/50000], Train Loss: 20.1159, Test Loss: 42.9459\n",
      "Epoch [38110/50000], Train Loss: 22.9958, Test Loss: 43.3910\n",
      "Epoch [38115/50000], Train Loss: 20.0135, Test Loss: 42.9228\n",
      "Epoch [38120/50000], Train Loss: 20.1455, Test Loss: 41.3851\n",
      "Epoch [38125/50000], Train Loss: 22.3134, Test Loss: 42.6746\n",
      "Epoch [38130/50000], Train Loss: 19.5826, Test Loss: 47.5338\n",
      "Epoch [38135/50000], Train Loss: 35.7227, Test Loss: 45.2071\n",
      "Epoch [38140/50000], Train Loss: 20.5513, Test Loss: 45.8594\n",
      "Epoch [38145/50000], Train Loss: 24.1378, Test Loss: 43.7847\n",
      "Epoch [38150/50000], Train Loss: 23.9976, Test Loss: 43.1019\n",
      "Epoch [38155/50000], Train Loss: 22.6737, Test Loss: 41.8142\n",
      "Epoch [38160/50000], Train Loss: 23.1900, Test Loss: 43.5769\n",
      "Epoch [38165/50000], Train Loss: 19.0769, Test Loss: 42.4856\n",
      "Epoch [38170/50000], Train Loss: 19.9332, Test Loss: 44.6567\n",
      "Epoch [38175/50000], Train Loss: 24.7490, Test Loss: 43.2607\n",
      "Epoch [38180/50000], Train Loss: 22.2843, Test Loss: 49.3508\n",
      "Epoch [38185/50000], Train Loss: 20.2026, Test Loss: 45.4761\n",
      "Epoch [38190/50000], Train Loss: 24.2757, Test Loss: 44.9161\n",
      "Epoch [38195/50000], Train Loss: 34.0632, Test Loss: 44.3253\n",
      "Epoch [38200/50000], Train Loss: 19.2733, Test Loss: 43.5190\n",
      "Epoch [38205/50000], Train Loss: 19.5151, Test Loss: 43.3987\n",
      "Epoch [38210/50000], Train Loss: 33.1623, Test Loss: 41.6605\n",
      "Epoch [38215/50000], Train Loss: 20.6354, Test Loss: 44.1487\n",
      "Epoch [38220/50000], Train Loss: 20.9000, Test Loss: 44.5311\n",
      "Epoch [38225/50000], Train Loss: 25.4623, Test Loss: 41.6487\n",
      "Epoch [38230/50000], Train Loss: 44.1963, Test Loss: 47.0937\n",
      "Epoch [38235/50000], Train Loss: 19.0564, Test Loss: 44.7113\n",
      "Epoch [38240/50000], Train Loss: 21.9551, Test Loss: 42.3627\n",
      "Epoch [38245/50000], Train Loss: 21.6607, Test Loss: 41.7798\n",
      "Epoch [38250/50000], Train Loss: 23.1660, Test Loss: 42.5516\n",
      "Epoch [38255/50000], Train Loss: 18.8361, Test Loss: 42.9265\n",
      "Epoch [38260/50000], Train Loss: 20.2212, Test Loss: 44.0353\n",
      "Epoch [38265/50000], Train Loss: 18.9738, Test Loss: 41.6468\n",
      "Epoch [38270/50000], Train Loss: 26.3096, Test Loss: 42.1735\n",
      "Epoch [38275/50000], Train Loss: 19.7704, Test Loss: 48.0486\n",
      "Epoch [38280/50000], Train Loss: 21.5508, Test Loss: 47.6607\n",
      "Epoch [38285/50000], Train Loss: 22.7314, Test Loss: 44.2555\n",
      "Epoch [38290/50000], Train Loss: 21.3534, Test Loss: 42.1623\n",
      "Epoch [38295/50000], Train Loss: 28.1299, Test Loss: 42.6161\n",
      "Epoch [38300/50000], Train Loss: 19.7283, Test Loss: 41.6189\n",
      "Epoch [38305/50000], Train Loss: 22.5219, Test Loss: 41.4217\n",
      "Epoch [38310/50000], Train Loss: 19.5726, Test Loss: 45.9751\n",
      "Epoch [38315/50000], Train Loss: 34.5799, Test Loss: 40.7588\n",
      "Epoch [38320/50000], Train Loss: 21.0787, Test Loss: 44.5804\n",
      "Epoch [38325/50000], Train Loss: 21.5280, Test Loss: 44.0388\n",
      "Epoch [38330/50000], Train Loss: 19.3986, Test Loss: 44.3542\n",
      "Epoch [38335/50000], Train Loss: 21.4830, Test Loss: 42.2610\n",
      "Epoch [38340/50000], Train Loss: 19.9273, Test Loss: 41.5481\n",
      "Epoch [38345/50000], Train Loss: 21.6948, Test Loss: 48.6332\n",
      "Epoch [38350/50000], Train Loss: 19.9061, Test Loss: 42.6455\n",
      "Epoch [38355/50000], Train Loss: 37.0041, Test Loss: 43.3511\n",
      "Epoch [38360/50000], Train Loss: 20.1510, Test Loss: 41.5654\n",
      "Epoch [38365/50000], Train Loss: 19.0140, Test Loss: 44.4237\n",
      "Epoch [38370/50000], Train Loss: 22.5465, Test Loss: 42.8637\n",
      "Epoch [38375/50000], Train Loss: 22.5014, Test Loss: 52.9901\n",
      "Epoch [38380/50000], Train Loss: 18.9148, Test Loss: 47.3011\n",
      "Epoch [38385/50000], Train Loss: 21.5237, Test Loss: 42.5869\n",
      "Epoch [38390/50000], Train Loss: 21.8118, Test Loss: 44.0674\n",
      "Epoch [38395/50000], Train Loss: 22.3379, Test Loss: 42.4275\n",
      "Epoch [38400/50000], Train Loss: 24.1039, Test Loss: 45.2341\n",
      "Epoch [38405/50000], Train Loss: 20.9184, Test Loss: 42.5308\n",
      "Epoch [38410/50000], Train Loss: 23.3664, Test Loss: 48.2412\n",
      "Epoch [38415/50000], Train Loss: 18.9063, Test Loss: 42.6206\n",
      "Epoch [38420/50000], Train Loss: 22.4417, Test Loss: 43.3153\n",
      "Epoch [38425/50000], Train Loss: 27.0680, Test Loss: 43.5488\n",
      "Epoch [38430/50000], Train Loss: 23.2272, Test Loss: 43.4297\n",
      "Epoch [38435/50000], Train Loss: 27.4087, Test Loss: 41.0619\n",
      "Epoch [38440/50000], Train Loss: 23.9714, Test Loss: 44.1356\n",
      "Epoch [38445/50000], Train Loss: 29.6502, Test Loss: 53.1433\n",
      "Epoch [38450/50000], Train Loss: 24.9785, Test Loss: 41.4567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38455/50000], Train Loss: 25.2265, Test Loss: 45.2218\n",
      "Epoch [38460/50000], Train Loss: 22.1765, Test Loss: 43.8833\n",
      "Epoch [38465/50000], Train Loss: 21.1409, Test Loss: 42.7505\n",
      "Epoch [38470/50000], Train Loss: 24.3431, Test Loss: 44.7453\n",
      "Epoch [38475/50000], Train Loss: 23.9845, Test Loss: 43.6749\n",
      "Epoch [38480/50000], Train Loss: 21.9196, Test Loss: 45.8653\n",
      "Epoch [38485/50000], Train Loss: 31.2186, Test Loss: 48.0569\n",
      "Epoch [38490/50000], Train Loss: 18.9300, Test Loss: 75.6073\n",
      "Epoch [38495/50000], Train Loss: 22.7765, Test Loss: 43.3559\n",
      "Epoch [38500/50000], Train Loss: 18.3169, Test Loss: 41.1730\n",
      "Epoch [38505/50000], Train Loss: 15.5316, Test Loss: 45.6208\n",
      "Epoch [38510/50000], Train Loss: 18.1559, Test Loss: 43.0596\n",
      "Epoch [38515/50000], Train Loss: 21.9276, Test Loss: 43.7274\n",
      "Epoch [38520/50000], Train Loss: 22.6932, Test Loss: 43.4096\n",
      "Epoch [38525/50000], Train Loss: 24.4559, Test Loss: 42.8823\n",
      "Epoch [38530/50000], Train Loss: 17.5067, Test Loss: 43.1000\n",
      "Epoch [38535/50000], Train Loss: 22.0601, Test Loss: 42.7163\n",
      "Epoch [38540/50000], Train Loss: 21.7446, Test Loss: 42.6191\n",
      "Epoch [38545/50000], Train Loss: 19.0438, Test Loss: 43.6785\n",
      "Epoch [38550/50000], Train Loss: 28.9078, Test Loss: 42.8099\n",
      "Epoch [38555/50000], Train Loss: 24.8713, Test Loss: 43.6311\n",
      "Epoch [38560/50000], Train Loss: 20.5978, Test Loss: 41.8377\n",
      "Epoch [38565/50000], Train Loss: 23.1490, Test Loss: 42.5672\n",
      "Epoch [38570/50000], Train Loss: 21.6848, Test Loss: 41.9614\n",
      "Epoch [38575/50000], Train Loss: 19.1801, Test Loss: 42.3824\n",
      "Epoch [38580/50000], Train Loss: 24.9551, Test Loss: 43.4357\n",
      "Epoch [38585/50000], Train Loss: 25.8003, Test Loss: 42.0036\n",
      "Epoch [38590/50000], Train Loss: 21.8156, Test Loss: 43.3654\n",
      "Epoch [38595/50000], Train Loss: 20.7755, Test Loss: 42.5851\n",
      "Epoch [38600/50000], Train Loss: 30.1890, Test Loss: 43.1513\n",
      "Epoch [38605/50000], Train Loss: 21.7800, Test Loss: 51.0745\n",
      "Epoch [38610/50000], Train Loss: 18.1578, Test Loss: 43.0841\n",
      "Epoch [38615/50000], Train Loss: 23.8100, Test Loss: 42.4843\n",
      "Epoch [38620/50000], Train Loss: 22.3459, Test Loss: 45.1381\n",
      "Epoch [38625/50000], Train Loss: 25.2801, Test Loss: 46.3507\n",
      "Epoch [38630/50000], Train Loss: 20.4828, Test Loss: 42.7607\n",
      "Epoch [38635/50000], Train Loss: 20.1488, Test Loss: 41.9507\n",
      "Epoch [38640/50000], Train Loss: 40.6878, Test Loss: 40.9106\n",
      "Epoch [38645/50000], Train Loss: 18.5240, Test Loss: 43.7245\n",
      "Epoch [38650/50000], Train Loss: 36.2654, Test Loss: 40.8787\n",
      "Epoch [38655/50000], Train Loss: 23.2292, Test Loss: 42.5035\n",
      "Epoch [38660/50000], Train Loss: 19.7203, Test Loss: 43.5780\n",
      "Epoch [38665/50000], Train Loss: 22.6310, Test Loss: 45.8776\n",
      "Epoch [38670/50000], Train Loss: 24.3497, Test Loss: 42.6181\n",
      "Epoch [38675/50000], Train Loss: 23.2187, Test Loss: 42.4847\n",
      "Epoch [38680/50000], Train Loss: 20.2827, Test Loss: 43.9765\n",
      "Epoch [38685/50000], Train Loss: 16.1411, Test Loss: 43.0124\n",
      "Epoch [38690/50000], Train Loss: 21.0821, Test Loss: 42.0118\n",
      "Epoch [38695/50000], Train Loss: 21.5024, Test Loss: 42.3601\n",
      "Epoch [38700/50000], Train Loss: 20.7892, Test Loss: 42.3073\n",
      "Epoch [38705/50000], Train Loss: 28.5799, Test Loss: 47.6658\n",
      "Epoch [38710/50000], Train Loss: 21.0024, Test Loss: 44.7330\n",
      "Epoch [38715/50000], Train Loss: 19.6365, Test Loss: 43.7812\n",
      "Epoch [38720/50000], Train Loss: 19.5846, Test Loss: 44.7496\n",
      "Epoch [38725/50000], Train Loss: 17.6113, Test Loss: 42.5700\n",
      "Epoch [38730/50000], Train Loss: 23.9962, Test Loss: 42.4127\n",
      "Epoch [38735/50000], Train Loss: 29.3858, Test Loss: 69.2476\n",
      "Epoch [38740/50000], Train Loss: 23.0112, Test Loss: 43.2666\n",
      "Epoch [38745/50000], Train Loss: 21.3539, Test Loss: 44.4038\n",
      "Epoch [38750/50000], Train Loss: 18.9396, Test Loss: 43.3086\n",
      "Epoch [38755/50000], Train Loss: 17.2427, Test Loss: 41.6609\n",
      "Epoch [38760/50000], Train Loss: 17.7694, Test Loss: 45.6099\n",
      "Epoch [38765/50000], Train Loss: 19.9298, Test Loss: 42.1522\n",
      "Epoch [38770/50000], Train Loss: 20.7363, Test Loss: 45.2076\n",
      "Epoch [38775/50000], Train Loss: 24.4657, Test Loss: 46.9763\n",
      "Epoch [38780/50000], Train Loss: 18.3044, Test Loss: 45.0309\n",
      "Epoch [38785/50000], Train Loss: 22.5456, Test Loss: 44.4630\n",
      "Epoch [38790/50000], Train Loss: 22.6864, Test Loss: 46.6797\n",
      "Epoch [38795/50000], Train Loss: 38.8550, Test Loss: 56.7596\n",
      "Epoch [38800/50000], Train Loss: 21.4911, Test Loss: 44.6650\n",
      "Epoch [38805/50000], Train Loss: 19.8082, Test Loss: 42.5961\n",
      "Epoch [38810/50000], Train Loss: 21.7710, Test Loss: 41.6787\n",
      "Epoch [38815/50000], Train Loss: 23.0088, Test Loss: 43.3019\n",
      "Epoch [38820/50000], Train Loss: 27.9042, Test Loss: 42.0535\n",
      "Epoch [38825/50000], Train Loss: 22.1238, Test Loss: 41.4437\n",
      "Epoch [38830/50000], Train Loss: 25.6356, Test Loss: 43.9801\n",
      "Epoch [38835/50000], Train Loss: 19.5249, Test Loss: 42.6147\n",
      "Epoch [38840/50000], Train Loss: 20.3251, Test Loss: 44.8665\n",
      "Epoch [38845/50000], Train Loss: 18.9790, Test Loss: 41.0116\n",
      "Epoch [38850/50000], Train Loss: 19.5693, Test Loss: 41.3354\n",
      "Epoch [38855/50000], Train Loss: 17.6069, Test Loss: 44.3448\n",
      "Epoch [38860/50000], Train Loss: 23.8752, Test Loss: 43.4212\n",
      "Epoch [38865/50000], Train Loss: 22.2159, Test Loss: 43.2320\n",
      "Epoch [38870/50000], Train Loss: 23.5906, Test Loss: 45.1173\n",
      "Epoch [38875/50000], Train Loss: 25.5782, Test Loss: 43.5440\n",
      "Epoch [38880/50000], Train Loss: 19.1118, Test Loss: 42.6563\n",
      "Epoch [38885/50000], Train Loss: 18.2410, Test Loss: 43.1733\n",
      "Epoch [38890/50000], Train Loss: 23.6421, Test Loss: 41.5466\n",
      "Epoch [38895/50000], Train Loss: 22.2020, Test Loss: 43.0078\n",
      "Epoch [38900/50000], Train Loss: 17.9534, Test Loss: 43.2536\n",
      "Epoch [38905/50000], Train Loss: 22.8546, Test Loss: 42.8335\n",
      "Epoch [38910/50000], Train Loss: 39.9140, Test Loss: 46.9676\n",
      "Epoch [38915/50000], Train Loss: 27.3826, Test Loss: 55.8235\n",
      "Epoch [38920/50000], Train Loss: 22.3689, Test Loss: 41.6019\n",
      "Epoch [38925/50000], Train Loss: 14.3835, Test Loss: 43.7397\n",
      "Epoch [38930/50000], Train Loss: 22.4505, Test Loss: 43.9674\n",
      "Epoch [38935/50000], Train Loss: 27.0748, Test Loss: 46.6988\n",
      "Epoch [38940/50000], Train Loss: 20.6525, Test Loss: 46.7701\n",
      "Epoch [38945/50000], Train Loss: 19.3687, Test Loss: 41.1118\n",
      "Epoch [38950/50000], Train Loss: 18.9501, Test Loss: 44.9501\n",
      "Epoch [38955/50000], Train Loss: 25.1307, Test Loss: 46.1955\n",
      "Epoch [38960/50000], Train Loss: 22.5373, Test Loss: 43.0993\n",
      "Epoch [38965/50000], Train Loss: 23.9423, Test Loss: 50.1861\n",
      "Epoch [38970/50000], Train Loss: 30.6001, Test Loss: 41.9277\n",
      "Epoch [38975/50000], Train Loss: 21.6404, Test Loss: 55.3823\n",
      "Epoch [38980/50000], Train Loss: 21.8970, Test Loss: 42.6767\n",
      "Epoch [38985/50000], Train Loss: 18.6986, Test Loss: 44.4257\n",
      "Epoch [38990/50000], Train Loss: 21.2780, Test Loss: 43.1340\n",
      "Epoch [38995/50000], Train Loss: 20.6227, Test Loss: 43.7285\n",
      "Epoch [39000/50000], Train Loss: 25.8899, Test Loss: 55.8370\n",
      "Epoch [39005/50000], Train Loss: 27.7605, Test Loss: 45.0257\n",
      "Epoch [39010/50000], Train Loss: 18.2829, Test Loss: 43.3073\n",
      "Epoch [39015/50000], Train Loss: 13.3719, Test Loss: 40.8681\n",
      "Epoch [39020/50000], Train Loss: 28.6174, Test Loss: 50.3391\n",
      "Epoch [39025/50000], Train Loss: 25.7243, Test Loss: 43.7049\n",
      "Epoch [39030/50000], Train Loss: 21.8595, Test Loss: 49.7191\n",
      "Epoch [39035/50000], Train Loss: 29.0441, Test Loss: 41.3773\n",
      "Epoch [39040/50000], Train Loss: 24.4138, Test Loss: 42.9456\n",
      "Epoch [39045/50000], Train Loss: 24.3385, Test Loss: 45.8942\n",
      "Epoch [39050/50000], Train Loss: 23.7879, Test Loss: 44.8874\n",
      "Epoch [39055/50000], Train Loss: 17.1300, Test Loss: 43.5184\n",
      "Epoch [39060/50000], Train Loss: 20.4183, Test Loss: 45.8177\n",
      "Epoch [39065/50000], Train Loss: 20.7272, Test Loss: 44.5587\n",
      "Epoch [39070/50000], Train Loss: 26.5767, Test Loss: 41.4401\n",
      "Epoch [39075/50000], Train Loss: 26.0039, Test Loss: 45.6055\n",
      "Epoch [39080/50000], Train Loss: 26.0494, Test Loss: 42.7247\n",
      "Epoch [39085/50000], Train Loss: 20.9699, Test Loss: 42.9816\n",
      "Epoch [39090/50000], Train Loss: 22.3847, Test Loss: 53.9601\n",
      "Epoch [39095/50000], Train Loss: 22.7701, Test Loss: 41.3503\n",
      "Epoch [39100/50000], Train Loss: 19.0226, Test Loss: 42.2252\n",
      "Epoch [39105/50000], Train Loss: 24.2159, Test Loss: 43.6169\n",
      "Epoch [39110/50000], Train Loss: 24.6780, Test Loss: 49.0523\n",
      "Epoch [39115/50000], Train Loss: 23.8139, Test Loss: 52.4799\n",
      "Epoch [39120/50000], Train Loss: 22.1808, Test Loss: 42.0719\n",
      "Epoch [39125/50000], Train Loss: 21.4600, Test Loss: 42.2428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39130/50000], Train Loss: 17.6082, Test Loss: 42.9305\n",
      "Epoch [39135/50000], Train Loss: 21.2981, Test Loss: 44.5629\n",
      "Epoch [39140/50000], Train Loss: 18.8887, Test Loss: 44.0019\n",
      "Epoch [39145/50000], Train Loss: 24.5761, Test Loss: 43.5002\n",
      "Epoch [39150/50000], Train Loss: 24.1089, Test Loss: 43.0742\n",
      "Epoch [39155/50000], Train Loss: 27.2421, Test Loss: 43.4151\n",
      "Epoch [39160/50000], Train Loss: 19.6658, Test Loss: 44.9565\n",
      "Epoch [39165/50000], Train Loss: 24.7025, Test Loss: 42.2894\n",
      "Epoch [39170/50000], Train Loss: 21.8167, Test Loss: 41.5451\n",
      "Epoch [39175/50000], Train Loss: 24.3776, Test Loss: 45.5244\n",
      "Epoch [39180/50000], Train Loss: 22.1429, Test Loss: 41.3801\n",
      "Epoch [39185/50000], Train Loss: 23.1353, Test Loss: 43.0092\n",
      "Epoch [39190/50000], Train Loss: 18.5578, Test Loss: 44.4416\n",
      "Epoch [39195/50000], Train Loss: 28.4557, Test Loss: 41.1181\n",
      "Epoch [39200/50000], Train Loss: 26.0853, Test Loss: 42.4694\n",
      "Epoch [39205/50000], Train Loss: 22.9146, Test Loss: 50.1472\n",
      "Epoch [39210/50000], Train Loss: 22.9842, Test Loss: 44.1208\n",
      "Epoch [39215/50000], Train Loss: 20.5798, Test Loss: 41.4837\n",
      "Epoch [39220/50000], Train Loss: 25.2231, Test Loss: 42.7373\n",
      "Epoch [39225/50000], Train Loss: 20.6138, Test Loss: 45.0963\n",
      "Epoch [39230/50000], Train Loss: 22.7661, Test Loss: 41.7592\n",
      "Epoch [39235/50000], Train Loss: 17.6522, Test Loss: 41.5097\n",
      "Epoch [39240/50000], Train Loss: 23.4061, Test Loss: 43.7409\n",
      "Epoch [39245/50000], Train Loss: 21.1586, Test Loss: 46.1055\n",
      "Epoch [39250/50000], Train Loss: 20.0797, Test Loss: 45.7200\n",
      "Epoch [39255/50000], Train Loss: 22.2897, Test Loss: 42.8860\n",
      "Epoch [39260/50000], Train Loss: 22.5781, Test Loss: 47.0256\n",
      "Epoch [39265/50000], Train Loss: 19.7926, Test Loss: 41.7538\n",
      "Epoch [39270/50000], Train Loss: 21.6724, Test Loss: 42.3087\n",
      "Epoch [39275/50000], Train Loss: 20.7200, Test Loss: 42.3617\n",
      "Epoch [39280/50000], Train Loss: 22.0856, Test Loss: 45.9260\n",
      "Epoch [39285/50000], Train Loss: 21.4051, Test Loss: 41.6390\n",
      "Epoch [39290/50000], Train Loss: 31.6613, Test Loss: 44.8712\n",
      "Epoch [39295/50000], Train Loss: 24.2450, Test Loss: 42.3021\n",
      "Epoch [39300/50000], Train Loss: 25.4656, Test Loss: 46.5592\n",
      "Epoch [39305/50000], Train Loss: 16.2541, Test Loss: 41.5250\n",
      "Epoch [39310/50000], Train Loss: 30.6023, Test Loss: 42.4288\n",
      "Epoch [39315/50000], Train Loss: 19.6729, Test Loss: 43.7746\n",
      "Epoch [39320/50000], Train Loss: 23.3803, Test Loss: 41.6119\n",
      "Epoch [39325/50000], Train Loss: 23.3372, Test Loss: 41.8081\n",
      "Epoch [39330/50000], Train Loss: 23.3074, Test Loss: 49.5364\n",
      "Epoch [39335/50000], Train Loss: 19.0771, Test Loss: 42.0246\n",
      "Epoch [39340/50000], Train Loss: 40.9875, Test Loss: 94.6767\n",
      "Epoch [39345/50000], Train Loss: 22.4685, Test Loss: 45.1764\n",
      "Epoch [39350/50000], Train Loss: 17.1783, Test Loss: 41.4004\n",
      "Epoch [39355/50000], Train Loss: 20.1656, Test Loss: 43.2504\n",
      "Epoch [39360/50000], Train Loss: 20.5674, Test Loss: 43.1008\n",
      "Epoch [39365/50000], Train Loss: 18.4100, Test Loss: 48.5232\n",
      "Epoch [39370/50000], Train Loss: 15.4880, Test Loss: 41.6585\n",
      "Epoch [39375/50000], Train Loss: 22.9081, Test Loss: 42.9322\n",
      "Epoch [39380/50000], Train Loss: 20.4481, Test Loss: 41.5127\n",
      "Epoch [39385/50000], Train Loss: 36.3501, Test Loss: 153.0917\n",
      "Epoch [39390/50000], Train Loss: 19.9716, Test Loss: 45.7966\n",
      "Epoch [39395/50000], Train Loss: 32.9316, Test Loss: 48.7080\n",
      "Epoch [39400/50000], Train Loss: 21.5876, Test Loss: 43.7147\n",
      "Epoch [39405/50000], Train Loss: 20.1959, Test Loss: 44.4105\n",
      "Epoch [39410/50000], Train Loss: 28.6593, Test Loss: 42.3078\n",
      "Epoch [39415/50000], Train Loss: 22.7701, Test Loss: 45.2216\n",
      "Epoch [39420/50000], Train Loss: 23.1085, Test Loss: 42.0172\n",
      "Epoch [39425/50000], Train Loss: 23.3758, Test Loss: 45.8423\n",
      "Epoch [39430/50000], Train Loss: 22.8779, Test Loss: 42.7958\n",
      "Epoch [39435/50000], Train Loss: 37.1394, Test Loss: 43.3932\n",
      "Epoch [39440/50000], Train Loss: 19.2599, Test Loss: 41.5141\n",
      "Epoch [39445/50000], Train Loss: 22.4445, Test Loss: 43.8997\n",
      "Epoch [39450/50000], Train Loss: 17.3947, Test Loss: 42.9509\n",
      "Epoch [39455/50000], Train Loss: 31.4325, Test Loss: 40.6873\n",
      "Epoch [39460/50000], Train Loss: 18.0770, Test Loss: 41.6972\n",
      "Epoch [39465/50000], Train Loss: 40.8509, Test Loss: 44.9388\n",
      "Epoch [39470/50000], Train Loss: 23.8196, Test Loss: 48.1600\n",
      "Epoch [39475/50000], Train Loss: 28.4982, Test Loss: 47.7159\n",
      "Epoch [39480/50000], Train Loss: 25.1994, Test Loss: 44.6667\n",
      "Epoch [39485/50000], Train Loss: 22.6893, Test Loss: 44.0066\n",
      "Epoch [39490/50000], Train Loss: 38.3486, Test Loss: 41.7684\n",
      "Epoch [39495/50000], Train Loss: 20.6112, Test Loss: 43.7906\n",
      "Epoch [39500/50000], Train Loss: 21.7898, Test Loss: 43.1868\n",
      "Epoch [39505/50000], Train Loss: 19.7268, Test Loss: 44.4720\n",
      "Epoch [39510/50000], Train Loss: 21.7896, Test Loss: 41.8875\n",
      "Epoch [39515/50000], Train Loss: 21.8643, Test Loss: 47.7121\n",
      "Epoch [39520/50000], Train Loss: 21.9794, Test Loss: 44.6869\n",
      "Epoch [39525/50000], Train Loss: 21.1427, Test Loss: 41.8845\n",
      "Epoch [39530/50000], Train Loss: 20.7480, Test Loss: 42.3103\n",
      "Epoch [39535/50000], Train Loss: 22.7645, Test Loss: 47.0632\n",
      "Epoch [39540/50000], Train Loss: 25.5488, Test Loss: 42.9203\n",
      "Epoch [39545/50000], Train Loss: 24.9448, Test Loss: 42.0928\n",
      "Epoch [39550/50000], Train Loss: 21.6530, Test Loss: 42.1589\n",
      "Epoch [39555/50000], Train Loss: 30.3317, Test Loss: 41.8347\n",
      "Epoch [39560/50000], Train Loss: 26.9642, Test Loss: 45.5325\n",
      "Epoch [39565/50000], Train Loss: 23.8691, Test Loss: 47.0931\n",
      "Epoch [39570/50000], Train Loss: 18.0711, Test Loss: 46.0261\n",
      "Epoch [39575/50000], Train Loss: 34.6543, Test Loss: 43.2427\n",
      "Epoch [39580/50000], Train Loss: 22.0455, Test Loss: 41.0548\n",
      "Epoch [39585/50000], Train Loss: 20.7153, Test Loss: 42.3983\n",
      "Epoch [39590/50000], Train Loss: 20.2082, Test Loss: 41.8747\n",
      "Epoch [39595/50000], Train Loss: 16.9247, Test Loss: 41.6046\n",
      "Epoch [39600/50000], Train Loss: 23.8440, Test Loss: 44.1687\n",
      "Epoch [39605/50000], Train Loss: 24.6799, Test Loss: 46.6326\n",
      "Epoch [39610/50000], Train Loss: 20.2097, Test Loss: 43.0779\n",
      "Epoch [39615/50000], Train Loss: 22.4553, Test Loss: 41.9251\n",
      "Epoch [39620/50000], Train Loss: 27.4205, Test Loss: 41.6261\n",
      "Epoch [39625/50000], Train Loss: 20.7678, Test Loss: 43.8605\n",
      "Epoch [39630/50000], Train Loss: 24.5238, Test Loss: 42.2359\n",
      "Epoch [39635/50000], Train Loss: 16.1291, Test Loss: 42.6718\n",
      "Epoch [39640/50000], Train Loss: 28.4953, Test Loss: 45.0057\n",
      "Epoch [39645/50000], Train Loss: 22.3942, Test Loss: 42.9206\n",
      "Epoch [39650/50000], Train Loss: 21.8708, Test Loss: 41.7762\n",
      "Epoch [39655/50000], Train Loss: 22.3088, Test Loss: 45.0764\n",
      "Epoch [39660/50000], Train Loss: 21.9990, Test Loss: 43.1292\n",
      "Epoch [39665/50000], Train Loss: 21.5545, Test Loss: 42.0122\n",
      "Epoch [39670/50000], Train Loss: 20.9097, Test Loss: 42.4242\n",
      "Epoch [39675/50000], Train Loss: 18.8709, Test Loss: 47.7362\n",
      "Epoch [39680/50000], Train Loss: 22.4269, Test Loss: 40.7303\n",
      "Epoch [39685/50000], Train Loss: 21.8985, Test Loss: 41.5810\n",
      "Epoch [39690/50000], Train Loss: 23.9116, Test Loss: 42.7155\n",
      "Epoch [39695/50000], Train Loss: 21.3358, Test Loss: 47.3694\n",
      "Epoch [39700/50000], Train Loss: 21.0044, Test Loss: 45.6453\n",
      "Epoch [39705/50000], Train Loss: 19.8116, Test Loss: 41.5946\n",
      "Epoch [39710/50000], Train Loss: 28.1961, Test Loss: 63.3458\n",
      "Epoch [39715/50000], Train Loss: 22.9775, Test Loss: 43.8212\n",
      "Epoch [39720/50000], Train Loss: 25.2706, Test Loss: 44.0352\n",
      "Epoch [39725/50000], Train Loss: 40.2095, Test Loss: 42.3684\n",
      "Epoch [39730/50000], Train Loss: 20.9956, Test Loss: 42.9877\n",
      "Epoch [39735/50000], Train Loss: 16.9501, Test Loss: 42.2964\n",
      "Epoch [39740/50000], Train Loss: 20.1521, Test Loss: 44.6721\n",
      "Epoch [39745/50000], Train Loss: 26.7111, Test Loss: 42.6979\n",
      "Epoch [39750/50000], Train Loss: 23.1885, Test Loss: 46.6506\n",
      "Epoch [39755/50000], Train Loss: 21.3233, Test Loss: 41.9207\n",
      "Epoch [39760/50000], Train Loss: 21.2799, Test Loss: 42.8071\n",
      "Epoch [39765/50000], Train Loss: 25.2666, Test Loss: 41.3192\n",
      "Epoch [39770/50000], Train Loss: 18.8103, Test Loss: 43.0383\n",
      "Epoch [39775/50000], Train Loss: 28.4637, Test Loss: 43.6331\n",
      "Epoch [39780/50000], Train Loss: 23.4654, Test Loss: 42.1894\n",
      "Epoch [39785/50000], Train Loss: 19.8952, Test Loss: 45.5108\n",
      "Epoch [39790/50000], Train Loss: 24.5427, Test Loss: 44.5173\n",
      "Epoch [39795/50000], Train Loss: 30.2263, Test Loss: 42.5543\n",
      "Epoch [39800/50000], Train Loss: 18.7650, Test Loss: 45.7632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39805/50000], Train Loss: 28.1286, Test Loss: 43.4912\n",
      "Epoch [39810/50000], Train Loss: 21.0430, Test Loss: 40.9055\n",
      "Epoch [39815/50000], Train Loss: 21.7903, Test Loss: 43.6731\n",
      "Epoch [39820/50000], Train Loss: 25.8488, Test Loss: 41.8153\n",
      "Epoch [39825/50000], Train Loss: 19.7797, Test Loss: 43.2357\n",
      "Epoch [39830/50000], Train Loss: 14.3473, Test Loss: 44.1107\n",
      "Epoch [39835/50000], Train Loss: 23.6043, Test Loss: 42.6910\n",
      "Epoch [39840/50000], Train Loss: 16.3216, Test Loss: 42.6263\n",
      "Epoch [39845/50000], Train Loss: 21.8807, Test Loss: 43.3138\n",
      "Epoch [39850/50000], Train Loss: 21.8316, Test Loss: 42.0727\n",
      "Epoch [39855/50000], Train Loss: 13.3733, Test Loss: 41.4149\n",
      "Epoch [39860/50000], Train Loss: 20.0426, Test Loss: 42.2029\n",
      "Epoch [39865/50000], Train Loss: 19.0029, Test Loss: 41.1145\n",
      "Epoch [39870/50000], Train Loss: 20.0607, Test Loss: 41.8830\n",
      "Epoch [39875/50000], Train Loss: 17.1309, Test Loss: 43.5318\n",
      "Epoch [39880/50000], Train Loss: 19.3485, Test Loss: 42.3707\n",
      "Epoch [39885/50000], Train Loss: 23.2359, Test Loss: 41.4475\n",
      "Epoch [39890/50000], Train Loss: 25.6953, Test Loss: 40.8249\n",
      "Epoch [39895/50000], Train Loss: 18.4199, Test Loss: 43.3832\n",
      "Epoch [39900/50000], Train Loss: 23.0597, Test Loss: 43.0334\n",
      "Epoch [39905/50000], Train Loss: 30.7694, Test Loss: 41.6133\n",
      "Epoch [39910/50000], Train Loss: 19.7162, Test Loss: 42.7533\n",
      "Epoch [39915/50000], Train Loss: 18.8992, Test Loss: 44.4414\n",
      "Epoch [39920/50000], Train Loss: 17.9318, Test Loss: 43.1959\n",
      "Epoch [39925/50000], Train Loss: 72.9058, Test Loss: 40.8683\n",
      "Epoch [39930/50000], Train Loss: 19.1533, Test Loss: 43.4157\n",
      "Epoch [39935/50000], Train Loss: 37.1286, Test Loss: 41.3934\n",
      "Epoch [39940/50000], Train Loss: 21.1988, Test Loss: 44.1665\n",
      "Epoch [39945/50000], Train Loss: 28.7854, Test Loss: 44.2733\n",
      "Epoch [39950/50000], Train Loss: 18.8866, Test Loss: 41.9386\n",
      "Epoch [39955/50000], Train Loss: 18.2490, Test Loss: 42.7582\n",
      "Epoch [39960/50000], Train Loss: 21.9459, Test Loss: 43.2562\n",
      "Epoch [39965/50000], Train Loss: 22.8291, Test Loss: 46.4556\n",
      "Epoch [39970/50000], Train Loss: 20.6005, Test Loss: 41.6394\n",
      "Epoch [39975/50000], Train Loss: 19.4248, Test Loss: 42.2677\n",
      "Epoch [39980/50000], Train Loss: 18.9414, Test Loss: 42.9892\n",
      "Epoch [39985/50000], Train Loss: 21.8469, Test Loss: 41.3655\n",
      "Epoch [39990/50000], Train Loss: 25.4781, Test Loss: 41.4009\n",
      "Epoch [39995/50000], Train Loss: 19.0553, Test Loss: 44.3544\n",
      "Epoch [40000/50000], Train Loss: 16.9000, Test Loss: 46.6911\n",
      "Epoch [40005/50000], Train Loss: 16.1459, Test Loss: 42.0432\n",
      "Epoch [40010/50000], Train Loss: 31.2207, Test Loss: 46.0677\n",
      "Epoch [40015/50000], Train Loss: 24.7381, Test Loss: 41.4167\n",
      "Epoch [40020/50000], Train Loss: 20.3098, Test Loss: 42.5071\n",
      "Epoch [40025/50000], Train Loss: 27.5595, Test Loss: 45.0037\n",
      "Epoch [40030/50000], Train Loss: 21.1803, Test Loss: 40.7096\n",
      "Epoch [40035/50000], Train Loss: 18.3785, Test Loss: 45.0616\n",
      "Epoch [40040/50000], Train Loss: 20.9396, Test Loss: 44.9534\n",
      "Epoch [40045/50000], Train Loss: 17.5681, Test Loss: 42.4223\n",
      "Epoch [40050/50000], Train Loss: 20.4472, Test Loss: 42.5817\n",
      "Epoch [40055/50000], Train Loss: 22.4738, Test Loss: 42.1500\n",
      "Epoch [40060/50000], Train Loss: 23.2130, Test Loss: 42.2972\n",
      "Epoch [40065/50000], Train Loss: 19.9998, Test Loss: 42.3955\n",
      "Epoch [40070/50000], Train Loss: 18.4250, Test Loss: 47.0342\n",
      "Epoch [40075/50000], Train Loss: 22.9797, Test Loss: 43.4408\n",
      "Epoch [40080/50000], Train Loss: 21.0482, Test Loss: 45.5390\n",
      "Epoch [40085/50000], Train Loss: 24.4190, Test Loss: 46.1236\n",
      "Epoch [40090/50000], Train Loss: 19.3086, Test Loss: 42.4833\n",
      "Epoch [40095/50000], Train Loss: 22.5703, Test Loss: 46.6494\n",
      "Epoch [40100/50000], Train Loss: 22.3042, Test Loss: 41.7616\n",
      "Epoch [40105/50000], Train Loss: 17.8534, Test Loss: 41.8024\n",
      "Epoch [40110/50000], Train Loss: 21.3947, Test Loss: 42.1957\n",
      "Epoch [40115/50000], Train Loss: 24.1817, Test Loss: 42.8259\n",
      "Epoch [40120/50000], Train Loss: 23.5130, Test Loss: 43.5519\n",
      "Epoch [40125/50000], Train Loss: 22.0527, Test Loss: 41.3857\n",
      "Epoch [40130/50000], Train Loss: 18.9373, Test Loss: 42.2799\n",
      "Epoch [40135/50000], Train Loss: 21.1071, Test Loss: 42.0022\n",
      "Epoch [40140/50000], Train Loss: 23.7847, Test Loss: 42.8601\n",
      "Epoch [40145/50000], Train Loss: 22.0368, Test Loss: 42.2125\n",
      "Epoch [40150/50000], Train Loss: 14.0591, Test Loss: 41.1357\n",
      "Epoch [40155/50000], Train Loss: 19.3376, Test Loss: 42.4178\n",
      "Epoch [40160/50000], Train Loss: 24.8254, Test Loss: 44.6041\n",
      "Epoch [40165/50000], Train Loss: 24.2243, Test Loss: 41.8991\n",
      "Epoch [40170/50000], Train Loss: 20.5020, Test Loss: 41.9735\n",
      "Epoch [40175/50000], Train Loss: 23.2853, Test Loss: 42.8146\n",
      "Epoch [40180/50000], Train Loss: 22.6052, Test Loss: 49.8333\n",
      "Epoch [40185/50000], Train Loss: 16.1947, Test Loss: 41.0839\n",
      "Epoch [40190/50000], Train Loss: 20.1728, Test Loss: 41.6854\n",
      "Epoch [40195/50000], Train Loss: 18.9038, Test Loss: 44.0382\n",
      "Epoch [40200/50000], Train Loss: 20.1173, Test Loss: 43.1857\n",
      "Epoch [40205/50000], Train Loss: 24.4528, Test Loss: 63.2991\n",
      "Epoch [40210/50000], Train Loss: 45.6476, Test Loss: 46.1679\n",
      "Epoch [40215/50000], Train Loss: 15.3023, Test Loss: 41.4558\n",
      "Epoch [40220/50000], Train Loss: 18.6700, Test Loss: 43.0598\n",
      "Epoch [40225/50000], Train Loss: 22.9342, Test Loss: 44.1820\n",
      "Epoch [40230/50000], Train Loss: 23.8653, Test Loss: 44.3790\n",
      "Epoch [40235/50000], Train Loss: 51.7198, Test Loss: 69.6553\n",
      "Epoch [40240/50000], Train Loss: 20.1999, Test Loss: 42.0030\n",
      "Epoch [40245/50000], Train Loss: 22.6567, Test Loss: 47.8936\n",
      "Epoch [40250/50000], Train Loss: 23.9988, Test Loss: 42.1832\n",
      "Epoch [40255/50000], Train Loss: 18.9383, Test Loss: 43.8885\n",
      "Epoch [40260/50000], Train Loss: 21.9626, Test Loss: 42.0474\n",
      "Epoch [40265/50000], Train Loss: 21.4036, Test Loss: 42.9018\n",
      "Epoch [40270/50000], Train Loss: 27.6903, Test Loss: 41.8382\n",
      "Epoch [40275/50000], Train Loss: 22.6944, Test Loss: 42.2456\n",
      "Epoch [40280/50000], Train Loss: 19.3264, Test Loss: 44.7198\n",
      "Epoch [40285/50000], Train Loss: 20.4989, Test Loss: 47.7057\n",
      "Epoch [40290/50000], Train Loss: 22.2141, Test Loss: 42.9981\n",
      "Epoch [40295/50000], Train Loss: 20.7706, Test Loss: 41.3112\n",
      "Epoch [40300/50000], Train Loss: 21.1204, Test Loss: 44.0619\n",
      "Epoch [40305/50000], Train Loss: 21.3388, Test Loss: 41.2553\n",
      "Epoch [40310/50000], Train Loss: 22.1302, Test Loss: 41.4273\n",
      "Epoch [40315/50000], Train Loss: 16.0771, Test Loss: 41.0992\n",
      "Epoch [40320/50000], Train Loss: 13.3522, Test Loss: 42.1969\n",
      "Epoch [40325/50000], Train Loss: 18.6020, Test Loss: 42.5795\n",
      "Epoch [40330/50000], Train Loss: 20.1148, Test Loss: 49.5476\n",
      "Epoch [40335/50000], Train Loss: 24.2089, Test Loss: 45.9825\n",
      "Epoch [40340/50000], Train Loss: 20.3345, Test Loss: 42.4615\n",
      "Epoch [40345/50000], Train Loss: 23.2274, Test Loss: 43.3792\n",
      "Epoch [40350/50000], Train Loss: 24.3084, Test Loss: 42.5152\n",
      "Epoch [40355/50000], Train Loss: 20.0951, Test Loss: 42.3266\n",
      "Epoch [40360/50000], Train Loss: 22.6043, Test Loss: 42.9147\n",
      "Epoch [40365/50000], Train Loss: 20.0972, Test Loss: 41.2370\n",
      "Epoch [40370/50000], Train Loss: 22.2169, Test Loss: 41.5842\n",
      "Epoch [40375/50000], Train Loss: 21.1210, Test Loss: 45.6119\n",
      "Epoch [40380/50000], Train Loss: 26.1939, Test Loss: 41.8672\n",
      "Epoch [40385/50000], Train Loss: 29.4659, Test Loss: 41.7854\n",
      "Epoch [40390/50000], Train Loss: 20.5419, Test Loss: 44.6034\n",
      "Epoch [40395/50000], Train Loss: 23.1918, Test Loss: 41.7531\n",
      "Epoch [40400/50000], Train Loss: 23.6634, Test Loss: 42.6254\n",
      "Epoch [40405/50000], Train Loss: 17.3296, Test Loss: 41.0776\n",
      "Epoch [40410/50000], Train Loss: 25.1660, Test Loss: 41.4685\n",
      "Epoch [40415/50000], Train Loss: 20.8888, Test Loss: 41.9507\n",
      "Epoch [40420/50000], Train Loss: 27.8399, Test Loss: 42.2643\n",
      "Epoch [40425/50000], Train Loss: 22.8692, Test Loss: 44.4887\n",
      "Epoch [40430/50000], Train Loss: 20.5340, Test Loss: 42.2864\n",
      "Epoch [40435/50000], Train Loss: 22.3590, Test Loss: 41.6228\n",
      "Epoch [40440/50000], Train Loss: 29.9449, Test Loss: 46.0695\n",
      "Epoch [40445/50000], Train Loss: 22.3838, Test Loss: 41.7008\n",
      "Epoch [40450/50000], Train Loss: 19.0669, Test Loss: 45.6577\n",
      "Epoch [40455/50000], Train Loss: 19.9080, Test Loss: 46.7507\n",
      "Epoch [40460/50000], Train Loss: 20.6829, Test Loss: 43.3455\n",
      "Epoch [40465/50000], Train Loss: 59.7031, Test Loss: 41.5678\n",
      "Epoch [40470/50000], Train Loss: 21.4193, Test Loss: 46.1178\n",
      "Epoch [40475/50000], Train Loss: 20.2642, Test Loss: 45.6043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40480/50000], Train Loss: 26.1018, Test Loss: 40.5483\n",
      "Epoch [40485/50000], Train Loss: 16.8396, Test Loss: 41.4004\n",
      "Epoch [40490/50000], Train Loss: 45.7648, Test Loss: 41.2237\n",
      "Epoch [40495/50000], Train Loss: 24.9020, Test Loss: 41.6993\n",
      "Epoch [40500/50000], Train Loss: 20.7159, Test Loss: 42.9181\n",
      "Epoch [40505/50000], Train Loss: 24.3627, Test Loss: 40.7605\n",
      "Epoch [40510/50000], Train Loss: 22.2370, Test Loss: 43.3458\n",
      "Epoch [40515/50000], Train Loss: 21.2765, Test Loss: 43.6895\n",
      "Epoch [40520/50000], Train Loss: 18.9322, Test Loss: 44.1134\n",
      "Epoch [40525/50000], Train Loss: 17.5779, Test Loss: 43.7340\n",
      "Epoch [40530/50000], Train Loss: 25.8059, Test Loss: 41.5715\n",
      "Epoch [40535/50000], Train Loss: 28.2630, Test Loss: 56.9348\n",
      "Epoch [40540/50000], Train Loss: 18.3720, Test Loss: 44.2677\n",
      "Epoch [40545/50000], Train Loss: 22.6259, Test Loss: 42.2083\n",
      "Epoch [40550/50000], Train Loss: 21.3684, Test Loss: 40.6141\n",
      "Epoch [40555/50000], Train Loss: 21.0284, Test Loss: 41.6599\n",
      "Epoch [40560/50000], Train Loss: 23.7685, Test Loss: 46.1390\n",
      "Epoch [40565/50000], Train Loss: 20.1417, Test Loss: 43.1562\n",
      "Epoch [40570/50000], Train Loss: 27.0630, Test Loss: 41.2455\n",
      "Epoch [40575/50000], Train Loss: 26.0447, Test Loss: 41.7458\n",
      "Epoch [40580/50000], Train Loss: 22.0351, Test Loss: 41.4139\n",
      "Epoch [40585/50000], Train Loss: 23.0665, Test Loss: 47.7997\n",
      "Epoch [40590/50000], Train Loss: 23.1815, Test Loss: 52.3908\n",
      "Epoch [40595/50000], Train Loss: 18.5718, Test Loss: 43.5188\n",
      "Epoch [40600/50000], Train Loss: 22.4125, Test Loss: 44.7395\n",
      "Epoch [40605/50000], Train Loss: 27.7530, Test Loss: 45.0450\n",
      "Epoch [40610/50000], Train Loss: 16.3251, Test Loss: 45.7203\n",
      "Epoch [40615/50000], Train Loss: 21.4842, Test Loss: 51.1500\n",
      "Epoch [40620/50000], Train Loss: 19.8571, Test Loss: 44.9795\n",
      "Epoch [40625/50000], Train Loss: 21.4601, Test Loss: 42.3549\n",
      "Epoch [40630/50000], Train Loss: 19.8557, Test Loss: 41.9511\n",
      "Epoch [40635/50000], Train Loss: 20.4015, Test Loss: 42.7032\n",
      "Epoch [40640/50000], Train Loss: 20.0450, Test Loss: 42.9936\n",
      "Epoch [40645/50000], Train Loss: 19.9922, Test Loss: 44.0559\n",
      "Epoch [40650/50000], Train Loss: 19.0066, Test Loss: 42.3151\n",
      "Epoch [40655/50000], Train Loss: 22.2749, Test Loss: 41.2220\n",
      "Epoch [40660/50000], Train Loss: 21.7553, Test Loss: 42.0124\n",
      "Epoch [40665/50000], Train Loss: 19.7528, Test Loss: 42.3542\n",
      "Epoch [40670/50000], Train Loss: 22.9310, Test Loss: 45.9722\n",
      "Epoch [40675/50000], Train Loss: 18.8002, Test Loss: 44.5085\n",
      "Epoch [40680/50000], Train Loss: 23.3694, Test Loss: 43.1045\n",
      "Epoch [40685/50000], Train Loss: 23.1084, Test Loss: 42.8774\n",
      "Epoch [40690/50000], Train Loss: 17.8418, Test Loss: 43.9081\n",
      "Epoch [40695/50000], Train Loss: 24.5963, Test Loss: 45.5830\n",
      "Epoch [40700/50000], Train Loss: 22.8660, Test Loss: 50.4821\n",
      "Epoch [40705/50000], Train Loss: 22.1496, Test Loss: 41.6941\n",
      "Epoch [40710/50000], Train Loss: 18.2127, Test Loss: 42.7831\n",
      "Epoch [40715/50000], Train Loss: 42.0055, Test Loss: 41.5877\n",
      "Epoch [40720/50000], Train Loss: 19.6455, Test Loss: 42.3367\n",
      "Epoch [40725/50000], Train Loss: 13.9029, Test Loss: 40.7884\n",
      "Epoch [40730/50000], Train Loss: 38.9926, Test Loss: 42.1140\n",
      "Epoch [40735/50000], Train Loss: 22.9059, Test Loss: 41.6990\n",
      "Epoch [40740/50000], Train Loss: 19.3251, Test Loss: 41.9856\n",
      "Epoch [40745/50000], Train Loss: 16.4949, Test Loss: 41.8059\n",
      "Epoch [40750/50000], Train Loss: 22.4660, Test Loss: 43.4885\n",
      "Epoch [40755/50000], Train Loss: 19.6179, Test Loss: 42.7034\n",
      "Epoch [40760/50000], Train Loss: 21.5084, Test Loss: 46.2910\n",
      "Epoch [40765/50000], Train Loss: 38.0779, Test Loss: 41.7106\n",
      "Epoch [40770/50000], Train Loss: 27.0731, Test Loss: 44.1153\n",
      "Epoch [40775/50000], Train Loss: 19.5327, Test Loss: 49.0082\n",
      "Epoch [40780/50000], Train Loss: 18.7969, Test Loss: 44.6865\n",
      "Epoch [40785/50000], Train Loss: 19.9320, Test Loss: 42.7958\n",
      "Epoch [40790/50000], Train Loss: 20.6376, Test Loss: 53.1054\n",
      "Epoch [40795/50000], Train Loss: 29.5369, Test Loss: 42.4276\n",
      "Epoch [40800/50000], Train Loss: 22.3479, Test Loss: 49.1810\n",
      "Epoch [40805/50000], Train Loss: 27.4300, Test Loss: 43.2904\n",
      "Epoch [40810/50000], Train Loss: 11.2997, Test Loss: 40.3166\n",
      "Epoch [40815/50000], Train Loss: 20.0759, Test Loss: 42.6404\n",
      "Epoch [40820/50000], Train Loss: 22.9124, Test Loss: 42.1209\n",
      "Epoch [40825/50000], Train Loss: 24.1101, Test Loss: 45.4004\n",
      "Epoch [40830/50000], Train Loss: 17.2498, Test Loss: 44.1438\n",
      "Epoch [40835/50000], Train Loss: 20.5304, Test Loss: 43.3908\n",
      "Epoch [40840/50000], Train Loss: 21.9146, Test Loss: 42.0377\n",
      "Epoch [40845/50000], Train Loss: 21.7930, Test Loss: 41.0998\n",
      "Epoch [40850/50000], Train Loss: 23.0288, Test Loss: 44.4066\n",
      "Epoch [40855/50000], Train Loss: 21.4666, Test Loss: 56.7190\n",
      "Epoch [40860/50000], Train Loss: 19.8507, Test Loss: 43.7440\n",
      "Epoch [40865/50000], Train Loss: 27.0848, Test Loss: 41.4286\n",
      "Epoch [40870/50000], Train Loss: 22.0725, Test Loss: 44.2639\n",
      "Epoch [40875/50000], Train Loss: 19.5595, Test Loss: 42.6886\n",
      "Epoch [40880/50000], Train Loss: 18.7667, Test Loss: 44.7115\n",
      "Epoch [40885/50000], Train Loss: 22.5639, Test Loss: 41.0739\n",
      "Epoch [40890/50000], Train Loss: 24.3015, Test Loss: 43.1236\n",
      "Epoch [40895/50000], Train Loss: 19.0768, Test Loss: 45.6597\n",
      "Epoch [40900/50000], Train Loss: 37.9248, Test Loss: 42.7351\n",
      "Epoch [40905/50000], Train Loss: 20.8828, Test Loss: 41.7322\n",
      "Epoch [40910/50000], Train Loss: 22.7152, Test Loss: 45.7846\n",
      "Epoch [40915/50000], Train Loss: 26.8968, Test Loss: 44.6810\n",
      "Epoch [40920/50000], Train Loss: 25.8751, Test Loss: 42.0414\n",
      "Epoch [40925/50000], Train Loss: 18.7853, Test Loss: 40.7679\n",
      "Epoch [40930/50000], Train Loss: 22.5675, Test Loss: 41.5554\n",
      "Epoch [40935/50000], Train Loss: 15.2666, Test Loss: 40.8928\n",
      "Epoch [40940/50000], Train Loss: 20.4098, Test Loss: 42.3190\n",
      "Epoch [40945/50000], Train Loss: 21.7375, Test Loss: 49.7101\n",
      "Epoch [40950/50000], Train Loss: 20.5952, Test Loss: 48.4611\n",
      "Epoch [40955/50000], Train Loss: 23.3750, Test Loss: 44.3673\n",
      "Epoch [40960/50000], Train Loss: 20.9624, Test Loss: 46.2818\n",
      "Epoch [40965/50000], Train Loss: 20.9087, Test Loss: 43.4382\n",
      "Epoch [40970/50000], Train Loss: 43.7096, Test Loss: 43.0547\n",
      "Epoch [40975/50000], Train Loss: 25.3602, Test Loss: 43.0656\n",
      "Epoch [40980/50000], Train Loss: 13.9694, Test Loss: 43.3182\n",
      "Epoch [40985/50000], Train Loss: 23.2223, Test Loss: 40.8264\n",
      "Epoch [40990/50000], Train Loss: 21.5843, Test Loss: 42.3695\n",
      "Epoch [40995/50000], Train Loss: 24.1383, Test Loss: 41.8084\n",
      "Epoch [41000/50000], Train Loss: 17.8920, Test Loss: 47.7546\n",
      "Epoch [41005/50000], Train Loss: 21.4283, Test Loss: 44.9049\n",
      "Epoch [41010/50000], Train Loss: 19.6356, Test Loss: 43.9986\n",
      "Epoch [41015/50000], Train Loss: 21.4570, Test Loss: 48.7550\n",
      "Epoch [41020/50000], Train Loss: 24.8045, Test Loss: 57.3652\n",
      "Epoch [41025/50000], Train Loss: 20.3129, Test Loss: 53.2648\n",
      "Epoch [41030/50000], Train Loss: 15.9806, Test Loss: 41.3321\n",
      "Epoch [41035/50000], Train Loss: 25.2822, Test Loss: 42.7723\n",
      "Epoch [41040/50000], Train Loss: 19.6888, Test Loss: 42.1235\n",
      "Epoch [41045/50000], Train Loss: 22.1764, Test Loss: 43.1831\n",
      "Epoch [41050/50000], Train Loss: 20.8815, Test Loss: 41.7826\n",
      "Epoch [41055/50000], Train Loss: 19.4729, Test Loss: 42.5905\n",
      "Epoch [41060/50000], Train Loss: 21.1428, Test Loss: 43.0607\n",
      "Epoch [41065/50000], Train Loss: 21.7323, Test Loss: 47.1939\n",
      "Epoch [41070/50000], Train Loss: 18.0325, Test Loss: 43.6141\n",
      "Epoch [41075/50000], Train Loss: 23.4811, Test Loss: 41.7861\n",
      "Epoch [41080/50000], Train Loss: 17.8199, Test Loss: 41.5399\n",
      "Epoch [41085/50000], Train Loss: 26.9918, Test Loss: 42.6906\n",
      "Epoch [41090/50000], Train Loss: 18.8594, Test Loss: 42.2619\n",
      "Epoch [41095/50000], Train Loss: 21.2073, Test Loss: 40.5206\n",
      "Epoch [41100/50000], Train Loss: 50.2792, Test Loss: 41.2738\n",
      "Epoch [41105/50000], Train Loss: 22.2801, Test Loss: 47.7759\n",
      "Epoch [41110/50000], Train Loss: 18.2932, Test Loss: 41.8976\n",
      "Epoch [41115/50000], Train Loss: 22.2494, Test Loss: 43.5037\n",
      "Epoch [41120/50000], Train Loss: 24.1690, Test Loss: 40.7304\n",
      "Epoch [41125/50000], Train Loss: 15.8828, Test Loss: 42.0055\n",
      "Epoch [41130/50000], Train Loss: 18.7589, Test Loss: 43.6333\n",
      "Epoch [41135/50000], Train Loss: 17.9984, Test Loss: 40.8216\n",
      "Epoch [41140/50000], Train Loss: 20.6347, Test Loss: 42.7014\n",
      "Epoch [41145/50000], Train Loss: 25.8410, Test Loss: 41.5706\n",
      "Epoch [41150/50000], Train Loss: 20.2629, Test Loss: 40.7624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41155/50000], Train Loss: 56.6452, Test Loss: 41.2867\n",
      "Epoch [41160/50000], Train Loss: 19.7755, Test Loss: 43.3757\n",
      "Epoch [41165/50000], Train Loss: 20.1811, Test Loss: 43.7054\n",
      "Epoch [41170/50000], Train Loss: 16.9633, Test Loss: 44.9960\n",
      "Epoch [41175/50000], Train Loss: 25.4127, Test Loss: 45.0881\n",
      "Epoch [41180/50000], Train Loss: 23.3853, Test Loss: 44.7298\n",
      "Epoch [41185/50000], Train Loss: 27.6884, Test Loss: 43.1061\n",
      "Epoch [41190/50000], Train Loss: 20.7547, Test Loss: 40.7094\n",
      "Epoch [41195/50000], Train Loss: 22.7463, Test Loss: 42.9406\n",
      "Epoch [41200/50000], Train Loss: 17.8659, Test Loss: 41.0865\n",
      "Epoch [41205/50000], Train Loss: 26.8040, Test Loss: 41.8144\n",
      "Epoch [41210/50000], Train Loss: 19.1428, Test Loss: 41.5749\n",
      "Epoch [41215/50000], Train Loss: 24.4281, Test Loss: 42.9284\n",
      "Epoch [41220/50000], Train Loss: 19.2461, Test Loss: 40.9283\n",
      "Epoch [41225/50000], Train Loss: 20.3323, Test Loss: 41.4234\n",
      "Epoch [41230/50000], Train Loss: 21.9868, Test Loss: 41.9767\n",
      "Epoch [41235/50000], Train Loss: 24.5999, Test Loss: 41.1479\n",
      "Epoch [41240/50000], Train Loss: 19.5241, Test Loss: 42.2859\n",
      "Epoch [41245/50000], Train Loss: 21.1615, Test Loss: 40.9480\n",
      "Epoch [41250/50000], Train Loss: 20.4542, Test Loss: 44.4314\n",
      "Epoch [41255/50000], Train Loss: 13.3423, Test Loss: 43.2359\n",
      "Epoch [41260/50000], Train Loss: 21.4913, Test Loss: 43.8308\n",
      "Epoch [41265/50000], Train Loss: 19.2248, Test Loss: 41.3164\n",
      "Epoch [41270/50000], Train Loss: 34.6525, Test Loss: 41.4694\n",
      "Epoch [41275/50000], Train Loss: 19.5554, Test Loss: 42.8078\n",
      "Epoch [41280/50000], Train Loss: 19.8081, Test Loss: 44.3528\n",
      "Epoch [41285/50000], Train Loss: 21.8262, Test Loss: 44.1357\n",
      "Epoch [41290/50000], Train Loss: 19.9492, Test Loss: 43.1708\n",
      "Epoch [41295/50000], Train Loss: 25.1911, Test Loss: 45.0821\n",
      "Epoch [41300/50000], Train Loss: 19.4555, Test Loss: 42.6521\n",
      "Epoch [41305/50000], Train Loss: 16.0141, Test Loss: 41.1811\n",
      "Epoch [41310/50000], Train Loss: 24.9129, Test Loss: 45.6281\n",
      "Epoch [41315/50000], Train Loss: 18.6156, Test Loss: 41.4253\n",
      "Epoch [41320/50000], Train Loss: 20.9460, Test Loss: 42.9480\n",
      "Epoch [41325/50000], Train Loss: 21.5420, Test Loss: 42.1390\n",
      "Epoch [41330/50000], Train Loss: 19.3715, Test Loss: 44.9667\n",
      "Epoch [41335/50000], Train Loss: 14.5422, Test Loss: 47.7425\n",
      "Epoch [41340/50000], Train Loss: 19.4654, Test Loss: 44.7457\n",
      "Epoch [41345/50000], Train Loss: 22.7717, Test Loss: 42.7577\n",
      "Epoch [41350/50000], Train Loss: 25.5213, Test Loss: 42.5688\n",
      "Epoch [41355/50000], Train Loss: 25.3422, Test Loss: 42.1838\n",
      "Epoch [41360/50000], Train Loss: 20.6849, Test Loss: 42.4023\n",
      "Epoch [41365/50000], Train Loss: 19.9630, Test Loss: 41.7700\n",
      "Epoch [41370/50000], Train Loss: 18.8976, Test Loss: 43.6720\n",
      "Epoch [41375/50000], Train Loss: 23.7724, Test Loss: 51.7872\n",
      "Epoch [41380/50000], Train Loss: 20.9988, Test Loss: 42.6535\n",
      "Epoch [41385/50000], Train Loss: 24.7194, Test Loss: 43.3770\n",
      "Epoch [41390/50000], Train Loss: 25.6266, Test Loss: 43.8744\n",
      "Epoch [41395/50000], Train Loss: 21.0350, Test Loss: 41.4002\n",
      "Epoch [41400/50000], Train Loss: 21.0680, Test Loss: 43.8685\n",
      "Epoch [41405/50000], Train Loss: 22.7642, Test Loss: 43.6799\n",
      "Epoch [41410/50000], Train Loss: 16.9764, Test Loss: 41.5466\n",
      "Epoch [41415/50000], Train Loss: 21.8965, Test Loss: 48.5591\n",
      "Epoch [41420/50000], Train Loss: 23.0796, Test Loss: 42.4391\n",
      "Epoch [41425/50000], Train Loss: 29.1787, Test Loss: 42.2689\n",
      "Epoch [41430/50000], Train Loss: 21.6989, Test Loss: 42.3893\n",
      "Epoch [41435/50000], Train Loss: 39.9764, Test Loss: 40.8580\n",
      "Epoch [41440/50000], Train Loss: 28.0222, Test Loss: 41.2072\n",
      "Epoch [41445/50000], Train Loss: 18.5026, Test Loss: 41.3760\n",
      "Epoch [41450/50000], Train Loss: 21.2034, Test Loss: 45.4709\n",
      "Epoch [41455/50000], Train Loss: 19.5159, Test Loss: 40.7117\n",
      "Epoch [41460/50000], Train Loss: 19.7393, Test Loss: 41.7047\n",
      "Epoch [41465/50000], Train Loss: 19.9053, Test Loss: 42.4738\n",
      "Epoch [41470/50000], Train Loss: 15.6736, Test Loss: 47.3987\n",
      "Epoch [41475/50000], Train Loss: 36.7786, Test Loss: 40.5683\n",
      "Epoch [41480/50000], Train Loss: 19.5239, Test Loss: 44.5392\n",
      "Epoch [41485/50000], Train Loss: 19.9739, Test Loss: 40.3107\n",
      "Epoch [41490/50000], Train Loss: 23.9194, Test Loss: 41.9471\n",
      "Epoch [41495/50000], Train Loss: 21.9972, Test Loss: 44.4858\n",
      "Epoch [41500/50000], Train Loss: 18.1344, Test Loss: 44.2395\n",
      "Epoch [41505/50000], Train Loss: 34.9347, Test Loss: 42.7322\n",
      "Epoch [41510/50000], Train Loss: 39.3228, Test Loss: 42.3426\n",
      "Epoch [41515/50000], Train Loss: 26.7549, Test Loss: 41.3412\n",
      "Epoch [41520/50000], Train Loss: 20.3521, Test Loss: 44.4292\n",
      "Epoch [41525/50000], Train Loss: 19.6927, Test Loss: 40.8523\n",
      "Epoch [41530/50000], Train Loss: 25.2762, Test Loss: 41.4536\n",
      "Epoch [41535/50000], Train Loss: 18.9227, Test Loss: 42.9029\n",
      "Epoch [41540/50000], Train Loss: 26.4796, Test Loss: 68.7720\n",
      "Epoch [41545/50000], Train Loss: 23.1465, Test Loss: 41.4659\n",
      "Epoch [41550/50000], Train Loss: 20.8945, Test Loss: 42.5331\n",
      "Epoch [41555/50000], Train Loss: 18.2586, Test Loss: 49.2956\n",
      "Epoch [41560/50000], Train Loss: 19.7701, Test Loss: 44.8175\n",
      "Epoch [41565/50000], Train Loss: 25.1998, Test Loss: 41.2929\n",
      "Epoch [41570/50000], Train Loss: 21.8749, Test Loss: 43.2093\n",
      "Epoch [41575/50000], Train Loss: 19.6432, Test Loss: 41.9003\n",
      "Epoch [41580/50000], Train Loss: 21.4116, Test Loss: 43.0790\n",
      "Epoch [41585/50000], Train Loss: 16.7130, Test Loss: 45.8379\n",
      "Epoch [41590/50000], Train Loss: 22.2050, Test Loss: 41.4838\n",
      "Epoch [41595/50000], Train Loss: 16.8511, Test Loss: 41.2836\n",
      "Epoch [41600/50000], Train Loss: 22.1043, Test Loss: 43.0247\n",
      "Epoch [41605/50000], Train Loss: 22.6719, Test Loss: 42.6546\n",
      "Epoch [41610/50000], Train Loss: 25.9101, Test Loss: 40.7959\n",
      "Epoch [41615/50000], Train Loss: 34.0920, Test Loss: 40.5913\n",
      "Epoch [41620/50000], Train Loss: 18.0454, Test Loss: 41.6071\n",
      "Epoch [41625/50000], Train Loss: 22.7373, Test Loss: 43.6744\n",
      "Epoch [41630/50000], Train Loss: 18.6866, Test Loss: 42.3098\n",
      "Epoch [41635/50000], Train Loss: 18.6913, Test Loss: 41.1732\n",
      "Epoch [41640/50000], Train Loss: 18.7815, Test Loss: 42.9434\n",
      "Epoch [41645/50000], Train Loss: 19.4814, Test Loss: 51.0606\n",
      "Epoch [41650/50000], Train Loss: 19.5048, Test Loss: 40.8387\n",
      "Epoch [41655/50000], Train Loss: 18.9884, Test Loss: 46.2172\n",
      "Epoch [41660/50000], Train Loss: 21.0687, Test Loss: 46.1831\n",
      "Epoch [41665/50000], Train Loss: 19.7702, Test Loss: 42.2084\n",
      "Epoch [41670/50000], Train Loss: 24.0781, Test Loss: 42.5587\n",
      "Epoch [41675/50000], Train Loss: 19.8226, Test Loss: 42.3072\n",
      "Epoch [41680/50000], Train Loss: 22.2019, Test Loss: 45.9300\n",
      "Epoch [41685/50000], Train Loss: 19.8418, Test Loss: 44.4339\n",
      "Epoch [41690/50000], Train Loss: 24.6305, Test Loss: 41.2573\n",
      "Epoch [41695/50000], Train Loss: 29.4831, Test Loss: 41.1521\n",
      "Epoch [41700/50000], Train Loss: 18.5235, Test Loss: 41.6348\n",
      "Epoch [41705/50000], Train Loss: 20.7207, Test Loss: 42.1443\n",
      "Epoch [41710/50000], Train Loss: 18.3706, Test Loss: 42.0634\n",
      "Epoch [41715/50000], Train Loss: 18.6025, Test Loss: 45.2799\n",
      "Epoch [41720/50000], Train Loss: 21.9398, Test Loss: 42.3206\n",
      "Epoch [41725/50000], Train Loss: 18.4996, Test Loss: 42.4678\n",
      "Epoch [41730/50000], Train Loss: 19.9994, Test Loss: 42.5897\n",
      "Epoch [41735/50000], Train Loss: 23.2775, Test Loss: 41.6063\n",
      "Epoch [41740/50000], Train Loss: 18.9417, Test Loss: 42.2205\n",
      "Epoch [41745/50000], Train Loss: 21.7353, Test Loss: 47.4280\n",
      "Epoch [41750/50000], Train Loss: 16.8581, Test Loss: 40.2995\n",
      "Epoch [41755/50000], Train Loss: 18.1719, Test Loss: 49.7373\n",
      "Epoch [41760/50000], Train Loss: 21.2107, Test Loss: 44.3334\n",
      "Epoch [41765/50000], Train Loss: 34.7776, Test Loss: 41.5304\n",
      "Epoch [41770/50000], Train Loss: 22.6961, Test Loss: 41.5905\n",
      "Epoch [41775/50000], Train Loss: 22.5749, Test Loss: 44.1270\n",
      "Epoch [41780/50000], Train Loss: 26.7330, Test Loss: 41.4385\n",
      "Epoch [41785/50000], Train Loss: 21.2801, Test Loss: 42.1342\n",
      "Epoch [41790/50000], Train Loss: 24.2380, Test Loss: 53.0636\n",
      "Epoch [41795/50000], Train Loss: 20.9290, Test Loss: 40.8501\n",
      "Epoch [41800/50000], Train Loss: 20.4267, Test Loss: 42.3219\n",
      "Epoch [41805/50000], Train Loss: 21.0580, Test Loss: 42.3223\n",
      "Epoch [41810/50000], Train Loss: 21.9296, Test Loss: 40.2462\n",
      "Epoch [41815/50000], Train Loss: 20.5051, Test Loss: 46.7632\n",
      "Epoch [41820/50000], Train Loss: 18.4955, Test Loss: 42.8697\n",
      "Epoch [41825/50000], Train Loss: 12.5367, Test Loss: 40.2849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41830/50000], Train Loss: 21.3329, Test Loss: 45.9430\n",
      "Epoch [41835/50000], Train Loss: 24.9805, Test Loss: 42.6116\n",
      "Epoch [41840/50000], Train Loss: 21.7125, Test Loss: 40.7849\n",
      "Epoch [41845/50000], Train Loss: 17.9380, Test Loss: 46.6908\n",
      "Epoch [41850/50000], Train Loss: 24.5551, Test Loss: 40.7961\n",
      "Epoch [41855/50000], Train Loss: 25.1763, Test Loss: 44.4517\n",
      "Epoch [41860/50000], Train Loss: 21.2470, Test Loss: 43.9722\n",
      "Epoch [41865/50000], Train Loss: 23.7270, Test Loss: 42.5377\n",
      "Epoch [41870/50000], Train Loss: 21.8338, Test Loss: 41.4871\n",
      "Epoch [41875/50000], Train Loss: 16.8449, Test Loss: 41.9254\n",
      "Epoch [41880/50000], Train Loss: 20.6006, Test Loss: 42.8540\n",
      "Epoch [41885/50000], Train Loss: 18.0027, Test Loss: 43.0052\n",
      "Epoch [41890/50000], Train Loss: 19.1110, Test Loss: 43.7141\n",
      "Epoch [41895/50000], Train Loss: 20.6703, Test Loss: 41.8808\n",
      "Epoch [41900/50000], Train Loss: 17.3832, Test Loss: 46.8188\n",
      "Epoch [41905/50000], Train Loss: 17.0790, Test Loss: 44.1433\n",
      "Epoch [41910/50000], Train Loss: 22.4031, Test Loss: 43.8999\n",
      "Epoch [41915/50000], Train Loss: 20.0872, Test Loss: 45.1815\n",
      "Epoch [41920/50000], Train Loss: 18.7648, Test Loss: 44.1701\n",
      "Epoch [41925/50000], Train Loss: 20.1164, Test Loss: 41.7241\n",
      "Epoch [41930/50000], Train Loss: 18.1094, Test Loss: 42.6235\n",
      "Epoch [41935/50000], Train Loss: 22.9687, Test Loss: 42.7709\n",
      "Epoch [41940/50000], Train Loss: 27.9175, Test Loss: 42.1624\n",
      "Epoch [41945/50000], Train Loss: 17.8287, Test Loss: 43.2403\n",
      "Epoch [41950/50000], Train Loss: 23.2821, Test Loss: 41.6532\n",
      "Epoch [41955/50000], Train Loss: 17.8814, Test Loss: 42.4727\n",
      "Epoch [41960/50000], Train Loss: 24.2935, Test Loss: 40.8650\n",
      "Epoch [41965/50000], Train Loss: 22.3099, Test Loss: 44.0646\n",
      "Epoch [41970/50000], Train Loss: 23.9290, Test Loss: 45.2901\n",
      "Epoch [41975/50000], Train Loss: 21.1614, Test Loss: 42.4970\n",
      "Epoch [41980/50000], Train Loss: 22.7983, Test Loss: 41.7713\n",
      "Epoch [41985/50000], Train Loss: 18.8739, Test Loss: 50.1301\n",
      "Epoch [41990/50000], Train Loss: 21.6071, Test Loss: 46.7033\n",
      "Epoch [41995/50000], Train Loss: 15.6361, Test Loss: 40.9827\n",
      "Epoch [42000/50000], Train Loss: 23.6821, Test Loss: 44.5596\n",
      "Epoch [42005/50000], Train Loss: 21.3705, Test Loss: 44.0691\n",
      "Epoch [42010/50000], Train Loss: 17.7777, Test Loss: 41.3986\n",
      "Epoch [42015/50000], Train Loss: 19.2559, Test Loss: 42.4603\n",
      "Epoch [42020/50000], Train Loss: 18.0164, Test Loss: 41.8458\n",
      "Epoch [42025/50000], Train Loss: 23.7474, Test Loss: 44.3831\n",
      "Epoch [42030/50000], Train Loss: 23.4249, Test Loss: 42.3273\n",
      "Epoch [42035/50000], Train Loss: 23.0715, Test Loss: 41.2974\n",
      "Epoch [42040/50000], Train Loss: 20.6443, Test Loss: 41.6565\n",
      "Epoch [42045/50000], Train Loss: 19.8061, Test Loss: 43.7579\n",
      "Epoch [42050/50000], Train Loss: 20.1523, Test Loss: 42.2291\n",
      "Epoch [42055/50000], Train Loss: 23.4523, Test Loss: 45.7922\n",
      "Epoch [42060/50000], Train Loss: 20.9918, Test Loss: 41.6798\n",
      "Epoch [42065/50000], Train Loss: 24.7117, Test Loss: 49.3574\n",
      "Epoch [42070/50000], Train Loss: 29.4025, Test Loss: 41.5836\n",
      "Epoch [42075/50000], Train Loss: 25.5888, Test Loss: 42.4115\n",
      "Epoch [42080/50000], Train Loss: 18.6856, Test Loss: 44.1135\n",
      "Epoch [42085/50000], Train Loss: 18.1752, Test Loss: 45.4737\n",
      "Epoch [42090/50000], Train Loss: 20.4478, Test Loss: 42.9554\n",
      "Epoch [42095/50000], Train Loss: 32.5151, Test Loss: 47.6662\n",
      "Epoch [42100/50000], Train Loss: 19.1171, Test Loss: 42.5932\n",
      "Epoch [42105/50000], Train Loss: 19.0317, Test Loss: 43.6206\n",
      "Epoch [42110/50000], Train Loss: 21.0128, Test Loss: 41.0941\n",
      "Epoch [42115/50000], Train Loss: 23.0936, Test Loss: 41.0170\n",
      "Epoch [42120/50000], Train Loss: 19.6491, Test Loss: 41.7994\n",
      "Epoch [42125/50000], Train Loss: 23.9765, Test Loss: 43.7438\n",
      "Epoch [42130/50000], Train Loss: 20.9521, Test Loss: 41.4069\n",
      "Epoch [42135/50000], Train Loss: 20.6805, Test Loss: 46.0074\n",
      "Epoch [42140/50000], Train Loss: 26.4150, Test Loss: 62.1105\n",
      "Epoch [42145/50000], Train Loss: 19.2917, Test Loss: 41.5689\n",
      "Epoch [42150/50000], Train Loss: 27.2654, Test Loss: 62.3286\n",
      "Epoch [42155/50000], Train Loss: 20.0336, Test Loss: 42.9010\n",
      "Epoch [42160/50000], Train Loss: 23.3737, Test Loss: 45.2444\n",
      "Epoch [42165/50000], Train Loss: 20.3080, Test Loss: 42.4940\n",
      "Epoch [42170/50000], Train Loss: 24.1627, Test Loss: 42.8637\n",
      "Epoch [42175/50000], Train Loss: 19.9625, Test Loss: 42.9404\n",
      "Epoch [42180/50000], Train Loss: 21.0836, Test Loss: 42.4467\n",
      "Epoch [42185/50000], Train Loss: 18.5449, Test Loss: 40.8279\n",
      "Epoch [42190/50000], Train Loss: 21.4794, Test Loss: 44.4446\n",
      "Epoch [42195/50000], Train Loss: 21.8903, Test Loss: 41.3694\n",
      "Epoch [42200/50000], Train Loss: 18.9139, Test Loss: 41.1601\n",
      "Epoch [42205/50000], Train Loss: 17.7661, Test Loss: 43.0115\n",
      "Epoch [42210/50000], Train Loss: 24.7148, Test Loss: 42.4960\n",
      "Epoch [42215/50000], Train Loss: 21.3664, Test Loss: 40.5970\n",
      "Epoch [42220/50000], Train Loss: 18.2608, Test Loss: 40.9765\n",
      "Epoch [42225/50000], Train Loss: 20.1271, Test Loss: 46.5473\n",
      "Epoch [42230/50000], Train Loss: 21.0070, Test Loss: 41.9018\n",
      "Epoch [42235/50000], Train Loss: 21.6894, Test Loss: 43.0779\n",
      "Epoch [42240/50000], Train Loss: 17.9471, Test Loss: 41.5812\n",
      "Epoch [42245/50000], Train Loss: 20.3950, Test Loss: 44.3523\n",
      "Epoch [42250/50000], Train Loss: 31.8848, Test Loss: 42.8417\n",
      "Epoch [42255/50000], Train Loss: 22.7022, Test Loss: 43.0521\n",
      "Epoch [42260/50000], Train Loss: 24.4301, Test Loss: 40.8731\n",
      "Epoch [42265/50000], Train Loss: 18.6750, Test Loss: 41.9922\n",
      "Epoch [42270/50000], Train Loss: 20.3830, Test Loss: 47.2578\n",
      "Epoch [42275/50000], Train Loss: 20.4332, Test Loss: 48.3243\n",
      "Epoch [42280/50000], Train Loss: 17.2821, Test Loss: 41.0246\n",
      "Epoch [42285/50000], Train Loss: 22.8619, Test Loss: 43.7419\n",
      "Epoch [42290/50000], Train Loss: 23.3665, Test Loss: 44.4893\n",
      "Epoch [42295/50000], Train Loss: 19.7126, Test Loss: 41.1349\n",
      "Epoch [42300/50000], Train Loss: 21.4584, Test Loss: 47.5711\n",
      "Epoch [42305/50000], Train Loss: 19.2914, Test Loss: 43.8435\n",
      "Epoch [42310/50000], Train Loss: 20.4075, Test Loss: 40.9691\n",
      "Epoch [42315/50000], Train Loss: 18.4886, Test Loss: 41.7457\n",
      "Epoch [42320/50000], Train Loss: 13.9124, Test Loss: 41.3139\n",
      "Epoch [42325/50000], Train Loss: 21.7250, Test Loss: 43.4073\n",
      "Epoch [42330/50000], Train Loss: 19.2413, Test Loss: 43.2302\n",
      "Epoch [42335/50000], Train Loss: 16.1903, Test Loss: 42.2930\n",
      "Epoch [42340/50000], Train Loss: 28.1558, Test Loss: 41.1554\n",
      "Epoch [42345/50000], Train Loss: 21.9420, Test Loss: 43.2256\n",
      "Epoch [42350/50000], Train Loss: 18.9116, Test Loss: 41.4293\n",
      "Epoch [42355/50000], Train Loss: 21.8624, Test Loss: 46.3353\n",
      "Epoch [42360/50000], Train Loss: 24.1406, Test Loss: 48.6036\n",
      "Epoch [42365/50000], Train Loss: 22.2754, Test Loss: 42.0139\n",
      "Epoch [42370/50000], Train Loss: 23.0953, Test Loss: 43.3907\n",
      "Epoch [42375/50000], Train Loss: 15.2044, Test Loss: 42.3660\n",
      "Epoch [42380/50000], Train Loss: 21.5448, Test Loss: 41.4337\n",
      "Epoch [42385/50000], Train Loss: 19.6473, Test Loss: 42.2437\n",
      "Epoch [42390/50000], Train Loss: 17.4669, Test Loss: 41.0099\n",
      "Epoch [42395/50000], Train Loss: 20.3188, Test Loss: 41.2994\n",
      "Epoch [42400/50000], Train Loss: 17.3542, Test Loss: 44.3942\n",
      "Epoch [42405/50000], Train Loss: 21.2617, Test Loss: 46.1034\n",
      "Epoch [42410/50000], Train Loss: 17.4160, Test Loss: 41.9823\n",
      "Epoch [42415/50000], Train Loss: 21.5566, Test Loss: 41.1972\n",
      "Epoch [42420/50000], Train Loss: 25.3361, Test Loss: 46.1525\n",
      "Epoch [42425/50000], Train Loss: 25.9614, Test Loss: 44.1619\n",
      "Epoch [42430/50000], Train Loss: 18.1713, Test Loss: 41.3667\n",
      "Epoch [42435/50000], Train Loss: 26.8291, Test Loss: 44.2282\n",
      "Epoch [42440/50000], Train Loss: 20.3453, Test Loss: 42.6604\n",
      "Epoch [42445/50000], Train Loss: 24.1109, Test Loss: 42.1935\n",
      "Epoch [42450/50000], Train Loss: 18.3885, Test Loss: 42.1080\n",
      "Epoch [42455/50000], Train Loss: 18.9879, Test Loss: 42.1956\n",
      "Epoch [42460/50000], Train Loss: 21.1309, Test Loss: 43.0042\n",
      "Epoch [42465/50000], Train Loss: 17.7297, Test Loss: 41.4366\n",
      "Epoch [42470/50000], Train Loss: 20.3774, Test Loss: 43.8444\n",
      "Epoch [42475/50000], Train Loss: 21.4319, Test Loss: 41.2348\n",
      "Epoch [42480/50000], Train Loss: 25.5081, Test Loss: 41.7330\n",
      "Epoch [42485/50000], Train Loss: 19.9111, Test Loss: 40.5927\n",
      "Epoch [42490/50000], Train Loss: 21.5578, Test Loss: 40.6622\n",
      "Epoch [42495/50000], Train Loss: 22.9097, Test Loss: 44.3241\n",
      "Epoch [42500/50000], Train Loss: 18.0507, Test Loss: 42.5495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42505/50000], Train Loss: 20.9702, Test Loss: 41.6441\n",
      "Epoch [42510/50000], Train Loss: 25.2908, Test Loss: 46.6884\n",
      "Epoch [42515/50000], Train Loss: 24.7899, Test Loss: 41.4100\n",
      "Epoch [42520/50000], Train Loss: 18.4361, Test Loss: 42.8980\n",
      "Epoch [42525/50000], Train Loss: 25.0538, Test Loss: 46.0112\n",
      "Epoch [42530/50000], Train Loss: 25.5544, Test Loss: 40.5600\n",
      "Epoch [42535/50000], Train Loss: 22.7461, Test Loss: 41.7114\n",
      "Epoch [42540/50000], Train Loss: 21.6311, Test Loss: 43.3627\n",
      "Epoch [42545/50000], Train Loss: 26.5217, Test Loss: 41.3524\n",
      "Epoch [42550/50000], Train Loss: 35.1224, Test Loss: 41.2838\n",
      "Epoch [42555/50000], Train Loss: 23.7873, Test Loss: 43.0967\n",
      "Epoch [42560/50000], Train Loss: 18.8699, Test Loss: 43.2634\n",
      "Epoch [42565/50000], Train Loss: 19.5181, Test Loss: 41.8776\n",
      "Epoch [42570/50000], Train Loss: 21.5492, Test Loss: 49.3939\n",
      "Epoch [42575/50000], Train Loss: 29.7606, Test Loss: 43.7369\n",
      "Epoch [42580/50000], Train Loss: 21.7218, Test Loss: 41.4686\n",
      "Epoch [42585/50000], Train Loss: 20.0584, Test Loss: 42.2232\n",
      "Epoch [42590/50000], Train Loss: 19.1445, Test Loss: 42.3476\n",
      "Epoch [42595/50000], Train Loss: 25.7347, Test Loss: 40.7998\n",
      "Epoch [42600/50000], Train Loss: 26.0168, Test Loss: 42.4724\n",
      "Epoch [42605/50000], Train Loss: 18.9990, Test Loss: 41.4502\n",
      "Epoch [42610/50000], Train Loss: 19.8848, Test Loss: 47.6820\n",
      "Epoch [42615/50000], Train Loss: 19.8157, Test Loss: 41.2828\n",
      "Epoch [42620/50000], Train Loss: 24.6749, Test Loss: 43.2215\n",
      "Epoch [42625/50000], Train Loss: 20.3625, Test Loss: 43.6208\n",
      "Epoch [42630/50000], Train Loss: 42.6974, Test Loss: 48.1883\n",
      "Epoch [42635/50000], Train Loss: 20.1069, Test Loss: 42.7999\n",
      "Epoch [42640/50000], Train Loss: 20.9839, Test Loss: 43.0724\n",
      "Epoch [42645/50000], Train Loss: 23.7993, Test Loss: 44.3871\n",
      "Epoch [42650/50000], Train Loss: 19.2187, Test Loss: 44.6646\n",
      "Epoch [42655/50000], Train Loss: 19.6464, Test Loss: 41.3925\n",
      "Epoch [42660/50000], Train Loss: 18.9814, Test Loss: 45.0758\n",
      "Epoch [42665/50000], Train Loss: 18.4668, Test Loss: 45.3052\n",
      "Epoch [42670/50000], Train Loss: 20.6178, Test Loss: 42.0673\n",
      "Epoch [42675/50000], Train Loss: 21.4035, Test Loss: 43.0758\n",
      "Epoch [42680/50000], Train Loss: 18.8820, Test Loss: 49.6092\n",
      "Epoch [42685/50000], Train Loss: 23.6274, Test Loss: 46.3576\n",
      "Epoch [42690/50000], Train Loss: 20.4831, Test Loss: 41.8663\n",
      "Epoch [42695/50000], Train Loss: 18.8848, Test Loss: 43.4053\n",
      "Epoch [42700/50000], Train Loss: 29.0527, Test Loss: 42.1871\n",
      "Epoch [42705/50000], Train Loss: 16.1268, Test Loss: 41.1349\n",
      "Epoch [42710/50000], Train Loss: 17.4327, Test Loss: 44.4213\n",
      "Epoch [42715/50000], Train Loss: 26.2616, Test Loss: 42.4982\n",
      "Epoch [42720/50000], Train Loss: 20.1573, Test Loss: 45.7468\n",
      "Epoch [42725/50000], Train Loss: 25.3214, Test Loss: 44.0322\n",
      "Epoch [42730/50000], Train Loss: 18.6364, Test Loss: 44.4415\n",
      "Epoch [42735/50000], Train Loss: 23.4906, Test Loss: 40.9348\n",
      "Epoch [42740/50000], Train Loss: 22.3343, Test Loss: 42.4555\n",
      "Epoch [42745/50000], Train Loss: 19.3515, Test Loss: 44.7364\n",
      "Epoch [42750/50000], Train Loss: 19.2156, Test Loss: 41.1177\n",
      "Epoch [42755/50000], Train Loss: 17.5176, Test Loss: 50.3783\n",
      "Epoch [42760/50000], Train Loss: 19.7167, Test Loss: 41.0137\n",
      "Epoch [42765/50000], Train Loss: 20.6861, Test Loss: 41.8008\n",
      "Epoch [42770/50000], Train Loss: 23.6128, Test Loss: 41.6046\n",
      "Epoch [42775/50000], Train Loss: 21.0095, Test Loss: 48.1577\n",
      "Epoch [42780/50000], Train Loss: 19.5030, Test Loss: 46.0829\n",
      "Epoch [42785/50000], Train Loss: 19.4552, Test Loss: 41.6527\n",
      "Epoch [42790/50000], Train Loss: 18.7131, Test Loss: 43.6859\n",
      "Epoch [42795/50000], Train Loss: 28.6380, Test Loss: 44.4497\n",
      "Epoch [42800/50000], Train Loss: 24.8625, Test Loss: 54.9573\n",
      "Epoch [42805/50000], Train Loss: 21.2107, Test Loss: 43.4843\n",
      "Epoch [42810/50000], Train Loss: 18.8655, Test Loss: 42.9772\n",
      "Epoch [42815/50000], Train Loss: 20.9862, Test Loss: 41.4441\n",
      "Epoch [42820/50000], Train Loss: 25.7254, Test Loss: 42.0598\n",
      "Epoch [42825/50000], Train Loss: 54.6122, Test Loss: 41.2510\n",
      "Epoch [42830/50000], Train Loss: 19.4006, Test Loss: 41.8778\n",
      "Epoch [42835/50000], Train Loss: 18.6570, Test Loss: 42.9511\n",
      "Epoch [42840/50000], Train Loss: 17.6700, Test Loss: 43.9323\n",
      "Epoch [42845/50000], Train Loss: 26.5886, Test Loss: 40.2799\n",
      "Epoch [42850/50000], Train Loss: 21.1267, Test Loss: 41.2527\n",
      "Epoch [42855/50000], Train Loss: 21.7424, Test Loss: 50.3842\n",
      "Epoch [42860/50000], Train Loss: 18.6616, Test Loss: 48.9746\n",
      "Epoch [42865/50000], Train Loss: 21.6408, Test Loss: 41.7108\n",
      "Epoch [42870/50000], Train Loss: 20.1603, Test Loss: 41.9069\n",
      "Epoch [42875/50000], Train Loss: 20.5528, Test Loss: 61.1602\n",
      "Epoch [42880/50000], Train Loss: 19.0921, Test Loss: 42.5790\n",
      "Epoch [42885/50000], Train Loss: 18.1512, Test Loss: 40.4757\n",
      "Epoch [42890/50000], Train Loss: 26.4725, Test Loss: 42.4612\n",
      "Epoch [42895/50000], Train Loss: 18.5093, Test Loss: 41.6229\n",
      "Epoch [42900/50000], Train Loss: 23.3184, Test Loss: 49.8352\n",
      "Epoch [42905/50000], Train Loss: 35.3500, Test Loss: 40.8854\n",
      "Epoch [42910/50000], Train Loss: 25.7609, Test Loss: 51.5684\n",
      "Epoch [42915/50000], Train Loss: 19.8880, Test Loss: 41.4482\n",
      "Epoch [42920/50000], Train Loss: 25.1384, Test Loss: 44.3025\n",
      "Epoch [42925/50000], Train Loss: 21.0497, Test Loss: 43.1525\n",
      "Epoch [42930/50000], Train Loss: 19.0829, Test Loss: 41.7328\n",
      "Epoch [42935/50000], Train Loss: 23.9166, Test Loss: 42.9745\n",
      "Epoch [42940/50000], Train Loss: 18.1130, Test Loss: 42.3143\n",
      "Epoch [42945/50000], Train Loss: 31.4197, Test Loss: 42.1405\n",
      "Epoch [42950/50000], Train Loss: 23.1475, Test Loss: 41.5423\n",
      "Epoch [42955/50000], Train Loss: 21.0851, Test Loss: 41.1115\n",
      "Epoch [42960/50000], Train Loss: 22.5527, Test Loss: 44.4139\n",
      "Epoch [42965/50000], Train Loss: 18.7947, Test Loss: 43.1923\n",
      "Epoch [42970/50000], Train Loss: 20.5157, Test Loss: 46.0546\n",
      "Epoch [42975/50000], Train Loss: 20.9670, Test Loss: 42.8969\n",
      "Epoch [42980/50000], Train Loss: 18.7710, Test Loss: 40.5385\n",
      "Epoch [42985/50000], Train Loss: 19.3132, Test Loss: 40.4662\n",
      "Epoch [42990/50000], Train Loss: 20.8571, Test Loss: 41.5914\n",
      "Epoch [42995/50000], Train Loss: 19.7076, Test Loss: 44.5347\n",
      "Epoch [43000/50000], Train Loss: 22.1011, Test Loss: 40.5368\n",
      "Epoch [43005/50000], Train Loss: 18.4566, Test Loss: 41.4648\n",
      "Epoch [43010/50000], Train Loss: 18.1310, Test Loss: 42.6510\n",
      "Epoch [43015/50000], Train Loss: 21.4647, Test Loss: 42.6955\n",
      "Epoch [43020/50000], Train Loss: 23.0952, Test Loss: 42.0901\n",
      "Epoch [43025/50000], Train Loss: 23.1640, Test Loss: 43.2790\n",
      "Epoch [43030/50000], Train Loss: 21.5756, Test Loss: 42.9150\n",
      "Epoch [43035/50000], Train Loss: 21.2624, Test Loss: 46.9199\n",
      "Epoch [43040/50000], Train Loss: 21.2844, Test Loss: 41.6629\n",
      "Epoch [43045/50000], Train Loss: 21.5377, Test Loss: 49.6215\n",
      "Epoch [43050/50000], Train Loss: 30.7260, Test Loss: 81.6178\n",
      "Epoch [43055/50000], Train Loss: 21.9400, Test Loss: 47.0903\n",
      "Epoch [43060/50000], Train Loss: 17.9397, Test Loss: 40.5939\n",
      "Epoch [43065/50000], Train Loss: 22.0244, Test Loss: 43.9763\n",
      "Epoch [43070/50000], Train Loss: 21.8920, Test Loss: 52.4722\n",
      "Epoch [43075/50000], Train Loss: 20.3956, Test Loss: 43.3781\n",
      "Epoch [43080/50000], Train Loss: 44.8231, Test Loss: 93.6133\n",
      "Epoch [43085/50000], Train Loss: 20.4720, Test Loss: 46.6987\n",
      "Epoch [43090/50000], Train Loss: 23.5686, Test Loss: 42.9077\n",
      "Epoch [43095/50000], Train Loss: 17.8933, Test Loss: 45.3259\n",
      "Epoch [43100/50000], Train Loss: 20.6796, Test Loss: 41.7122\n",
      "Epoch [43105/50000], Train Loss: 20.2595, Test Loss: 41.3888\n",
      "Epoch [43110/50000], Train Loss: 18.5797, Test Loss: 44.6765\n",
      "Epoch [43115/50000], Train Loss: 19.1804, Test Loss: 41.3313\n",
      "Epoch [43120/50000], Train Loss: 18.1009, Test Loss: 43.3031\n",
      "Epoch [43125/50000], Train Loss: 26.0702, Test Loss: 40.6791\n",
      "Epoch [43130/50000], Train Loss: 19.7170, Test Loss: 41.9885\n",
      "Epoch [43135/50000], Train Loss: 20.2063, Test Loss: 41.8183\n",
      "Epoch [43140/50000], Train Loss: 21.7063, Test Loss: 41.6248\n",
      "Epoch [43145/50000], Train Loss: 19.5138, Test Loss: 41.9055\n",
      "Epoch [43150/50000], Train Loss: 17.8492, Test Loss: 40.8831\n",
      "Epoch [43155/50000], Train Loss: 17.4166, Test Loss: 45.3335\n",
      "Epoch [43160/50000], Train Loss: 19.5938, Test Loss: 48.9706\n",
      "Epoch [43165/50000], Train Loss: 21.3142, Test Loss: 44.7934\n",
      "Epoch [43170/50000], Train Loss: 32.9103, Test Loss: 42.1146\n",
      "Epoch [43175/50000], Train Loss: 18.0895, Test Loss: 41.7048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43180/50000], Train Loss: 20.3202, Test Loss: 40.9063\n",
      "Epoch [43185/50000], Train Loss: 26.3354, Test Loss: 44.1524\n",
      "Epoch [43190/50000], Train Loss: 20.8292, Test Loss: 42.2152\n",
      "Epoch [43195/50000], Train Loss: 19.1745, Test Loss: 42.9547\n",
      "Epoch [43200/50000], Train Loss: 21.1982, Test Loss: 41.2408\n",
      "Epoch [43205/50000], Train Loss: 22.9048, Test Loss: 40.8427\n",
      "Epoch [43210/50000], Train Loss: 27.1104, Test Loss: 40.7192\n",
      "Epoch [43215/50000], Train Loss: 20.0228, Test Loss: 40.6933\n",
      "Epoch [43220/50000], Train Loss: 18.5916, Test Loss: 42.3259\n",
      "Epoch [43225/50000], Train Loss: 32.2788, Test Loss: 40.6158\n",
      "Epoch [43230/50000], Train Loss: 21.1770, Test Loss: 43.1967\n",
      "Epoch [43235/50000], Train Loss: 15.2908, Test Loss: 49.1559\n",
      "Epoch [43240/50000], Train Loss: 19.5737, Test Loss: 46.8862\n",
      "Epoch [43245/50000], Train Loss: 21.3053, Test Loss: 40.8583\n",
      "Epoch [43250/50000], Train Loss: 18.4088, Test Loss: 46.6676\n",
      "Epoch [43255/50000], Train Loss: 15.3253, Test Loss: 41.0313\n",
      "Epoch [43260/50000], Train Loss: 19.7573, Test Loss: 42.1172\n",
      "Epoch [43265/50000], Train Loss: 20.3704, Test Loss: 49.4836\n",
      "Epoch [43270/50000], Train Loss: 28.8194, Test Loss: 42.2707\n",
      "Epoch [43275/50000], Train Loss: 16.4490, Test Loss: 40.8729\n",
      "Epoch [43280/50000], Train Loss: 21.8272, Test Loss: 45.8149\n",
      "Epoch [43285/50000], Train Loss: 19.3252, Test Loss: 41.9963\n",
      "Epoch [43290/50000], Train Loss: 14.4774, Test Loss: 40.1880\n",
      "Epoch [43295/50000], Train Loss: 68.3161, Test Loss: 43.1681\n",
      "Epoch [43300/50000], Train Loss: 16.7791, Test Loss: 41.3721\n",
      "Epoch [43305/50000], Train Loss: 16.7949, Test Loss: 41.7733\n",
      "Epoch [43310/50000], Train Loss: 23.1925, Test Loss: 42.7730\n",
      "Epoch [43315/50000], Train Loss: 21.3509, Test Loss: 41.1243\n",
      "Epoch [43320/50000], Train Loss: 18.0420, Test Loss: 47.2249\n",
      "Epoch [43325/50000], Train Loss: 41.8414, Test Loss: 41.3582\n",
      "Epoch [43330/50000], Train Loss: 26.9713, Test Loss: 42.6390\n",
      "Epoch [43335/50000], Train Loss: 19.7356, Test Loss: 41.6645\n",
      "Epoch [43340/50000], Train Loss: 23.3137, Test Loss: 42.0754\n",
      "Epoch [43345/50000], Train Loss: 19.5611, Test Loss: 57.4708\n",
      "Epoch [43350/50000], Train Loss: 26.6079, Test Loss: 46.2839\n",
      "Epoch [43355/50000], Train Loss: 17.8155, Test Loss: 44.1789\n",
      "Epoch [43360/50000], Train Loss: 16.1004, Test Loss: 40.7870\n",
      "Epoch [43365/50000], Train Loss: 22.0611, Test Loss: 44.5453\n",
      "Epoch [43370/50000], Train Loss: 23.9575, Test Loss: 41.8650\n",
      "Epoch [43375/50000], Train Loss: 22.9884, Test Loss: 42.7245\n",
      "Epoch [43380/50000], Train Loss: 22.9663, Test Loss: 43.9840\n",
      "Epoch [43385/50000], Train Loss: 19.1730, Test Loss: 41.5289\n",
      "Epoch [43390/50000], Train Loss: 20.0300, Test Loss: 40.8869\n",
      "Epoch [43395/50000], Train Loss: 19.3275, Test Loss: 40.4967\n",
      "Epoch [43400/50000], Train Loss: 19.9855, Test Loss: 43.0840\n",
      "Epoch [43405/50000], Train Loss: 21.9689, Test Loss: 41.3291\n",
      "Epoch [43410/50000], Train Loss: 19.0704, Test Loss: 45.0134\n",
      "Epoch [43415/50000], Train Loss: 18.2529, Test Loss: 43.1239\n",
      "Epoch [43420/50000], Train Loss: 19.4111, Test Loss: 42.1495\n",
      "Epoch [43425/50000], Train Loss: 22.3019, Test Loss: 42.3028\n",
      "Epoch [43430/50000], Train Loss: 19.5619, Test Loss: 44.4762\n",
      "Epoch [43435/50000], Train Loss: 19.1169, Test Loss: 45.3393\n",
      "Epoch [43440/50000], Train Loss: 17.9956, Test Loss: 44.5852\n",
      "Epoch [43445/50000], Train Loss: 25.4210, Test Loss: 42.2643\n",
      "Epoch [43450/50000], Train Loss: 18.0606, Test Loss: 40.9549\n",
      "Epoch [43455/50000], Train Loss: 27.1029, Test Loss: 41.4228\n",
      "Epoch [43460/50000], Train Loss: 18.0554, Test Loss: 42.2909\n",
      "Epoch [43465/50000], Train Loss: 19.7261, Test Loss: 42.1726\n",
      "Epoch [43470/50000], Train Loss: 19.3001, Test Loss: 41.4382\n",
      "Epoch [43475/50000], Train Loss: 20.6710, Test Loss: 40.3572\n",
      "Epoch [43480/50000], Train Loss: 18.4936, Test Loss: 42.7147\n",
      "Epoch [43485/50000], Train Loss: 21.2873, Test Loss: 41.0986\n",
      "Epoch [43490/50000], Train Loss: 22.7383, Test Loss: 41.6506\n",
      "Epoch [43495/50000], Train Loss: 19.9238, Test Loss: 42.2234\n",
      "Epoch [43500/50000], Train Loss: 19.8438, Test Loss: 41.7485\n",
      "Epoch [43505/50000], Train Loss: 19.6300, Test Loss: 42.1321\n",
      "Epoch [43510/50000], Train Loss: 20.1898, Test Loss: 43.3635\n",
      "Epoch [43515/50000], Train Loss: 25.2164, Test Loss: 40.1083\n",
      "Epoch [43520/50000], Train Loss: 19.9949, Test Loss: 44.6630\n",
      "Epoch [43525/50000], Train Loss: 23.5197, Test Loss: 40.9785\n",
      "Epoch [43530/50000], Train Loss: 19.9901, Test Loss: 40.8163\n",
      "Epoch [43535/50000], Train Loss: 15.7492, Test Loss: 42.9364\n",
      "Epoch [43540/50000], Train Loss: 20.1022, Test Loss: 43.1961\n",
      "Epoch [43545/50000], Train Loss: 20.8818, Test Loss: 41.4947\n",
      "Epoch [43550/50000], Train Loss: 22.5545, Test Loss: 43.8923\n",
      "Epoch [43555/50000], Train Loss: 15.8018, Test Loss: 40.7804\n",
      "Epoch [43560/50000], Train Loss: 19.3287, Test Loss: 43.2934\n",
      "Epoch [43565/50000], Train Loss: 14.6562, Test Loss: 41.1551\n",
      "Epoch [43570/50000], Train Loss: 21.5597, Test Loss: 49.0409\n",
      "Epoch [43575/50000], Train Loss: 19.8960, Test Loss: 42.8560\n",
      "Epoch [43580/50000], Train Loss: 21.8358, Test Loss: 42.8938\n",
      "Epoch [43585/50000], Train Loss: 22.7934, Test Loss: 42.0438\n",
      "Epoch [43590/50000], Train Loss: 30.3974, Test Loss: 59.9154\n",
      "Epoch [43595/50000], Train Loss: 20.0265, Test Loss: 45.7255\n",
      "Epoch [43600/50000], Train Loss: 19.5224, Test Loss: 44.3273\n",
      "Epoch [43605/50000], Train Loss: 24.7275, Test Loss: 41.2825\n",
      "Epoch [43610/50000], Train Loss: 23.2180, Test Loss: 40.9866\n",
      "Epoch [43615/50000], Train Loss: 19.1968, Test Loss: 43.4993\n",
      "Epoch [43620/50000], Train Loss: 27.7745, Test Loss: 44.1370\n",
      "Epoch [43625/50000], Train Loss: 23.1978, Test Loss: 41.8618\n",
      "Epoch [43630/50000], Train Loss: 18.8376, Test Loss: 45.2127\n",
      "Epoch [43635/50000], Train Loss: 19.4085, Test Loss: 44.5757\n",
      "Epoch [43640/50000], Train Loss: 18.2662, Test Loss: 48.6164\n",
      "Epoch [43645/50000], Train Loss: 21.8758, Test Loss: 43.0574\n",
      "Epoch [43650/50000], Train Loss: 22.5479, Test Loss: 41.8587\n",
      "Epoch [43655/50000], Train Loss: 21.8890, Test Loss: 41.9231\n",
      "Epoch [43660/50000], Train Loss: 20.6935, Test Loss: 42.4813\n",
      "Epoch [43665/50000], Train Loss: 27.0597, Test Loss: 45.0283\n",
      "Epoch [43670/50000], Train Loss: 17.5826, Test Loss: 40.6649\n",
      "Epoch [43675/50000], Train Loss: 23.3657, Test Loss: 44.2740\n",
      "Epoch [43680/50000], Train Loss: 19.4940, Test Loss: 42.8229\n",
      "Epoch [43685/50000], Train Loss: 18.6146, Test Loss: 42.9516\n",
      "Epoch [43690/50000], Train Loss: 21.0738, Test Loss: 47.9892\n",
      "Epoch [43695/50000], Train Loss: 24.4517, Test Loss: 46.2572\n",
      "Epoch [43700/50000], Train Loss: 16.9790, Test Loss: 41.6767\n",
      "Epoch [43705/50000], Train Loss: 21.4243, Test Loss: 41.0547\n",
      "Epoch [43710/50000], Train Loss: 20.5044, Test Loss: 42.1262\n",
      "Epoch [43715/50000], Train Loss: 16.7474, Test Loss: 41.6643\n",
      "Epoch [43720/50000], Train Loss: 19.0526, Test Loss: 43.9526\n",
      "Epoch [43725/50000], Train Loss: 22.3263, Test Loss: 42.2445\n",
      "Epoch [43730/50000], Train Loss: 19.6489, Test Loss: 40.7385\n",
      "Epoch [43735/50000], Train Loss: 35.9876, Test Loss: 42.5093\n",
      "Epoch [43740/50000], Train Loss: 19.2872, Test Loss: 44.2129\n",
      "Epoch [43745/50000], Train Loss: 18.6315, Test Loss: 45.8049\n",
      "Epoch [43750/50000], Train Loss: 18.7135, Test Loss: 43.7677\n",
      "Epoch [43755/50000], Train Loss: 16.4610, Test Loss: 41.9276\n",
      "Epoch [43760/50000], Train Loss: 22.2939, Test Loss: 42.3599\n",
      "Epoch [43765/50000], Train Loss: 21.9618, Test Loss: 41.4485\n",
      "Epoch [43770/50000], Train Loss: 19.6399, Test Loss: 41.6211\n",
      "Epoch [43775/50000], Train Loss: 19.3685, Test Loss: 42.3889\n",
      "Epoch [43780/50000], Train Loss: 20.4498, Test Loss: 42.7418\n",
      "Epoch [43785/50000], Train Loss: 19.1236, Test Loss: 42.3799\n",
      "Epoch [43790/50000], Train Loss: 20.3385, Test Loss: 40.3341\n",
      "Epoch [43795/50000], Train Loss: 17.4321, Test Loss: 42.2500\n",
      "Epoch [43800/50000], Train Loss: 21.2504, Test Loss: 44.9948\n",
      "Epoch [43805/50000], Train Loss: 16.6328, Test Loss: 41.5438\n",
      "Epoch [43810/50000], Train Loss: 17.9600, Test Loss: 40.4073\n",
      "Epoch [43815/50000], Train Loss: 20.5744, Test Loss: 41.0825\n",
      "Epoch [43820/50000], Train Loss: 23.7938, Test Loss: 43.4476\n",
      "Epoch [43825/50000], Train Loss: 22.1627, Test Loss: 41.6848\n",
      "Epoch [43830/50000], Train Loss: 19.5165, Test Loss: 41.4250\n",
      "Epoch [43835/50000], Train Loss: 19.1141, Test Loss: 43.5721\n",
      "Epoch [43840/50000], Train Loss: 20.2067, Test Loss: 44.3206\n",
      "Epoch [43845/50000], Train Loss: 25.6278, Test Loss: 41.6571\n",
      "Epoch [43850/50000], Train Loss: 19.3980, Test Loss: 41.7830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43855/50000], Train Loss: 19.8316, Test Loss: 43.0196\n",
      "Epoch [43860/50000], Train Loss: 29.8289, Test Loss: 42.4783\n",
      "Epoch [43865/50000], Train Loss: 20.3132, Test Loss: 42.8989\n",
      "Epoch [43870/50000], Train Loss: 21.4791, Test Loss: 41.3846\n",
      "Epoch [43875/50000], Train Loss: 21.6035, Test Loss: 44.9769\n",
      "Epoch [43880/50000], Train Loss: 19.3010, Test Loss: 40.8999\n",
      "Epoch [43885/50000], Train Loss: 17.2927, Test Loss: 44.0793\n",
      "Epoch [43890/50000], Train Loss: 22.4452, Test Loss: 40.5801\n",
      "Epoch [43895/50000], Train Loss: 26.8384, Test Loss: 41.6796\n",
      "Epoch [43900/50000], Train Loss: 19.3267, Test Loss: 43.2854\n",
      "Epoch [43905/50000], Train Loss: 20.9175, Test Loss: 43.0054\n",
      "Epoch [43910/50000], Train Loss: 16.8004, Test Loss: 42.0081\n",
      "Epoch [43915/50000], Train Loss: 19.3955, Test Loss: 43.7403\n",
      "Epoch [43920/50000], Train Loss: 18.6432, Test Loss: 44.8235\n",
      "Epoch [43925/50000], Train Loss: 19.3819, Test Loss: 58.6058\n",
      "Epoch [43930/50000], Train Loss: 19.3209, Test Loss: 50.0992\n",
      "Epoch [43935/50000], Train Loss: 22.1569, Test Loss: 41.5825\n",
      "Epoch [43940/50000], Train Loss: 22.2363, Test Loss: 43.5461\n",
      "Epoch [43945/50000], Train Loss: 29.9602, Test Loss: 41.4534\n",
      "Epoch [43950/50000], Train Loss: 23.8364, Test Loss: 42.5707\n",
      "Epoch [43955/50000], Train Loss: 18.1554, Test Loss: 43.3657\n",
      "Epoch [43960/50000], Train Loss: 16.8978, Test Loss: 41.3658\n",
      "Epoch [43965/50000], Train Loss: 20.2791, Test Loss: 40.7384\n",
      "Epoch [43970/50000], Train Loss: 18.5743, Test Loss: 42.6620\n",
      "Epoch [43975/50000], Train Loss: 19.7650, Test Loss: 61.8272\n",
      "Epoch [43980/50000], Train Loss: 19.9784, Test Loss: 42.8345\n",
      "Epoch [43985/50000], Train Loss: 20.6295, Test Loss: 41.7991\n",
      "Epoch [43990/50000], Train Loss: 30.4846, Test Loss: 41.6011\n",
      "Epoch [43995/50000], Train Loss: 21.4166, Test Loss: 48.8620\n",
      "Epoch [44000/50000], Train Loss: 23.5429, Test Loss: 44.7835\n",
      "Epoch [44005/50000], Train Loss: 17.0498, Test Loss: 43.2782\n",
      "Epoch [44010/50000], Train Loss: 24.1554, Test Loss: 40.8263\n",
      "Epoch [44015/50000], Train Loss: 19.5434, Test Loss: 41.6038\n",
      "Epoch [44020/50000], Train Loss: 17.8138, Test Loss: 41.3679\n",
      "Epoch [44025/50000], Train Loss: 20.1978, Test Loss: 44.4909\n",
      "Epoch [44030/50000], Train Loss: 19.4612, Test Loss: 41.9154\n",
      "Epoch [44035/50000], Train Loss: 19.3582, Test Loss: 40.8086\n",
      "Epoch [44040/50000], Train Loss: 19.4787, Test Loss: 47.4369\n",
      "Epoch [44045/50000], Train Loss: 19.3510, Test Loss: 42.6541\n",
      "Epoch [44050/50000], Train Loss: 20.2773, Test Loss: 41.3009\n",
      "Epoch [44055/50000], Train Loss: 18.4247, Test Loss: 44.5697\n",
      "Epoch [44060/50000], Train Loss: 22.6337, Test Loss: 42.3441\n",
      "Epoch [44065/50000], Train Loss: 20.5518, Test Loss: 45.6203\n",
      "Epoch [44070/50000], Train Loss: 23.4804, Test Loss: 41.0609\n",
      "Epoch [44075/50000], Train Loss: 23.3074, Test Loss: 40.7705\n",
      "Epoch [44080/50000], Train Loss: 16.1531, Test Loss: 44.8013\n",
      "Epoch [44085/50000], Train Loss: 28.9664, Test Loss: 41.4379\n",
      "Epoch [44090/50000], Train Loss: 22.1772, Test Loss: 41.8236\n",
      "Epoch [44095/50000], Train Loss: 21.4182, Test Loss: 50.1412\n",
      "Epoch [44100/50000], Train Loss: 18.7847, Test Loss: 45.1460\n",
      "Epoch [44105/50000], Train Loss: 18.9961, Test Loss: 42.3910\n",
      "Epoch [44110/50000], Train Loss: 19.3285, Test Loss: 40.2135\n",
      "Epoch [44115/50000], Train Loss: 19.9169, Test Loss: 43.1270\n",
      "Epoch [44120/50000], Train Loss: 18.9986, Test Loss: 52.0668\n",
      "Epoch [44125/50000], Train Loss: 20.1822, Test Loss: 45.4852\n",
      "Epoch [44130/50000], Train Loss: 23.4046, Test Loss: 40.0685\n",
      "Epoch [44135/50000], Train Loss: 17.3646, Test Loss: 43.6050\n",
      "Epoch [44140/50000], Train Loss: 22.5585, Test Loss: 42.4897\n",
      "Epoch [44145/50000], Train Loss: 17.1784, Test Loss: 42.1819\n",
      "Epoch [44150/50000], Train Loss: 19.2681, Test Loss: 41.1590\n",
      "Epoch [44155/50000], Train Loss: 17.1593, Test Loss: 42.8339\n",
      "Epoch [44160/50000], Train Loss: 21.2779, Test Loss: 44.2766\n",
      "Epoch [44165/50000], Train Loss: 20.7814, Test Loss: 41.8604\n",
      "Epoch [44170/50000], Train Loss: 18.9559, Test Loss: 43.9398\n",
      "Epoch [44175/50000], Train Loss: 19.7148, Test Loss: 45.9079\n",
      "Epoch [44180/50000], Train Loss: 18.1587, Test Loss: 43.4098\n",
      "Epoch [44185/50000], Train Loss: 21.4431, Test Loss: 46.3234\n",
      "Epoch [44190/50000], Train Loss: 22.2640, Test Loss: 41.6770\n",
      "Epoch [44195/50000], Train Loss: 13.9138, Test Loss: 42.0546\n",
      "Epoch [44200/50000], Train Loss: 19.5510, Test Loss: 40.4335\n",
      "Epoch [44205/50000], Train Loss: 20.5914, Test Loss: 42.1193\n",
      "Epoch [44210/50000], Train Loss: 17.3509, Test Loss: 41.1408\n",
      "Epoch [44215/50000], Train Loss: 19.0916, Test Loss: 41.5544\n",
      "Epoch [44220/50000], Train Loss: 17.9587, Test Loss: 41.4483\n",
      "Epoch [44225/50000], Train Loss: 21.8854, Test Loss: 42.6261\n",
      "Epoch [44230/50000], Train Loss: 19.6990, Test Loss: 43.9523\n",
      "Epoch [44235/50000], Train Loss: 19.5001, Test Loss: 42.8572\n",
      "Epoch [44240/50000], Train Loss: 27.3054, Test Loss: 41.8782\n",
      "Epoch [44245/50000], Train Loss: 18.9567, Test Loss: 41.2629\n",
      "Epoch [44250/50000], Train Loss: 18.1691, Test Loss: 41.1877\n",
      "Epoch [44255/50000], Train Loss: 21.5231, Test Loss: 40.7334\n",
      "Epoch [44260/50000], Train Loss: 23.0341, Test Loss: 46.5593\n",
      "Epoch [44265/50000], Train Loss: 19.4227, Test Loss: 49.0134\n",
      "Epoch [44270/50000], Train Loss: 19.3780, Test Loss: 43.1560\n",
      "Epoch [44275/50000], Train Loss: 18.6310, Test Loss: 44.0040\n",
      "Epoch [44280/50000], Train Loss: 18.0812, Test Loss: 40.3789\n",
      "Epoch [44285/50000], Train Loss: 38.0250, Test Loss: 43.0136\n",
      "Epoch [44290/50000], Train Loss: 19.9740, Test Loss: 42.9452\n",
      "Epoch [44295/50000], Train Loss: 22.8846, Test Loss: 40.9653\n",
      "Epoch [44300/50000], Train Loss: 17.7249, Test Loss: 43.9231\n",
      "Epoch [44305/50000], Train Loss: 20.0089, Test Loss: 42.6762\n",
      "Epoch [44310/50000], Train Loss: 21.9835, Test Loss: 44.5557\n",
      "Epoch [44315/50000], Train Loss: 20.5279, Test Loss: 42.1287\n",
      "Epoch [44320/50000], Train Loss: 19.7006, Test Loss: 42.3585\n",
      "Epoch [44325/50000], Train Loss: 21.9832, Test Loss: 42.8756\n",
      "Epoch [44330/50000], Train Loss: 23.0353, Test Loss: 42.4543\n",
      "Epoch [44335/50000], Train Loss: 17.5000, Test Loss: 44.6430\n",
      "Epoch [44340/50000], Train Loss: 19.7224, Test Loss: 43.5116\n",
      "Epoch [44345/50000], Train Loss: 22.7734, Test Loss: 42.1350\n",
      "Epoch [44350/50000], Train Loss: 22.6644, Test Loss: 41.0863\n",
      "Epoch [44355/50000], Train Loss: 20.3050, Test Loss: 44.8448\n",
      "Epoch [44360/50000], Train Loss: 22.3313, Test Loss: 40.9112\n",
      "Epoch [44365/50000], Train Loss: 20.5263, Test Loss: 42.3170\n",
      "Epoch [44370/50000], Train Loss: 17.1256, Test Loss: 41.1488\n",
      "Epoch [44375/50000], Train Loss: 18.2541, Test Loss: 46.3752\n",
      "Epoch [44380/50000], Train Loss: 21.0529, Test Loss: 43.9762\n",
      "Epoch [44385/50000], Train Loss: 20.5586, Test Loss: 44.0096\n",
      "Epoch [44390/50000], Train Loss: 22.4097, Test Loss: 41.0650\n",
      "Epoch [44395/50000], Train Loss: 19.0955, Test Loss: 47.5574\n",
      "Epoch [44400/50000], Train Loss: 19.4138, Test Loss: 46.0977\n",
      "Epoch [44405/50000], Train Loss: 18.9436, Test Loss: 42.3757\n",
      "Epoch [44410/50000], Train Loss: 24.5856, Test Loss: 40.5655\n",
      "Epoch [44415/50000], Train Loss: 21.3042, Test Loss: 44.9964\n",
      "Epoch [44420/50000], Train Loss: 23.2785, Test Loss: 43.8338\n",
      "Epoch [44425/50000], Train Loss: 18.4899, Test Loss: 41.5444\n",
      "Epoch [44430/50000], Train Loss: 21.7314, Test Loss: 41.5903\n",
      "Epoch [44435/50000], Train Loss: 19.4032, Test Loss: 48.5029\n",
      "Epoch [44440/50000], Train Loss: 25.6776, Test Loss: 42.6575\n",
      "Epoch [44445/50000], Train Loss: 19.9584, Test Loss: 43.8486\n",
      "Epoch [44450/50000], Train Loss: 21.2944, Test Loss: 40.7188\n",
      "Epoch [44455/50000], Train Loss: 33.0390, Test Loss: 45.7750\n",
      "Epoch [44460/50000], Train Loss: 18.8029, Test Loss: 41.8382\n",
      "Epoch [44465/50000], Train Loss: 20.3503, Test Loss: 40.7481\n",
      "Epoch [44470/50000], Train Loss: 21.2027, Test Loss: 41.7880\n",
      "Epoch [44475/50000], Train Loss: 21.6735, Test Loss: 42.6039\n",
      "Epoch [44480/50000], Train Loss: 18.9959, Test Loss: 45.3938\n",
      "Epoch [44485/50000], Train Loss: 21.6262, Test Loss: 43.1963\n",
      "Epoch [44490/50000], Train Loss: 15.4759, Test Loss: 44.1582\n",
      "Epoch [44495/50000], Train Loss: 22.9573, Test Loss: 41.0881\n",
      "Epoch [44500/50000], Train Loss: 21.3851, Test Loss: 47.1459\n",
      "Epoch [44505/50000], Train Loss: 14.5384, Test Loss: 41.6491\n",
      "Epoch [44510/50000], Train Loss: 21.9722, Test Loss: 43.4096\n",
      "Epoch [44515/50000], Train Loss: 21.1507, Test Loss: 40.5237\n",
      "Epoch [44520/50000], Train Loss: 23.0722, Test Loss: 43.5905\n",
      "Epoch [44525/50000], Train Loss: 28.3525, Test Loss: 40.7669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44530/50000], Train Loss: 21.5712, Test Loss: 42.9265\n",
      "Epoch [44535/50000], Train Loss: 19.1977, Test Loss: 43.5252\n",
      "Epoch [44540/50000], Train Loss: 13.5908, Test Loss: 40.2871\n",
      "Epoch [44545/50000], Train Loss: 22.8735, Test Loss: 42.9389\n",
      "Epoch [44550/50000], Train Loss: 18.2559, Test Loss: 43.1365\n",
      "Epoch [44555/50000], Train Loss: 23.1835, Test Loss: 46.0825\n",
      "Epoch [44560/50000], Train Loss: 19.4491, Test Loss: 42.4604\n",
      "Epoch [44565/50000], Train Loss: 21.7653, Test Loss: 42.1894\n",
      "Epoch [44570/50000], Train Loss: 21.3752, Test Loss: 40.1797\n",
      "Epoch [44575/50000], Train Loss: 15.9640, Test Loss: 41.2827\n",
      "Epoch [44580/50000], Train Loss: 20.8198, Test Loss: 43.7786\n",
      "Epoch [44585/50000], Train Loss: 22.0849, Test Loss: 45.8950\n",
      "Epoch [44590/50000], Train Loss: 21.7523, Test Loss: 40.6974\n",
      "Epoch [44595/50000], Train Loss: 22.2791, Test Loss: 43.1169\n",
      "Epoch [44600/50000], Train Loss: 19.4900, Test Loss: 41.9097\n",
      "Epoch [44605/50000], Train Loss: 20.5783, Test Loss: 41.6696\n",
      "Epoch [44610/50000], Train Loss: 18.4692, Test Loss: 42.8930\n",
      "Epoch [44615/50000], Train Loss: 19.1498, Test Loss: 42.4681\n",
      "Epoch [44620/50000], Train Loss: 18.3849, Test Loss: 46.5104\n",
      "Epoch [44625/50000], Train Loss: 22.1102, Test Loss: 40.1476\n",
      "Epoch [44630/50000], Train Loss: 18.4448, Test Loss: 43.0380\n",
      "Epoch [44635/50000], Train Loss: 59.1464, Test Loss: 39.9932\n",
      "Epoch [44640/50000], Train Loss: 19.5709, Test Loss: 44.3732\n",
      "Epoch [44645/50000], Train Loss: 19.8785, Test Loss: 41.9438\n",
      "Epoch [44650/50000], Train Loss: 19.5636, Test Loss: 42.1503\n",
      "Epoch [44655/50000], Train Loss: 17.5029, Test Loss: 40.5533\n",
      "Epoch [44660/50000], Train Loss: 18.9916, Test Loss: 42.0332\n",
      "Epoch [44665/50000], Train Loss: 15.3628, Test Loss: 44.2544\n",
      "Epoch [44670/50000], Train Loss: 18.9491, Test Loss: 41.5059\n",
      "Epoch [44675/50000], Train Loss: 25.4603, Test Loss: 44.9464\n",
      "Epoch [44680/50000], Train Loss: 22.0265, Test Loss: 42.0227\n",
      "Epoch [44685/50000], Train Loss: 19.3353, Test Loss: 44.4420\n",
      "Epoch [44690/50000], Train Loss: 20.1319, Test Loss: 41.1979\n",
      "Epoch [44695/50000], Train Loss: 19.8876, Test Loss: 40.9940\n",
      "Epoch [44700/50000], Train Loss: 23.2612, Test Loss: 41.3710\n",
      "Epoch [44705/50000], Train Loss: 17.3112, Test Loss: 41.3982\n",
      "Epoch [44710/50000], Train Loss: 23.1306, Test Loss: 42.6000\n",
      "Epoch [44715/50000], Train Loss: 19.7566, Test Loss: 40.2771\n",
      "Epoch [44720/50000], Train Loss: 21.9274, Test Loss: 41.5126\n",
      "Epoch [44725/50000], Train Loss: 22.3829, Test Loss: 42.7079\n",
      "Epoch [44730/50000], Train Loss: 23.6014, Test Loss: 40.7274\n",
      "Epoch [44735/50000], Train Loss: 18.4178, Test Loss: 42.0426\n",
      "Epoch [44740/50000], Train Loss: 17.6910, Test Loss: 40.3750\n",
      "Epoch [44745/50000], Train Loss: 17.1356, Test Loss: 43.4221\n",
      "Epoch [44750/50000], Train Loss: 17.3572, Test Loss: 42.2436\n",
      "Epoch [44755/50000], Train Loss: 18.3637, Test Loss: 42.2612\n",
      "Epoch [44760/50000], Train Loss: 18.6812, Test Loss: 42.0095\n",
      "Epoch [44765/50000], Train Loss: 18.3797, Test Loss: 43.9444\n",
      "Epoch [44770/50000], Train Loss: 18.5778, Test Loss: 44.4869\n",
      "Epoch [44775/50000], Train Loss: 21.6003, Test Loss: 41.0316\n",
      "Epoch [44780/50000], Train Loss: 16.1104, Test Loss: 43.1675\n",
      "Epoch [44785/50000], Train Loss: 20.3025, Test Loss: 43.9536\n",
      "Epoch [44790/50000], Train Loss: 23.2386, Test Loss: 42.1448\n",
      "Epoch [44795/50000], Train Loss: 16.3210, Test Loss: 41.8425\n",
      "Epoch [44800/50000], Train Loss: 23.7271, Test Loss: 41.8265\n",
      "Epoch [44805/50000], Train Loss: 19.6488, Test Loss: 43.5291\n",
      "Epoch [44810/50000], Train Loss: 30.7559, Test Loss: 56.6415\n",
      "Epoch [44815/50000], Train Loss: 21.0377, Test Loss: 41.3537\n",
      "Epoch [44820/50000], Train Loss: 17.2782, Test Loss: 40.4602\n",
      "Epoch [44825/50000], Train Loss: 39.1213, Test Loss: 52.0953\n",
      "Epoch [44830/50000], Train Loss: 19.7330, Test Loss: 41.3813\n",
      "Epoch [44835/50000], Train Loss: 23.3404, Test Loss: 42.9431\n",
      "Epoch [44840/50000], Train Loss: 16.9475, Test Loss: 46.9749\n",
      "Epoch [44845/50000], Train Loss: 21.6943, Test Loss: 40.7475\n",
      "Epoch [44850/50000], Train Loss: 26.4420, Test Loss: 42.1017\n",
      "Epoch [44855/50000], Train Loss: 20.2145, Test Loss: 47.3781\n",
      "Epoch [44860/50000], Train Loss: 21.1778, Test Loss: 40.9088\n",
      "Epoch [44865/50000], Train Loss: 20.0115, Test Loss: 41.8079\n",
      "Epoch [44870/50000], Train Loss: 20.5748, Test Loss: 46.5756\n",
      "Epoch [44875/50000], Train Loss: 20.2574, Test Loss: 46.2865\n",
      "Epoch [44880/50000], Train Loss: 20.3524, Test Loss: 43.1778\n",
      "Epoch [44885/50000], Train Loss: 19.1437, Test Loss: 41.8969\n",
      "Epoch [44890/50000], Train Loss: 24.5118, Test Loss: 41.3527\n",
      "Epoch [44895/50000], Train Loss: 21.8769, Test Loss: 50.1342\n",
      "Epoch [44900/50000], Train Loss: 22.5220, Test Loss: 41.0994\n",
      "Epoch [44905/50000], Train Loss: 17.2650, Test Loss: 41.5590\n",
      "Epoch [44910/50000], Train Loss: 18.6203, Test Loss: 41.3481\n",
      "Epoch [44915/50000], Train Loss: 21.0219, Test Loss: 46.5501\n",
      "Epoch [44920/50000], Train Loss: 16.9730, Test Loss: 41.5631\n",
      "Epoch [44925/50000], Train Loss: 20.9668, Test Loss: 41.0528\n",
      "Epoch [44930/50000], Train Loss: 19.7436, Test Loss: 42.1051\n",
      "Epoch [44935/50000], Train Loss: 19.9052, Test Loss: 42.8213\n",
      "Epoch [44940/50000], Train Loss: 19.2513, Test Loss: 41.3318\n",
      "Epoch [44945/50000], Train Loss: 19.9177, Test Loss: 43.4346\n",
      "Epoch [44950/50000], Train Loss: 20.7240, Test Loss: 43.7905\n",
      "Epoch [44955/50000], Train Loss: 21.3647, Test Loss: 40.7582\n",
      "Epoch [44960/50000], Train Loss: 19.8742, Test Loss: 42.2543\n",
      "Epoch [44965/50000], Train Loss: 20.8440, Test Loss: 42.0215\n",
      "Epoch [44970/50000], Train Loss: 23.2661, Test Loss: 41.0595\n",
      "Epoch [44975/50000], Train Loss: 19.1818, Test Loss: 43.4134\n",
      "Epoch [44980/50000], Train Loss: 18.3271, Test Loss: 42.3456\n",
      "Epoch [44985/50000], Train Loss: 19.7263, Test Loss: 40.8442\n",
      "Epoch [44990/50000], Train Loss: 19.4588, Test Loss: 43.0845\n",
      "Epoch [44995/50000], Train Loss: 17.3959, Test Loss: 45.3850\n",
      "Epoch [45000/50000], Train Loss: 25.8792, Test Loss: 44.0521\n",
      "Epoch [45005/50000], Train Loss: 23.8341, Test Loss: 41.1504\n",
      "Epoch [45010/50000], Train Loss: 17.4591, Test Loss: 41.9854\n",
      "Epoch [45015/50000], Train Loss: 16.4576, Test Loss: 42.9421\n",
      "Epoch [45020/50000], Train Loss: 29.6288, Test Loss: 42.7370\n",
      "Epoch [45025/50000], Train Loss: 21.4572, Test Loss: 41.5279\n",
      "Epoch [45030/50000], Train Loss: 20.4221, Test Loss: 41.1570\n",
      "Epoch [45035/50000], Train Loss: 19.1037, Test Loss: 40.9033\n",
      "Epoch [45040/50000], Train Loss: 23.2537, Test Loss: 41.3824\n",
      "Epoch [45045/50000], Train Loss: 17.1690, Test Loss: 40.9696\n",
      "Epoch [45050/50000], Train Loss: 21.6692, Test Loss: 42.7894\n",
      "Epoch [45055/50000], Train Loss: 19.1027, Test Loss: 41.6141\n",
      "Epoch [45060/50000], Train Loss: 17.4517, Test Loss: 42.1132\n",
      "Epoch [45065/50000], Train Loss: 21.5628, Test Loss: 43.2547\n",
      "Epoch [45070/50000], Train Loss: 24.4558, Test Loss: 41.9782\n",
      "Epoch [45075/50000], Train Loss: 20.0253, Test Loss: 42.4837\n",
      "Epoch [45080/50000], Train Loss: 17.6055, Test Loss: 42.4753\n",
      "Epoch [45085/50000], Train Loss: 18.7330, Test Loss: 43.6286\n",
      "Epoch [45090/50000], Train Loss: 28.4251, Test Loss: 42.1442\n",
      "Epoch [45095/50000], Train Loss: 18.7930, Test Loss: 40.7700\n",
      "Epoch [45100/50000], Train Loss: 19.7734, Test Loss: 41.2963\n",
      "Epoch [45105/50000], Train Loss: 26.2019, Test Loss: 41.0153\n",
      "Epoch [45110/50000], Train Loss: 20.6335, Test Loss: 41.9289\n",
      "Epoch [45115/50000], Train Loss: 19.6940, Test Loss: 41.2116\n",
      "Epoch [45120/50000], Train Loss: 19.3621, Test Loss: 40.5624\n",
      "Epoch [45125/50000], Train Loss: 18.8808, Test Loss: 41.4486\n",
      "Epoch [45130/50000], Train Loss: 20.5671, Test Loss: 43.4611\n",
      "Epoch [45135/50000], Train Loss: 19.3328, Test Loss: 45.1457\n",
      "Epoch [45140/50000], Train Loss: 19.9788, Test Loss: 40.9317\n",
      "Epoch [45145/50000], Train Loss: 20.8819, Test Loss: 45.7655\n",
      "Epoch [45150/50000], Train Loss: 18.2195, Test Loss: 41.5555\n",
      "Epoch [45155/50000], Train Loss: 22.7677, Test Loss: 44.8411\n",
      "Epoch [45160/50000], Train Loss: 17.7626, Test Loss: 42.3903\n",
      "Epoch [45165/50000], Train Loss: 24.1257, Test Loss: 46.0292\n",
      "Epoch [45170/50000], Train Loss: 19.1669, Test Loss: 40.9250\n",
      "Epoch [45175/50000], Train Loss: 20.2558, Test Loss: 42.9834\n",
      "Epoch [45180/50000], Train Loss: 21.0752, Test Loss: 41.5878\n",
      "Epoch [45185/50000], Train Loss: 34.2396, Test Loss: 40.7960\n",
      "Epoch [45190/50000], Train Loss: 21.7609, Test Loss: 42.6978\n",
      "Epoch [45195/50000], Train Loss: 23.0870, Test Loss: 42.3444\n",
      "Epoch [45200/50000], Train Loss: 20.0178, Test Loss: 42.0093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45205/50000], Train Loss: 18.8052, Test Loss: 43.4893\n",
      "Epoch [45210/50000], Train Loss: 19.5360, Test Loss: 42.0161\n",
      "Epoch [45215/50000], Train Loss: 19.2056, Test Loss: 48.3276\n",
      "Epoch [45220/50000], Train Loss: 21.9228, Test Loss: 48.5602\n",
      "Epoch [45225/50000], Train Loss: 11.4224, Test Loss: 40.8085\n",
      "Epoch [45230/50000], Train Loss: 20.3251, Test Loss: 45.3012\n",
      "Epoch [45235/50000], Train Loss: 18.4166, Test Loss: 41.1345\n",
      "Epoch [45240/50000], Train Loss: 18.5185, Test Loss: 41.7854\n",
      "Epoch [45245/50000], Train Loss: 25.3734, Test Loss: 44.7173\n",
      "Epoch [45250/50000], Train Loss: 21.9155, Test Loss: 41.4665\n",
      "Epoch [45255/50000], Train Loss: 18.7772, Test Loss: 43.9012\n",
      "Epoch [45260/50000], Train Loss: 17.5394, Test Loss: 43.5719\n",
      "Epoch [45265/50000], Train Loss: 13.8634, Test Loss: 40.5605\n",
      "Epoch [45270/50000], Train Loss: 18.2793, Test Loss: 42.8779\n",
      "Epoch [45275/50000], Train Loss: 22.6071, Test Loss: 42.1601\n",
      "Epoch [45280/50000], Train Loss: 18.9702, Test Loss: 42.8444\n",
      "Epoch [45285/50000], Train Loss: 20.1996, Test Loss: 42.7802\n",
      "Epoch [45290/50000], Train Loss: 23.2456, Test Loss: 41.1335\n",
      "Epoch [45295/50000], Train Loss: 19.8462, Test Loss: 42.9127\n",
      "Epoch [45300/50000], Train Loss: 22.2717, Test Loss: 43.2319\n",
      "Epoch [45305/50000], Train Loss: 18.9370, Test Loss: 44.7465\n",
      "Epoch [45310/50000], Train Loss: 17.5607, Test Loss: 41.5866\n",
      "Epoch [45315/50000], Train Loss: 21.4322, Test Loss: 42.9445\n",
      "Epoch [45320/50000], Train Loss: 17.2477, Test Loss: 47.8381\n",
      "Epoch [45325/50000], Train Loss: 32.3576, Test Loss: 40.8893\n",
      "Epoch [45330/50000], Train Loss: 18.8067, Test Loss: 42.1413\n",
      "Epoch [45335/50000], Train Loss: 14.3573, Test Loss: 43.0427\n",
      "Epoch [45340/50000], Train Loss: 25.6057, Test Loss: 40.8275\n",
      "Epoch [45345/50000], Train Loss: 19.9297, Test Loss: 41.9634\n",
      "Epoch [45350/50000], Train Loss: 21.0022, Test Loss: 40.7615\n",
      "Epoch [45355/50000], Train Loss: 28.1693, Test Loss: 42.6330\n",
      "Epoch [45360/50000], Train Loss: 17.1786, Test Loss: 42.0477\n",
      "Epoch [45365/50000], Train Loss: 17.8352, Test Loss: 44.4702\n",
      "Epoch [45370/50000], Train Loss: 24.1450, Test Loss: 50.5803\n",
      "Epoch [45375/50000], Train Loss: 19.8294, Test Loss: 42.3327\n",
      "Epoch [45380/50000], Train Loss: 18.2517, Test Loss: 41.7091\n",
      "Epoch [45385/50000], Train Loss: 21.1903, Test Loss: 47.6918\n",
      "Epoch [45390/50000], Train Loss: 21.8084, Test Loss: 42.6109\n",
      "Epoch [45395/50000], Train Loss: 19.3661, Test Loss: 47.4210\n",
      "Epoch [45400/50000], Train Loss: 14.7101, Test Loss: 42.3838\n",
      "Epoch [45405/50000], Train Loss: 31.6034, Test Loss: 41.0046\n",
      "Epoch [45410/50000], Train Loss: 19.0046, Test Loss: 46.0257\n",
      "Epoch [45415/50000], Train Loss: 19.5510, Test Loss: 49.8680\n",
      "Epoch [45420/50000], Train Loss: 18.4516, Test Loss: 41.2734\n",
      "Epoch [45425/50000], Train Loss: 23.0256, Test Loss: 40.5411\n",
      "Epoch [45430/50000], Train Loss: 18.9594, Test Loss: 44.4134\n",
      "Epoch [45435/50000], Train Loss: 21.0252, Test Loss: 57.2511\n",
      "Epoch [45440/50000], Train Loss: 18.2921, Test Loss: 44.8863\n",
      "Epoch [45445/50000], Train Loss: 15.2265, Test Loss: 42.9714\n",
      "Epoch [45450/50000], Train Loss: 20.0457, Test Loss: 40.4119\n",
      "Epoch [45455/50000], Train Loss: 19.1903, Test Loss: 40.7656\n",
      "Epoch [45460/50000], Train Loss: 18.9022, Test Loss: 43.9356\n",
      "Epoch [45465/50000], Train Loss: 20.2952, Test Loss: 41.0636\n",
      "Epoch [45470/50000], Train Loss: 16.4729, Test Loss: 41.5222\n",
      "Epoch [45475/50000], Train Loss: 44.2718, Test Loss: 40.4845\n",
      "Epoch [45480/50000], Train Loss: 18.8928, Test Loss: 40.4388\n",
      "Epoch [45485/50000], Train Loss: 19.8166, Test Loss: 41.7663\n",
      "Epoch [45490/50000], Train Loss: 19.8264, Test Loss: 40.1799\n",
      "Epoch [45495/50000], Train Loss: 20.6783, Test Loss: 42.5938\n",
      "Epoch [45500/50000], Train Loss: 22.6566, Test Loss: 42.9776\n",
      "Epoch [45505/50000], Train Loss: 18.3281, Test Loss: 41.4579\n",
      "Epoch [45510/50000], Train Loss: 21.3309, Test Loss: 42.9526\n",
      "Epoch [45515/50000], Train Loss: 18.8511, Test Loss: 42.5471\n",
      "Epoch [45520/50000], Train Loss: 26.2919, Test Loss: 40.8504\n",
      "Epoch [45525/50000], Train Loss: 21.4418, Test Loss: 46.5418\n",
      "Epoch [45530/50000], Train Loss: 18.3462, Test Loss: 41.6190\n",
      "Epoch [45535/50000], Train Loss: 22.3306, Test Loss: 45.3324\n",
      "Epoch [45540/50000], Train Loss: 22.4866, Test Loss: 41.1485\n",
      "Epoch [45545/50000], Train Loss: 18.1506, Test Loss: 44.3083\n",
      "Epoch [45550/50000], Train Loss: 22.1720, Test Loss: 45.1186\n",
      "Epoch [45555/50000], Train Loss: 23.7316, Test Loss: 43.1112\n",
      "Epoch [45560/50000], Train Loss: 19.9690, Test Loss: 42.5568\n",
      "Epoch [45565/50000], Train Loss: 17.9079, Test Loss: 44.0200\n",
      "Epoch [45570/50000], Train Loss: 17.8439, Test Loss: 44.7896\n",
      "Epoch [45575/50000], Train Loss: 24.6322, Test Loss: 40.5446\n",
      "Epoch [45580/50000], Train Loss: 22.1744, Test Loss: 48.4176\n",
      "Epoch [45585/50000], Train Loss: 17.7783, Test Loss: 49.8708\n",
      "Epoch [45590/50000], Train Loss: 15.5782, Test Loss: 42.1394\n",
      "Epoch [45595/50000], Train Loss: 22.4131, Test Loss: 41.3077\n",
      "Epoch [45600/50000], Train Loss: 18.2929, Test Loss: 42.5045\n",
      "Epoch [45605/50000], Train Loss: 37.2217, Test Loss: 40.9895\n",
      "Epoch [45610/50000], Train Loss: 18.1509, Test Loss: 42.5200\n",
      "Epoch [45615/50000], Train Loss: 21.0031, Test Loss: 45.7555\n",
      "Epoch [45620/50000], Train Loss: 18.2563, Test Loss: 44.3650\n",
      "Epoch [45625/50000], Train Loss: 22.1627, Test Loss: 43.6627\n",
      "Epoch [45630/50000], Train Loss: 20.6377, Test Loss: 42.7172\n",
      "Epoch [45635/50000], Train Loss: 21.1186, Test Loss: 42.4197\n",
      "Epoch [45640/50000], Train Loss: 20.7209, Test Loss: 41.4408\n",
      "Epoch [45645/50000], Train Loss: 19.1500, Test Loss: 43.1302\n",
      "Epoch [45650/50000], Train Loss: 19.3565, Test Loss: 43.4026\n",
      "Epoch [45655/50000], Train Loss: 38.5610, Test Loss: 68.1916\n",
      "Epoch [45660/50000], Train Loss: 21.7774, Test Loss: 42.2889\n",
      "Epoch [45665/50000], Train Loss: 22.0624, Test Loss: 44.6978\n",
      "Epoch [45670/50000], Train Loss: 22.8846, Test Loss: 40.1925\n",
      "Epoch [45675/50000], Train Loss: 20.2504, Test Loss: 45.1758\n",
      "Epoch [45680/50000], Train Loss: 19.1029, Test Loss: 46.1525\n",
      "Epoch [45685/50000], Train Loss: 20.1389, Test Loss: 44.3640\n",
      "Epoch [45690/50000], Train Loss: 22.2433, Test Loss: 42.4462\n",
      "Epoch [45695/50000], Train Loss: 21.7275, Test Loss: 42.2672\n",
      "Epoch [45700/50000], Train Loss: 25.5709, Test Loss: 42.8959\n",
      "Epoch [45705/50000], Train Loss: 23.4640, Test Loss: 43.4580\n",
      "Epoch [45710/50000], Train Loss: 19.2980, Test Loss: 42.7512\n",
      "Epoch [45715/50000], Train Loss: 16.4782, Test Loss: 42.5077\n",
      "Epoch [45720/50000], Train Loss: 22.6946, Test Loss: 43.8331\n",
      "Epoch [45725/50000], Train Loss: 22.3024, Test Loss: 44.2996\n",
      "Epoch [45730/50000], Train Loss: 16.5732, Test Loss: 42.7811\n",
      "Epoch [45735/50000], Train Loss: 19.4404, Test Loss: 42.4532\n",
      "Epoch [45740/50000], Train Loss: 20.5432, Test Loss: 41.7011\n",
      "Epoch [45745/50000], Train Loss: 22.1296, Test Loss: 41.1394\n",
      "Epoch [45750/50000], Train Loss: 19.1785, Test Loss: 45.8329\n",
      "Epoch [45755/50000], Train Loss: 18.4464, Test Loss: 41.0186\n",
      "Epoch [45760/50000], Train Loss: 17.3604, Test Loss: 42.3009\n",
      "Epoch [45765/50000], Train Loss: 32.2999, Test Loss: 45.3781\n",
      "Epoch [45770/50000], Train Loss: 16.9173, Test Loss: 50.0358\n",
      "Epoch [45775/50000], Train Loss: 19.0000, Test Loss: 41.5316\n",
      "Epoch [45780/50000], Train Loss: 17.9873, Test Loss: 42.2535\n",
      "Epoch [45785/50000], Train Loss: 24.7442, Test Loss: 41.3317\n",
      "Epoch [45790/50000], Train Loss: 20.9651, Test Loss: 42.5669\n",
      "Epoch [45795/50000], Train Loss: 17.8304, Test Loss: 41.9857\n",
      "Epoch [45800/50000], Train Loss: 21.1051, Test Loss: 40.4472\n",
      "Epoch [45805/50000], Train Loss: 19.0700, Test Loss: 41.3560\n",
      "Epoch [45810/50000], Train Loss: 32.7530, Test Loss: 40.8339\n",
      "Epoch [45815/50000], Train Loss: 21.3724, Test Loss: 42.8962\n",
      "Epoch [45820/50000], Train Loss: 23.3770, Test Loss: 44.3892\n",
      "Epoch [45825/50000], Train Loss: 19.4675, Test Loss: 44.4935\n",
      "Epoch [45830/50000], Train Loss: 16.9055, Test Loss: 41.0056\n",
      "Epoch [45835/50000], Train Loss: 18.8387, Test Loss: 42.9281\n",
      "Epoch [45840/50000], Train Loss: 23.3902, Test Loss: 48.6128\n",
      "Epoch [45845/50000], Train Loss: 16.1346, Test Loss: 41.5434\n",
      "Epoch [45850/50000], Train Loss: 20.7754, Test Loss: 45.1041\n",
      "Epoch [45855/50000], Train Loss: 22.6369, Test Loss: 42.3533\n",
      "Epoch [45860/50000], Train Loss: 19.8381, Test Loss: 44.5285\n",
      "Epoch [45865/50000], Train Loss: 66.7922, Test Loss: 50.9059\n",
      "Epoch [45870/50000], Train Loss: 23.1874, Test Loss: 43.3643\n",
      "Epoch [45875/50000], Train Loss: 21.9112, Test Loss: 42.4988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45880/50000], Train Loss: 19.3502, Test Loss: 45.7143\n",
      "Epoch [45885/50000], Train Loss: 21.2005, Test Loss: 41.7393\n",
      "Epoch [45890/50000], Train Loss: 19.7288, Test Loss: 41.7863\n",
      "Epoch [45895/50000], Train Loss: 30.1123, Test Loss: 41.8873\n",
      "Epoch [45900/50000], Train Loss: 16.9029, Test Loss: 41.7624\n",
      "Epoch [45905/50000], Train Loss: 18.5765, Test Loss: 40.8319\n",
      "Epoch [45910/50000], Train Loss: 14.4315, Test Loss: 40.7655\n",
      "Epoch [45915/50000], Train Loss: 19.6899, Test Loss: 42.2917\n",
      "Epoch [45920/50000], Train Loss: 20.5798, Test Loss: 42.7682\n",
      "Epoch [45925/50000], Train Loss: 16.8560, Test Loss: 43.1312\n",
      "Epoch [45930/50000], Train Loss: 23.9782, Test Loss: 43.2491\n",
      "Epoch [45935/50000], Train Loss: 21.0570, Test Loss: 41.1223\n",
      "Epoch [45940/50000], Train Loss: 18.9123, Test Loss: 48.7088\n",
      "Epoch [45945/50000], Train Loss: 14.2144, Test Loss: 40.9161\n",
      "Epoch [45950/50000], Train Loss: 17.4766, Test Loss: 40.5390\n",
      "Epoch [45955/50000], Train Loss: 22.4935, Test Loss: 42.9548\n",
      "Epoch [45960/50000], Train Loss: 20.0509, Test Loss: 42.6037\n",
      "Epoch [45965/50000], Train Loss: 16.8618, Test Loss: 42.0887\n",
      "Epoch [45970/50000], Train Loss: 18.8712, Test Loss: 43.8227\n",
      "Epoch [45975/50000], Train Loss: 19.6617, Test Loss: 43.0904\n",
      "Epoch [45980/50000], Train Loss: 20.3143, Test Loss: 44.6474\n",
      "Epoch [45985/50000], Train Loss: 21.8669, Test Loss: 41.9275\n",
      "Epoch [45990/50000], Train Loss: 26.4123, Test Loss: 50.5809\n",
      "Epoch [45995/50000], Train Loss: 18.3525, Test Loss: 41.0498\n",
      "Epoch [46000/50000], Train Loss: 19.2099, Test Loss: 40.6433\n",
      "Epoch [46005/50000], Train Loss: 18.6169, Test Loss: 46.8514\n",
      "Epoch [46010/50000], Train Loss: 19.3116, Test Loss: 42.1483\n",
      "Epoch [46015/50000], Train Loss: 16.4027, Test Loss: 41.2193\n",
      "Epoch [46020/50000], Train Loss: 15.3571, Test Loss: 44.4643\n",
      "Epoch [46025/50000], Train Loss: 24.5857, Test Loss: 40.9575\n",
      "Epoch [46030/50000], Train Loss: 20.2469, Test Loss: 47.8117\n",
      "Epoch [46035/50000], Train Loss: 18.6616, Test Loss: 42.6249\n",
      "Epoch [46040/50000], Train Loss: 21.4553, Test Loss: 43.9147\n",
      "Epoch [46045/50000], Train Loss: 25.1273, Test Loss: 40.6228\n",
      "Epoch [46050/50000], Train Loss: 18.2561, Test Loss: 43.9262\n",
      "Epoch [46055/50000], Train Loss: 13.9453, Test Loss: 41.1830\n",
      "Epoch [46060/50000], Train Loss: 19.1105, Test Loss: 40.9522\n",
      "Epoch [46065/50000], Train Loss: 17.0276, Test Loss: 43.0748\n",
      "Epoch [46070/50000], Train Loss: 20.8499, Test Loss: 40.8531\n",
      "Epoch [46075/50000], Train Loss: 17.2378, Test Loss: 42.0557\n",
      "Epoch [46080/50000], Train Loss: 16.5716, Test Loss: 51.2578\n",
      "Epoch [46085/50000], Train Loss: 20.3951, Test Loss: 43.4336\n",
      "Epoch [46090/50000], Train Loss: 18.2961, Test Loss: 42.2183\n",
      "Epoch [46095/50000], Train Loss: 19.6855, Test Loss: 43.5644\n",
      "Epoch [46100/50000], Train Loss: 22.1576, Test Loss: 40.8731\n",
      "Epoch [46105/50000], Train Loss: 19.6734, Test Loss: 44.5545\n",
      "Epoch [46110/50000], Train Loss: 20.6244, Test Loss: 41.7839\n",
      "Epoch [46115/50000], Train Loss: 19.8497, Test Loss: 42.7082\n",
      "Epoch [46120/50000], Train Loss: 18.0497, Test Loss: 41.7741\n",
      "Epoch [46125/50000], Train Loss: 16.0905, Test Loss: 43.2767\n",
      "Epoch [46130/50000], Train Loss: 21.0604, Test Loss: 43.0729\n",
      "Epoch [46135/50000], Train Loss: 17.1604, Test Loss: 42.4037\n",
      "Epoch [46140/50000], Train Loss: 28.7587, Test Loss: 41.6380\n",
      "Epoch [46145/50000], Train Loss: 18.7555, Test Loss: 42.7998\n",
      "Epoch [46150/50000], Train Loss: 28.1461, Test Loss: 44.4650\n",
      "Epoch [46155/50000], Train Loss: 18.6299, Test Loss: 42.8180\n",
      "Epoch [46160/50000], Train Loss: 16.5689, Test Loss: 46.8414\n",
      "Epoch [46165/50000], Train Loss: 23.4809, Test Loss: 42.2592\n",
      "Epoch [46170/50000], Train Loss: 16.9335, Test Loss: 43.1887\n",
      "Epoch [46175/50000], Train Loss: 20.3927, Test Loss: 44.0511\n",
      "Epoch [46180/50000], Train Loss: 32.1382, Test Loss: 41.6276\n",
      "Epoch [46185/50000], Train Loss: 20.0375, Test Loss: 41.2629\n",
      "Epoch [46190/50000], Train Loss: 16.6781, Test Loss: 41.5529\n",
      "Epoch [46195/50000], Train Loss: 20.5676, Test Loss: 50.3287\n",
      "Epoch [46200/50000], Train Loss: 21.6836, Test Loss: 40.4243\n",
      "Epoch [46205/50000], Train Loss: 19.4248, Test Loss: 42.7612\n",
      "Epoch [46210/50000], Train Loss: 21.5734, Test Loss: 42.2583\n",
      "Epoch [46215/50000], Train Loss: 15.5929, Test Loss: 41.6977\n",
      "Epoch [46220/50000], Train Loss: 18.3342, Test Loss: 42.1287\n",
      "Epoch [46225/50000], Train Loss: 27.9431, Test Loss: 41.9559\n",
      "Epoch [46230/50000], Train Loss: 20.5260, Test Loss: 42.5379\n",
      "Epoch [46235/50000], Train Loss: 52.3238, Test Loss: 48.5259\n",
      "Epoch [46240/50000], Train Loss: 18.6359, Test Loss: 40.1080\n",
      "Epoch [46245/50000], Train Loss: 18.7648, Test Loss: 44.0635\n",
      "Epoch [46250/50000], Train Loss: 19.7119, Test Loss: 41.2617\n",
      "Epoch [46255/50000], Train Loss: 19.7999, Test Loss: 42.3091\n",
      "Epoch [46260/50000], Train Loss: 20.4126, Test Loss: 42.3475\n",
      "Epoch [46265/50000], Train Loss: 36.3595, Test Loss: 40.4990\n",
      "Epoch [46270/50000], Train Loss: 20.4068, Test Loss: 41.2012\n",
      "Epoch [46275/50000], Train Loss: 21.3508, Test Loss: 43.6230\n",
      "Epoch [46280/50000], Train Loss: 20.3713, Test Loss: 43.6860\n",
      "Epoch [46285/50000], Train Loss: 18.0316, Test Loss: 41.7539\n",
      "Epoch [46290/50000], Train Loss: 22.7524, Test Loss: 46.9848\n",
      "Epoch [46295/50000], Train Loss: 18.4258, Test Loss: 44.1514\n",
      "Epoch [46300/50000], Train Loss: 23.2505, Test Loss: 43.2394\n",
      "Epoch [46305/50000], Train Loss: 17.2084, Test Loss: 43.6579\n",
      "Epoch [46310/50000], Train Loss: 38.7858, Test Loss: 70.7148\n",
      "Epoch [46315/50000], Train Loss: 17.8523, Test Loss: 41.9703\n",
      "Epoch [46320/50000], Train Loss: 20.6530, Test Loss: 49.5187\n",
      "Epoch [46325/50000], Train Loss: 19.9379, Test Loss: 41.0272\n",
      "Epoch [46330/50000], Train Loss: 15.7045, Test Loss: 42.9934\n",
      "Epoch [46335/50000], Train Loss: 22.5628, Test Loss: 42.3693\n",
      "Epoch [46340/50000], Train Loss: 21.2701, Test Loss: 42.2944\n",
      "Epoch [46345/50000], Train Loss: 19.9120, Test Loss: 43.5346\n",
      "Epoch [46350/50000], Train Loss: 15.6141, Test Loss: 41.1139\n",
      "Epoch [46355/50000], Train Loss: 21.6550, Test Loss: 42.2124\n",
      "Epoch [46360/50000], Train Loss: 16.0839, Test Loss: 41.5566\n",
      "Epoch [46365/50000], Train Loss: 22.9198, Test Loss: 42.8510\n",
      "Epoch [46370/50000], Train Loss: 20.4281, Test Loss: 44.0561\n",
      "Epoch [46375/50000], Train Loss: 21.7615, Test Loss: 45.3366\n",
      "Epoch [46380/50000], Train Loss: 18.4479, Test Loss: 41.0988\n",
      "Epoch [46385/50000], Train Loss: 18.0286, Test Loss: 40.5015\n",
      "Epoch [46390/50000], Train Loss: 17.8092, Test Loss: 40.5643\n",
      "Epoch [46395/50000], Train Loss: 19.6765, Test Loss: 43.7844\n",
      "Epoch [46400/50000], Train Loss: 19.1624, Test Loss: 42.7391\n",
      "Epoch [46405/50000], Train Loss: 16.1547, Test Loss: 42.8398\n",
      "Epoch [46410/50000], Train Loss: 18.4756, Test Loss: 62.8709\n",
      "Epoch [46415/50000], Train Loss: 22.1868, Test Loss: 48.2680\n",
      "Epoch [46420/50000], Train Loss: 20.4175, Test Loss: 46.7614\n",
      "Epoch [46425/50000], Train Loss: 18.3518, Test Loss: 43.3273\n",
      "Epoch [46430/50000], Train Loss: 23.8728, Test Loss: 41.5224\n",
      "Epoch [46435/50000], Train Loss: 25.2526, Test Loss: 42.4025\n",
      "Epoch [46440/50000], Train Loss: 17.8904, Test Loss: 41.4825\n",
      "Epoch [46445/50000], Train Loss: 18.8415, Test Loss: 42.6340\n",
      "Epoch [46450/50000], Train Loss: 21.5504, Test Loss: 44.8158\n",
      "Epoch [46455/50000], Train Loss: 18.7933, Test Loss: 45.4791\n",
      "Epoch [46460/50000], Train Loss: 23.6021, Test Loss: 44.2684\n",
      "Epoch [46465/50000], Train Loss: 19.9450, Test Loss: 42.5900\n",
      "Epoch [46470/50000], Train Loss: 31.4453, Test Loss: 85.7601\n",
      "Epoch [46475/50000], Train Loss: 21.7955, Test Loss: 41.9936\n",
      "Epoch [46480/50000], Train Loss: 21.2261, Test Loss: 41.0723\n",
      "Epoch [46485/50000], Train Loss: 13.0714, Test Loss: 40.3247\n",
      "Epoch [46490/50000], Train Loss: 19.3377, Test Loss: 41.4180\n",
      "Epoch [46495/50000], Train Loss: 18.5766, Test Loss: 43.4312\n",
      "Epoch [46500/50000], Train Loss: 22.9518, Test Loss: 43.8496\n",
      "Epoch [46505/50000], Train Loss: 18.2281, Test Loss: 42.3117\n",
      "Epoch [46510/50000], Train Loss: 18.3464, Test Loss: 42.5723\n",
      "Epoch [46515/50000], Train Loss: 22.6578, Test Loss: 48.4562\n",
      "Epoch [46520/50000], Train Loss: 21.6186, Test Loss: 42.8984\n",
      "Epoch [46525/50000], Train Loss: 19.1804, Test Loss: 41.6361\n",
      "Epoch [46530/50000], Train Loss: 29.1504, Test Loss: 42.0509\n",
      "Epoch [46535/50000], Train Loss: 18.9825, Test Loss: 43.7120\n",
      "Epoch [46540/50000], Train Loss: 18.0436, Test Loss: 45.4129\n",
      "Epoch [46545/50000], Train Loss: 16.3984, Test Loss: 43.3070\n",
      "Epoch [46550/50000], Train Loss: 16.4730, Test Loss: 45.4124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46555/50000], Train Loss: 20.5114, Test Loss: 44.4337\n",
      "Epoch [46560/50000], Train Loss: 18.3927, Test Loss: 45.2928\n",
      "Epoch [46565/50000], Train Loss: 19.9933, Test Loss: 41.5653\n",
      "Epoch [46570/50000], Train Loss: 20.2599, Test Loss: 47.2069\n",
      "Epoch [46575/50000], Train Loss: 26.4268, Test Loss: 42.7910\n",
      "Epoch [46580/50000], Train Loss: 20.4384, Test Loss: 41.2926\n",
      "Epoch [46585/50000], Train Loss: 14.2297, Test Loss: 40.0192\n",
      "Epoch [46590/50000], Train Loss: 26.6890, Test Loss: 41.7150\n",
      "Epoch [46595/50000], Train Loss: 25.7360, Test Loss: 42.0550\n",
      "Epoch [46600/50000], Train Loss: 21.8050, Test Loss: 44.4479\n",
      "Epoch [46605/50000], Train Loss: 24.6157, Test Loss: 42.5588\n",
      "Epoch [46610/50000], Train Loss: 17.3488, Test Loss: 41.3303\n",
      "Epoch [46615/50000], Train Loss: 28.6551, Test Loss: 40.9598\n",
      "Epoch [46620/50000], Train Loss: 15.6205, Test Loss: 41.9367\n",
      "Epoch [46625/50000], Train Loss: 22.9439, Test Loss: 41.5441\n",
      "Epoch [46630/50000], Train Loss: 16.5294, Test Loss: 45.2691\n",
      "Epoch [46635/50000], Train Loss: 18.5644, Test Loss: 40.9216\n",
      "Epoch [46640/50000], Train Loss: 18.5829, Test Loss: 49.0998\n",
      "Epoch [46645/50000], Train Loss: 20.2785, Test Loss: 50.0338\n",
      "Epoch [46650/50000], Train Loss: 19.3121, Test Loss: 43.1340\n",
      "Epoch [46655/50000], Train Loss: 19.0154, Test Loss: 42.4924\n",
      "Epoch [46660/50000], Train Loss: 19.0551, Test Loss: 42.1358\n",
      "Epoch [46665/50000], Train Loss: 16.2368, Test Loss: 42.2022\n",
      "Epoch [46670/50000], Train Loss: 18.4347, Test Loss: 41.2438\n",
      "Epoch [46675/50000], Train Loss: 18.3624, Test Loss: 41.1630\n",
      "Epoch [46680/50000], Train Loss: 18.4368, Test Loss: 44.0146\n",
      "Epoch [46685/50000], Train Loss: 20.4840, Test Loss: 47.5573\n",
      "Epoch [46690/50000], Train Loss: 17.0173, Test Loss: 44.6721\n",
      "Epoch [46695/50000], Train Loss: 18.3501, Test Loss: 41.5784\n",
      "Epoch [46700/50000], Train Loss: 15.7628, Test Loss: 41.5041\n",
      "Epoch [46705/50000], Train Loss: 22.4159, Test Loss: 41.5587\n",
      "Epoch [46710/50000], Train Loss: 17.2969, Test Loss: 41.2378\n",
      "Epoch [46715/50000], Train Loss: 16.8097, Test Loss: 47.2704\n",
      "Epoch [46720/50000], Train Loss: 21.6202, Test Loss: 42.0520\n",
      "Epoch [46725/50000], Train Loss: 19.5381, Test Loss: 42.3038\n",
      "Epoch [46730/50000], Train Loss: 19.9108, Test Loss: 42.3794\n",
      "Epoch [46735/50000], Train Loss: 24.2824, Test Loss: 42.0453\n",
      "Epoch [46740/50000], Train Loss: 17.7579, Test Loss: 42.9646\n",
      "Epoch [46745/50000], Train Loss: 17.7986, Test Loss: 41.7776\n",
      "Epoch [46750/50000], Train Loss: 21.6094, Test Loss: 43.7230\n",
      "Epoch [46755/50000], Train Loss: 18.6562, Test Loss: 41.2479\n",
      "Epoch [46760/50000], Train Loss: 21.6274, Test Loss: 40.5308\n",
      "Epoch [46765/50000], Train Loss: 19.8966, Test Loss: 43.0144\n",
      "Epoch [46770/50000], Train Loss: 16.4372, Test Loss: 43.6252\n",
      "Epoch [46775/50000], Train Loss: 20.9603, Test Loss: 41.6864\n",
      "Epoch [46780/50000], Train Loss: 17.9565, Test Loss: 42.0094\n",
      "Epoch [46785/50000], Train Loss: 16.6575, Test Loss: 42.8360\n",
      "Epoch [46790/50000], Train Loss: 18.0324, Test Loss: 41.6505\n",
      "Epoch [46795/50000], Train Loss: 18.8478, Test Loss: 40.8335\n",
      "Epoch [46800/50000], Train Loss: 17.0194, Test Loss: 42.4549\n",
      "Epoch [46805/50000], Train Loss: 19.2550, Test Loss: 41.8555\n",
      "Epoch [46810/50000], Train Loss: 24.0726, Test Loss: 40.8416\n",
      "Epoch [46815/50000], Train Loss: 20.5881, Test Loss: 45.6260\n",
      "Epoch [46820/50000], Train Loss: 17.9588, Test Loss: 44.0151\n",
      "Epoch [46825/50000], Train Loss: 17.4207, Test Loss: 44.9527\n",
      "Epoch [46830/50000], Train Loss: 18.8509, Test Loss: 41.5704\n",
      "Epoch [46835/50000], Train Loss: 23.5837, Test Loss: 43.0543\n",
      "Epoch [46840/50000], Train Loss: 24.6727, Test Loss: 42.8663\n",
      "Epoch [46845/50000], Train Loss: 18.1926, Test Loss: 40.6307\n",
      "Epoch [46850/50000], Train Loss: 18.8507, Test Loss: 42.1370\n",
      "Epoch [46855/50000], Train Loss: 14.5562, Test Loss: 42.5241\n",
      "Epoch [46860/50000], Train Loss: 21.8315, Test Loss: 43.0173\n",
      "Epoch [46865/50000], Train Loss: 20.2505, Test Loss: 41.8416\n",
      "Epoch [46870/50000], Train Loss: 16.4446, Test Loss: 40.6992\n",
      "Epoch [46875/50000], Train Loss: 14.0090, Test Loss: 43.2962\n",
      "Epoch [46880/50000], Train Loss: 21.3940, Test Loss: 41.6118\n",
      "Epoch [46885/50000], Train Loss: 18.8997, Test Loss: 40.5425\n",
      "Epoch [46890/50000], Train Loss: 19.1559, Test Loss: 41.4940\n",
      "Epoch [46895/50000], Train Loss: 19.9036, Test Loss: 42.7523\n",
      "Epoch [46900/50000], Train Loss: 18.1551, Test Loss: 42.2874\n",
      "Epoch [46905/50000], Train Loss: 19.0140, Test Loss: 47.8478\n",
      "Epoch [46910/50000], Train Loss: 17.1735, Test Loss: 45.0641\n",
      "Epoch [46915/50000], Train Loss: 22.2750, Test Loss: 41.5375\n",
      "Epoch [46920/50000], Train Loss: 19.8381, Test Loss: 42.6594\n",
      "Epoch [46925/50000], Train Loss: 20.5212, Test Loss: 43.8925\n",
      "Epoch [46930/50000], Train Loss: 19.3980, Test Loss: 40.9579\n",
      "Epoch [46935/50000], Train Loss: 16.5007, Test Loss: 41.0322\n",
      "Epoch [46940/50000], Train Loss: 17.8941, Test Loss: 41.4554\n",
      "Epoch [46945/50000], Train Loss: 20.0451, Test Loss: 51.2763\n",
      "Epoch [46950/50000], Train Loss: 19.0890, Test Loss: 41.2008\n",
      "Epoch [46955/50000], Train Loss: 21.9183, Test Loss: 43.9746\n",
      "Epoch [46960/50000], Train Loss: 28.3406, Test Loss: 41.2571\n",
      "Epoch [46965/50000], Train Loss: 19.5756, Test Loss: 43.1604\n",
      "Epoch [46970/50000], Train Loss: 20.1594, Test Loss: 40.4268\n",
      "Epoch [46975/50000], Train Loss: 20.4217, Test Loss: 44.0616\n",
      "Epoch [46980/50000], Train Loss: 17.9406, Test Loss: 42.2350\n",
      "Epoch [46985/50000], Train Loss: 17.9878, Test Loss: 41.2729\n",
      "Epoch [46990/50000], Train Loss: 16.9620, Test Loss: 41.1759\n",
      "Epoch [46995/50000], Train Loss: 21.7825, Test Loss: 43.3224\n",
      "Epoch [47000/50000], Train Loss: 21.2315, Test Loss: 41.7999\n",
      "Epoch [47005/50000], Train Loss: 18.5707, Test Loss: 40.7370\n",
      "Epoch [47010/50000], Train Loss: 18.6495, Test Loss: 43.0341\n",
      "Epoch [47015/50000], Train Loss: 27.4354, Test Loss: 40.8740\n",
      "Epoch [47020/50000], Train Loss: 18.3259, Test Loss: 41.3476\n",
      "Epoch [47025/50000], Train Loss: 21.6349, Test Loss: 40.7875\n",
      "Epoch [47030/50000], Train Loss: 19.5733, Test Loss: 45.1347\n",
      "Epoch [47035/50000], Train Loss: 19.5129, Test Loss: 42.2491\n",
      "Epoch [47040/50000], Train Loss: 19.7976, Test Loss: 48.5201\n",
      "Epoch [47045/50000], Train Loss: 19.1678, Test Loss: 42.9796\n",
      "Epoch [47050/50000], Train Loss: 18.8113, Test Loss: 46.0657\n",
      "Epoch [47055/50000], Train Loss: 16.6014, Test Loss: 42.2570\n",
      "Epoch [47060/50000], Train Loss: 18.4009, Test Loss: 40.4173\n",
      "Epoch [47065/50000], Train Loss: 18.8084, Test Loss: 41.1469\n",
      "Epoch [47070/50000], Train Loss: 18.3306, Test Loss: 41.2862\n",
      "Epoch [47075/50000], Train Loss: 17.8465, Test Loss: 41.6379\n",
      "Epoch [47080/50000], Train Loss: 21.2787, Test Loss: 42.5884\n",
      "Epoch [47085/50000], Train Loss: 20.8270, Test Loss: 42.4198\n",
      "Epoch [47090/50000], Train Loss: 102.6547, Test Loss: 42.4596\n",
      "Epoch [47095/50000], Train Loss: 17.7850, Test Loss: 43.8809\n",
      "Epoch [47100/50000], Train Loss: 15.4721, Test Loss: 41.6037\n",
      "Epoch [47105/50000], Train Loss: 18.6697, Test Loss: 43.5010\n",
      "Epoch [47110/50000], Train Loss: 21.0899, Test Loss: 41.5564\n",
      "Epoch [47115/50000], Train Loss: 18.5994, Test Loss: 42.5190\n",
      "Epoch [47120/50000], Train Loss: 21.8366, Test Loss: 41.6628\n",
      "Epoch [47125/50000], Train Loss: 17.8027, Test Loss: 43.4034\n",
      "Epoch [47130/50000], Train Loss: 23.2971, Test Loss: 41.9850\n",
      "Epoch [47135/50000], Train Loss: 18.7286, Test Loss: 41.7208\n",
      "Epoch [47140/50000], Train Loss: 21.0620, Test Loss: 46.0702\n",
      "Epoch [47145/50000], Train Loss: 18.9002, Test Loss: 43.1854\n",
      "Epoch [47150/50000], Train Loss: 18.7569, Test Loss: 41.3488\n",
      "Epoch [47155/50000], Train Loss: 18.0342, Test Loss: 44.4634\n",
      "Epoch [47160/50000], Train Loss: 21.5835, Test Loss: 41.4403\n",
      "Epoch [47165/50000], Train Loss: 25.9768, Test Loss: 40.7777\n",
      "Epoch [47170/50000], Train Loss: 15.3935, Test Loss: 42.1553\n",
      "Epoch [47175/50000], Train Loss: 17.3043, Test Loss: 42.9337\n",
      "Epoch [47180/50000], Train Loss: 21.5695, Test Loss: 41.2757\n",
      "Epoch [47185/50000], Train Loss: 23.1374, Test Loss: 40.4682\n",
      "Epoch [47190/50000], Train Loss: 15.0409, Test Loss: 41.9286\n",
      "Epoch [47195/50000], Train Loss: 21.1590, Test Loss: 50.3768\n",
      "Epoch [47200/50000], Train Loss: 19.3656, Test Loss: 41.2098\n",
      "Epoch [47205/50000], Train Loss: 21.2608, Test Loss: 45.9104\n",
      "Epoch [47210/50000], Train Loss: 17.3685, Test Loss: 43.3281\n",
      "Epoch [47215/50000], Train Loss: 13.2574, Test Loss: 43.0254\n",
      "Epoch [47220/50000], Train Loss: 18.3159, Test Loss: 40.3970\n",
      "Epoch [47225/50000], Train Loss: 19.0458, Test Loss: 43.9349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47230/50000], Train Loss: 19.2310, Test Loss: 40.6748\n",
      "Epoch [47235/50000], Train Loss: 19.3231, Test Loss: 42.7470\n",
      "Epoch [47240/50000], Train Loss: 15.9906, Test Loss: 42.1673\n",
      "Epoch [47245/50000], Train Loss: 20.8340, Test Loss: 41.6055\n",
      "Epoch [47250/50000], Train Loss: 17.2898, Test Loss: 42.4493\n",
      "Epoch [47255/50000], Train Loss: 19.3715, Test Loss: 40.8963\n",
      "Epoch [47260/50000], Train Loss: 17.8172, Test Loss: 40.4733\n",
      "Epoch [47265/50000], Train Loss: 19.9055, Test Loss: 43.4286\n",
      "Epoch [47270/50000], Train Loss: 17.0266, Test Loss: 41.4889\n",
      "Epoch [47275/50000], Train Loss: 18.0978, Test Loss: 42.2765\n",
      "Epoch [47280/50000], Train Loss: 18.4964, Test Loss: 42.3943\n",
      "Epoch [47285/50000], Train Loss: 16.7810, Test Loss: 41.0402\n",
      "Epoch [47290/50000], Train Loss: 19.4277, Test Loss: 44.3043\n",
      "Epoch [47295/50000], Train Loss: 18.8770, Test Loss: 41.4824\n",
      "Epoch [47300/50000], Train Loss: 22.0793, Test Loss: 42.3698\n",
      "Epoch [47305/50000], Train Loss: 23.0350, Test Loss: 48.9167\n",
      "Epoch [47310/50000], Train Loss: 28.3520, Test Loss: 40.7854\n",
      "Epoch [47315/50000], Train Loss: 18.0263, Test Loss: 40.6266\n",
      "Epoch [47320/50000], Train Loss: 27.6783, Test Loss: 41.2806\n",
      "Epoch [47325/50000], Train Loss: 21.4671, Test Loss: 42.5567\n",
      "Epoch [47330/50000], Train Loss: 20.8590, Test Loss: 42.6683\n",
      "Epoch [47335/50000], Train Loss: 16.2061, Test Loss: 40.5267\n",
      "Epoch [47340/50000], Train Loss: 20.7861, Test Loss: 43.1493\n",
      "Epoch [47345/50000], Train Loss: 19.2740, Test Loss: 46.4109\n",
      "Epoch [47350/50000], Train Loss: 16.6763, Test Loss: 41.4530\n",
      "Epoch [47355/50000], Train Loss: 17.6106, Test Loss: 43.8582\n",
      "Epoch [47360/50000], Train Loss: 27.6792, Test Loss: 44.2221\n",
      "Epoch [47365/50000], Train Loss: 18.0470, Test Loss: 40.0268\n",
      "Epoch [47370/50000], Train Loss: 20.9675, Test Loss: 41.7875\n",
      "Epoch [47375/50000], Train Loss: 33.8493, Test Loss: 40.4764\n",
      "Epoch [47380/50000], Train Loss: 19.9667, Test Loss: 41.7633\n",
      "Epoch [47385/50000], Train Loss: 17.3804, Test Loss: 41.3078\n",
      "Epoch [47390/50000], Train Loss: 20.4285, Test Loss: 41.6985\n",
      "Epoch [47395/50000], Train Loss: 20.7919, Test Loss: 41.0781\n",
      "Epoch [47400/50000], Train Loss: 23.6574, Test Loss: 42.4717\n",
      "Epoch [47405/50000], Train Loss: 20.8711, Test Loss: 41.0783\n",
      "Epoch [47410/50000], Train Loss: 17.2288, Test Loss: 40.7819\n",
      "Epoch [47415/50000], Train Loss: 23.7635, Test Loss: 42.8057\n",
      "Epoch [47420/50000], Train Loss: 19.5021, Test Loss: 40.6919\n",
      "Epoch [47425/50000], Train Loss: 18.8329, Test Loss: 42.9318\n",
      "Epoch [47430/50000], Train Loss: 22.3752, Test Loss: 42.0544\n",
      "Epoch [47435/50000], Train Loss: 12.1628, Test Loss: 39.9424\n",
      "Epoch [47440/50000], Train Loss: 17.4472, Test Loss: 41.4333\n",
      "Epoch [47445/50000], Train Loss: 29.7158, Test Loss: 50.1343\n",
      "Epoch [47450/50000], Train Loss: 17.2748, Test Loss: 42.7769\n",
      "Epoch [47455/50000], Train Loss: 14.7522, Test Loss: 40.4427\n",
      "Epoch [47460/50000], Train Loss: 18.8591, Test Loss: 40.5825\n",
      "Epoch [47465/50000], Train Loss: 32.3312, Test Loss: 42.0114\n",
      "Epoch [47470/50000], Train Loss: 19.3962, Test Loss: 48.6458\n",
      "Epoch [47475/50000], Train Loss: 20.8673, Test Loss: 41.3635\n",
      "Epoch [47480/50000], Train Loss: 23.5844, Test Loss: 46.9729\n",
      "Epoch [47485/50000], Train Loss: 19.3021, Test Loss: 42.2294\n",
      "Epoch [47490/50000], Train Loss: 40.9949, Test Loss: 41.2443\n",
      "Epoch [47495/50000], Train Loss: 18.0685, Test Loss: 41.9332\n",
      "Epoch [47500/50000], Train Loss: 19.3335, Test Loss: 41.2179\n",
      "Epoch [47505/50000], Train Loss: 13.7248, Test Loss: 43.6490\n",
      "Epoch [47510/50000], Train Loss: 16.7677, Test Loss: 41.5042\n",
      "Epoch [47515/50000], Train Loss: 19.0841, Test Loss: 42.1570\n",
      "Epoch [47520/50000], Train Loss: 17.9907, Test Loss: 42.9281\n",
      "Epoch [47525/50000], Train Loss: 17.9989, Test Loss: 45.2658\n",
      "Epoch [47530/50000], Train Loss: 18.2609, Test Loss: 42.0876\n",
      "Epoch [47535/50000], Train Loss: 23.1270, Test Loss: 41.2051\n",
      "Epoch [47540/50000], Train Loss: 19.4745, Test Loss: 41.6578\n",
      "Epoch [47545/50000], Train Loss: 19.0907, Test Loss: 42.3495\n",
      "Epoch [47550/50000], Train Loss: 44.6835, Test Loss: 41.2243\n",
      "Epoch [47555/50000], Train Loss: 20.7331, Test Loss: 47.9814\n",
      "Epoch [47560/50000], Train Loss: 21.0260, Test Loss: 41.3555\n",
      "Epoch [47565/50000], Train Loss: 19.3948, Test Loss: 43.0282\n",
      "Epoch [47570/50000], Train Loss: 20.0774, Test Loss: 43.4657\n",
      "Epoch [47575/50000], Train Loss: 16.4126, Test Loss: 41.0828\n",
      "Epoch [47580/50000], Train Loss: 24.9010, Test Loss: 42.0072\n",
      "Epoch [47585/50000], Train Loss: 17.9623, Test Loss: 41.4565\n",
      "Epoch [47590/50000], Train Loss: 17.1352, Test Loss: 46.6108\n",
      "Epoch [47595/50000], Train Loss: 23.5304, Test Loss: 40.6289\n",
      "Epoch [47600/50000], Train Loss: 24.0063, Test Loss: 41.7663\n",
      "Epoch [47605/50000], Train Loss: 26.9480, Test Loss: 44.1100\n",
      "Epoch [47610/50000], Train Loss: 16.7349, Test Loss: 45.6658\n",
      "Epoch [47615/50000], Train Loss: 16.0230, Test Loss: 41.2268\n",
      "Epoch [47620/50000], Train Loss: 19.5338, Test Loss: 42.9103\n",
      "Epoch [47625/50000], Train Loss: 17.9199, Test Loss: 40.9486\n",
      "Epoch [47630/50000], Train Loss: 16.2332, Test Loss: 41.9651\n",
      "Epoch [47635/50000], Train Loss: 21.5923, Test Loss: 43.9610\n",
      "Epoch [47640/50000], Train Loss: 19.9754, Test Loss: 40.7874\n",
      "Epoch [47645/50000], Train Loss: 18.8186, Test Loss: 42.5142\n",
      "Epoch [47650/50000], Train Loss: 21.7238, Test Loss: 44.9439\n",
      "Epoch [47655/50000], Train Loss: 19.8574, Test Loss: 40.8916\n",
      "Epoch [47660/50000], Train Loss: 18.0612, Test Loss: 44.4427\n",
      "Epoch [47665/50000], Train Loss: 24.5813, Test Loss: 44.2374\n",
      "Epoch [47670/50000], Train Loss: 13.9522, Test Loss: 47.0251\n",
      "Epoch [47675/50000], Train Loss: 22.3619, Test Loss: 43.9438\n",
      "Epoch [47680/50000], Train Loss: 21.2576, Test Loss: 42.4948\n",
      "Epoch [47685/50000], Train Loss: 17.4991, Test Loss: 42.2174\n",
      "Epoch [47690/50000], Train Loss: 20.0186, Test Loss: 45.6122\n",
      "Epoch [47695/50000], Train Loss: 18.9673, Test Loss: 41.6401\n",
      "Epoch [47700/50000], Train Loss: 14.0397, Test Loss: 41.5075\n",
      "Epoch [47705/50000], Train Loss: 16.9215, Test Loss: 41.7783\n",
      "Epoch [47710/50000], Train Loss: 21.8707, Test Loss: 42.0517\n",
      "Epoch [47715/50000], Train Loss: 20.4129, Test Loss: 43.5677\n",
      "Epoch [47720/50000], Train Loss: 18.9383, Test Loss: 40.9186\n",
      "Epoch [47725/50000], Train Loss: 15.6564, Test Loss: 41.1792\n",
      "Epoch [47730/50000], Train Loss: 16.5107, Test Loss: 42.3813\n",
      "Epoch [47735/50000], Train Loss: 22.0219, Test Loss: 41.7663\n",
      "Epoch [47740/50000], Train Loss: 20.1982, Test Loss: 46.4350\n",
      "Epoch [47745/50000], Train Loss: 14.9849, Test Loss: 40.5113\n",
      "Epoch [47750/50000], Train Loss: 21.6933, Test Loss: 41.8399\n",
      "Epoch [47755/50000], Train Loss: 17.3145, Test Loss: 41.7263\n",
      "Epoch [47760/50000], Train Loss: 17.8899, Test Loss: 41.6858\n",
      "Epoch [47765/50000], Train Loss: 18.4189, Test Loss: 41.8630\n",
      "Epoch [47770/50000], Train Loss: 19.7676, Test Loss: 41.2361\n",
      "Epoch [47775/50000], Train Loss: 21.4398, Test Loss: 44.3083\n",
      "Epoch [47780/50000], Train Loss: 16.5785, Test Loss: 40.9588\n",
      "Epoch [47785/50000], Train Loss: 19.6505, Test Loss: 42.8471\n",
      "Epoch [47790/50000], Train Loss: 18.6847, Test Loss: 41.3564\n",
      "Epoch [47795/50000], Train Loss: 17.0048, Test Loss: 41.2553\n",
      "Epoch [47800/50000], Train Loss: 15.4007, Test Loss: 42.0817\n",
      "Epoch [47805/50000], Train Loss: 22.6874, Test Loss: 40.7751\n",
      "Epoch [47810/50000], Train Loss: 22.5050, Test Loss: 41.3759\n",
      "Epoch [47815/50000], Train Loss: 19.4580, Test Loss: 42.6142\n",
      "Epoch [47820/50000], Train Loss: 35.9369, Test Loss: 40.1994\n",
      "Epoch [47825/50000], Train Loss: 19.1686, Test Loss: 45.6920\n",
      "Epoch [47830/50000], Train Loss: 21.7438, Test Loss: 41.3647\n",
      "Epoch [47835/50000], Train Loss: 18.6047, Test Loss: 43.8567\n",
      "Epoch [47840/50000], Train Loss: 21.5062, Test Loss: 43.9489\n",
      "Epoch [47845/50000], Train Loss: 16.1330, Test Loss: 42.2600\n",
      "Epoch [47850/50000], Train Loss: 15.6672, Test Loss: 40.9902\n",
      "Epoch [47855/50000], Train Loss: 17.0525, Test Loss: 44.4433\n",
      "Epoch [47860/50000], Train Loss: 20.0481, Test Loss: 41.8229\n",
      "Epoch [47865/50000], Train Loss: 23.3707, Test Loss: 42.9508\n",
      "Epoch [47870/50000], Train Loss: 30.3631, Test Loss: 42.4181\n",
      "Epoch [47875/50000], Train Loss: 23.0466, Test Loss: 44.1514\n",
      "Epoch [47880/50000], Train Loss: 18.4955, Test Loss: 48.2975\n",
      "Epoch [47885/50000], Train Loss: 21.0198, Test Loss: 49.1365\n",
      "Epoch [47890/50000], Train Loss: 17.5437, Test Loss: 42.5934\n",
      "Epoch [47895/50000], Train Loss: 21.4193, Test Loss: 40.7977\n",
      "Epoch [47900/50000], Train Loss: 20.3165, Test Loss: 41.2532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47905/50000], Train Loss: 20.3359, Test Loss: 45.2964\n",
      "Epoch [47910/50000], Train Loss: 22.3421, Test Loss: 40.8401\n",
      "Epoch [47915/50000], Train Loss: 24.2162, Test Loss: 46.9052\n",
      "Epoch [47920/50000], Train Loss: 17.7081, Test Loss: 41.8002\n",
      "Epoch [47925/50000], Train Loss: 26.2779, Test Loss: 40.9169\n",
      "Epoch [47930/50000], Train Loss: 20.0779, Test Loss: 41.5490\n",
      "Epoch [47935/50000], Train Loss: 18.2404, Test Loss: 44.6146\n",
      "Epoch [47940/50000], Train Loss: 19.2473, Test Loss: 41.9358\n",
      "Epoch [47945/50000], Train Loss: 15.7506, Test Loss: 42.0002\n",
      "Epoch [47950/50000], Train Loss: 21.3348, Test Loss: 42.5569\n",
      "Epoch [47955/50000], Train Loss: 22.8433, Test Loss: 42.6675\n",
      "Epoch [47960/50000], Train Loss: 31.4586, Test Loss: 40.5131\n",
      "Epoch [47965/50000], Train Loss: 19.2317, Test Loss: 43.2998\n",
      "Epoch [47970/50000], Train Loss: 20.3651, Test Loss: 42.8240\n",
      "Epoch [47975/50000], Train Loss: 20.9980, Test Loss: 40.8598\n",
      "Epoch [47980/50000], Train Loss: 18.2502, Test Loss: 44.8857\n",
      "Epoch [47985/50000], Train Loss: 18.9086, Test Loss: 45.8642\n",
      "Epoch [47990/50000], Train Loss: 21.4296, Test Loss: 41.8691\n",
      "Epoch [47995/50000], Train Loss: 18.8397, Test Loss: 57.4703\n",
      "Epoch [48000/50000], Train Loss: 16.9018, Test Loss: 42.9414\n",
      "Epoch [48005/50000], Train Loss: 15.0811, Test Loss: 40.2631\n",
      "Epoch [48010/50000], Train Loss: 28.4408, Test Loss: 43.9790\n",
      "Epoch [48015/50000], Train Loss: 20.0721, Test Loss: 41.2445\n",
      "Epoch [48020/50000], Train Loss: 18.3122, Test Loss: 40.5913\n",
      "Epoch [48025/50000], Train Loss: 21.4908, Test Loss: 42.3954\n",
      "Epoch [48030/50000], Train Loss: 18.4380, Test Loss: 52.0888\n",
      "Epoch [48035/50000], Train Loss: 19.5341, Test Loss: 42.7385\n",
      "Epoch [48040/50000], Train Loss: 19.4883, Test Loss: 42.4123\n",
      "Epoch [48045/50000], Train Loss: 20.6118, Test Loss: 40.7186\n",
      "Epoch [48050/50000], Train Loss: 19.9534, Test Loss: 43.0549\n",
      "Epoch [48055/50000], Train Loss: 16.9485, Test Loss: 44.0246\n",
      "Epoch [48060/50000], Train Loss: 22.1896, Test Loss: 40.5069\n",
      "Epoch [48065/50000], Train Loss: 21.3883, Test Loss: 42.2583\n",
      "Epoch [48070/50000], Train Loss: 16.6740, Test Loss: 41.2490\n",
      "Epoch [48075/50000], Train Loss: 17.4197, Test Loss: 42.2613\n",
      "Epoch [48080/50000], Train Loss: 21.3208, Test Loss: 43.7206\n",
      "Epoch [48085/50000], Train Loss: 16.8894, Test Loss: 45.1132\n",
      "Epoch [48090/50000], Train Loss: 27.1821, Test Loss: 41.9765\n",
      "Epoch [48095/50000], Train Loss: 18.6894, Test Loss: 43.7675\n",
      "Epoch [48100/50000], Train Loss: 16.7798, Test Loss: 41.9940\n",
      "Epoch [48105/50000], Train Loss: 16.1568, Test Loss: 41.8527\n",
      "Epoch [48110/50000], Train Loss: 20.9350, Test Loss: 43.1262\n",
      "Epoch [48115/50000], Train Loss: 16.3755, Test Loss: 42.4207\n",
      "Epoch [48120/50000], Train Loss: 18.3220, Test Loss: 44.3906\n",
      "Epoch [48125/50000], Train Loss: 19.6941, Test Loss: 42.0112\n",
      "Epoch [48130/50000], Train Loss: 20.8777, Test Loss: 46.5420\n",
      "Epoch [48135/50000], Train Loss: 20.3085, Test Loss: 43.2731\n",
      "Epoch [48140/50000], Train Loss: 17.3092, Test Loss: 40.4858\n",
      "Epoch [48145/50000], Train Loss: 19.4659, Test Loss: 40.5821\n",
      "Epoch [48150/50000], Train Loss: 17.5946, Test Loss: 42.3631\n",
      "Epoch [48155/50000], Train Loss: 29.9000, Test Loss: 42.6233\n",
      "Epoch [48160/50000], Train Loss: 21.0054, Test Loss: 42.3732\n",
      "Epoch [48165/50000], Train Loss: 18.1083, Test Loss: 44.0178\n",
      "Epoch [48170/50000], Train Loss: 19.2082, Test Loss: 40.8092\n",
      "Epoch [48175/50000], Train Loss: 18.9646, Test Loss: 53.4592\n",
      "Epoch [48180/50000], Train Loss: 16.3967, Test Loss: 40.9006\n",
      "Epoch [48185/50000], Train Loss: 14.0541, Test Loss: 40.8957\n",
      "Epoch [48190/50000], Train Loss: 29.3194, Test Loss: 44.1351\n",
      "Epoch [48195/50000], Train Loss: 14.1181, Test Loss: 40.5589\n",
      "Epoch [48200/50000], Train Loss: 16.7002, Test Loss: 41.6898\n",
      "Epoch [48205/50000], Train Loss: 18.8786, Test Loss: 42.3259\n",
      "Epoch [48210/50000], Train Loss: 16.7367, Test Loss: 40.8737\n",
      "Epoch [48215/50000], Train Loss: 15.7457, Test Loss: 40.9545\n",
      "Epoch [48220/50000], Train Loss: 17.4668, Test Loss: 42.1329\n",
      "Epoch [48225/50000], Train Loss: 20.7688, Test Loss: 42.3025\n",
      "Epoch [48230/50000], Train Loss: 22.8070, Test Loss: 41.7433\n",
      "Epoch [48235/50000], Train Loss: 19.8373, Test Loss: 43.9724\n",
      "Epoch [48240/50000], Train Loss: 18.5078, Test Loss: 44.1091\n",
      "Epoch [48245/50000], Train Loss: 21.3905, Test Loss: 42.8109\n",
      "Epoch [48250/50000], Train Loss: 17.4084, Test Loss: 40.8229\n",
      "Epoch [48255/50000], Train Loss: 23.4414, Test Loss: 42.6319\n",
      "Epoch [48260/50000], Train Loss: 37.5655, Test Loss: 58.4674\n",
      "Epoch [48265/50000], Train Loss: 49.1478, Test Loss: 40.2040\n",
      "Epoch [48270/50000], Train Loss: 16.4284, Test Loss: 42.3182\n",
      "Epoch [48275/50000], Train Loss: 17.3819, Test Loss: 41.6600\n",
      "Epoch [48280/50000], Train Loss: 19.4539, Test Loss: 42.1567\n",
      "Epoch [48285/50000], Train Loss: 19.9957, Test Loss: 40.5175\n",
      "Epoch [48290/50000], Train Loss: 21.8184, Test Loss: 45.9898\n",
      "Epoch [48295/50000], Train Loss: 18.0538, Test Loss: 43.0419\n",
      "Epoch [48300/50000], Train Loss: 17.5635, Test Loss: 43.4306\n",
      "Epoch [48305/50000], Train Loss: 18.3822, Test Loss: 43.5492\n",
      "Epoch [48310/50000], Train Loss: 23.6029, Test Loss: 48.0374\n",
      "Epoch [48315/50000], Train Loss: 20.3217, Test Loss: 47.5532\n",
      "Epoch [48320/50000], Train Loss: 16.5025, Test Loss: 41.4066\n",
      "Epoch [48325/50000], Train Loss: 15.0232, Test Loss: 47.8276\n",
      "Epoch [48330/50000], Train Loss: 19.9013, Test Loss: 41.9326\n",
      "Epoch [48335/50000], Train Loss: 20.0545, Test Loss: 41.6983\n",
      "Epoch [48340/50000], Train Loss: 18.8843, Test Loss: 41.7796\n",
      "Epoch [48345/50000], Train Loss: 18.4521, Test Loss: 43.4850\n",
      "Epoch [48350/50000], Train Loss: 18.8338, Test Loss: 43.6730\n",
      "Epoch [48355/50000], Train Loss: 19.6662, Test Loss: 41.4699\n",
      "Epoch [48360/50000], Train Loss: 19.1076, Test Loss: 42.3689\n",
      "Epoch [48365/50000], Train Loss: 16.6405, Test Loss: 41.0717\n",
      "Epoch [48370/50000], Train Loss: 16.8087, Test Loss: 41.2775\n",
      "Epoch [48375/50000], Train Loss: 19.8859, Test Loss: 40.6604\n",
      "Epoch [48380/50000], Train Loss: 18.2988, Test Loss: 49.7020\n",
      "Epoch [48385/50000], Train Loss: 16.8778, Test Loss: 41.8439\n",
      "Epoch [48390/50000], Train Loss: 23.2349, Test Loss: 42.8432\n",
      "Epoch [48395/50000], Train Loss: 33.0552, Test Loss: 40.6855\n",
      "Epoch [48400/50000], Train Loss: 18.4635, Test Loss: 41.4682\n",
      "Epoch [48405/50000], Train Loss: 17.6552, Test Loss: 44.9345\n",
      "Epoch [48410/50000], Train Loss: 27.4369, Test Loss: 40.5624\n",
      "Epoch [48415/50000], Train Loss: 23.4049, Test Loss: 44.1710\n",
      "Epoch [48420/50000], Train Loss: 18.5882, Test Loss: 40.6800\n",
      "Epoch [48425/50000], Train Loss: 19.0216, Test Loss: 42.4717\n",
      "Epoch [48430/50000], Train Loss: 19.0849, Test Loss: 42.8635\n",
      "Epoch [48435/50000], Train Loss: 17.3225, Test Loss: 41.1012\n",
      "Epoch [48440/50000], Train Loss: 19.0939, Test Loss: 41.7278\n",
      "Epoch [48445/50000], Train Loss: 15.2308, Test Loss: 42.8850\n",
      "Epoch [48450/50000], Train Loss: 17.0719, Test Loss: 41.7509\n",
      "Epoch [48455/50000], Train Loss: 17.9087, Test Loss: 43.6838\n",
      "Epoch [48460/50000], Train Loss: 18.0536, Test Loss: 42.0780\n",
      "Epoch [48465/50000], Train Loss: 17.4917, Test Loss: 42.7895\n",
      "Epoch [48470/50000], Train Loss: 17.9768, Test Loss: 43.2032\n",
      "Epoch [48475/50000], Train Loss: 18.2386, Test Loss: 41.6680\n",
      "Epoch [48480/50000], Train Loss: 13.0521, Test Loss: 42.4903\n",
      "Epoch [48485/50000], Train Loss: 22.8478, Test Loss: 41.1828\n",
      "Epoch [48490/50000], Train Loss: 19.7582, Test Loss: 42.5751\n",
      "Epoch [48495/50000], Train Loss: 17.7582, Test Loss: 42.2287\n",
      "Epoch [48500/50000], Train Loss: 20.4737, Test Loss: 40.2440\n",
      "Epoch [48505/50000], Train Loss: 16.5858, Test Loss: 43.1273\n",
      "Epoch [48510/50000], Train Loss: 24.8172, Test Loss: 41.8470\n",
      "Epoch [48515/50000], Train Loss: 16.3297, Test Loss: 41.1485\n",
      "Epoch [48520/50000], Train Loss: 13.4859, Test Loss: 41.5860\n",
      "Epoch [48525/50000], Train Loss: 19.5537, Test Loss: 41.8145\n",
      "Epoch [48530/50000], Train Loss: 20.7061, Test Loss: 45.0710\n",
      "Epoch [48535/50000], Train Loss: 13.5748, Test Loss: 41.3893\n",
      "Epoch [48540/50000], Train Loss: 15.9243, Test Loss: 41.0979\n",
      "Epoch [48545/50000], Train Loss: 18.2883, Test Loss: 40.4533\n",
      "Epoch [48550/50000], Train Loss: 19.1734, Test Loss: 41.1915\n",
      "Epoch [48555/50000], Train Loss: 24.1912, Test Loss: 43.5489\n",
      "Epoch [48560/50000], Train Loss: 17.2165, Test Loss: 41.2314\n",
      "Epoch [48565/50000], Train Loss: 17.5845, Test Loss: 42.0669\n",
      "Epoch [48570/50000], Train Loss: 18.9967, Test Loss: 41.1941\n",
      "Epoch [48575/50000], Train Loss: 21.6236, Test Loss: 44.2560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48580/50000], Train Loss: 17.4856, Test Loss: 47.2530\n",
      "Epoch [48585/50000], Train Loss: 19.4686, Test Loss: 48.6642\n",
      "Epoch [48590/50000], Train Loss: 21.1571, Test Loss: 44.2304\n",
      "Epoch [48595/50000], Train Loss: 17.3638, Test Loss: 42.0544\n",
      "Epoch [48600/50000], Train Loss: 18.7614, Test Loss: 41.0615\n",
      "Epoch [48605/50000], Train Loss: 18.1159, Test Loss: 44.9975\n",
      "Epoch [48610/50000], Train Loss: 16.2917, Test Loss: 45.2839\n",
      "Epoch [48615/50000], Train Loss: 21.7228, Test Loss: 40.4770\n",
      "Epoch [48620/50000], Train Loss: 21.8032, Test Loss: 40.7516\n",
      "Epoch [48625/50000], Train Loss: 18.3509, Test Loss: 41.0687\n",
      "Epoch [48630/50000], Train Loss: 18.2220, Test Loss: 44.7115\n",
      "Epoch [48635/50000], Train Loss: 18.1894, Test Loss: 43.5376\n",
      "Epoch [48640/50000], Train Loss: 20.7795, Test Loss: 42.8381\n",
      "Epoch [48645/50000], Train Loss: 15.4903, Test Loss: 42.5460\n",
      "Epoch [48650/50000], Train Loss: 18.9593, Test Loss: 42.1023\n",
      "Epoch [48655/50000], Train Loss: 19.9289, Test Loss: 41.4520\n",
      "Epoch [48660/50000], Train Loss: 12.8486, Test Loss: 41.6563\n",
      "Epoch [48665/50000], Train Loss: 19.0645, Test Loss: 43.3442\n",
      "Epoch [48670/50000], Train Loss: 18.1147, Test Loss: 40.9927\n",
      "Epoch [48675/50000], Train Loss: 19.0568, Test Loss: 45.7606\n",
      "Epoch [48680/50000], Train Loss: 18.7965, Test Loss: 41.9522\n",
      "Epoch [48685/50000], Train Loss: 17.9297, Test Loss: 42.8700\n",
      "Epoch [48690/50000], Train Loss: 19.8736, Test Loss: 42.3715\n",
      "Epoch [48695/50000], Train Loss: 35.5440, Test Loss: 41.2638\n",
      "Epoch [48700/50000], Train Loss: 19.4900, Test Loss: 42.4335\n",
      "Epoch [48705/50000], Train Loss: 23.5678, Test Loss: 48.3407\n",
      "Epoch [48710/50000], Train Loss: 17.9116, Test Loss: 42.1287\n",
      "Epoch [48715/50000], Train Loss: 18.6595, Test Loss: 42.7488\n",
      "Epoch [48720/50000], Train Loss: 21.3663, Test Loss: 41.2083\n",
      "Epoch [48725/50000], Train Loss: 21.3218, Test Loss: 43.6892\n",
      "Epoch [48730/50000], Train Loss: 19.6129, Test Loss: 42.1895\n",
      "Epoch [48735/50000], Train Loss: 24.7378, Test Loss: 45.1737\n",
      "Epoch [48740/50000], Train Loss: 18.0903, Test Loss: 41.7033\n",
      "Epoch [48745/50000], Train Loss: 22.3956, Test Loss: 42.8831\n",
      "Epoch [48750/50000], Train Loss: 17.1438, Test Loss: 42.7289\n",
      "Epoch [48755/50000], Train Loss: 18.8701, Test Loss: 42.5410\n",
      "Epoch [48760/50000], Train Loss: 18.8160, Test Loss: 41.5517\n",
      "Epoch [48765/50000], Train Loss: 20.9851, Test Loss: 44.2014\n",
      "Epoch [48770/50000], Train Loss: 21.6522, Test Loss: 42.3482\n",
      "Epoch [48775/50000], Train Loss: 18.6317, Test Loss: 42.3804\n",
      "Epoch [48780/50000], Train Loss: 19.0327, Test Loss: 41.7167\n",
      "Epoch [48785/50000], Train Loss: 19.3240, Test Loss: 47.7767\n",
      "Epoch [48790/50000], Train Loss: 19.3363, Test Loss: 43.7620\n",
      "Epoch [48795/50000], Train Loss: 22.5866, Test Loss: 42.3735\n",
      "Epoch [48800/50000], Train Loss: 20.2768, Test Loss: 44.8193\n",
      "Epoch [48805/50000], Train Loss: 22.5979, Test Loss: 43.0036\n",
      "Epoch [48810/50000], Train Loss: 22.1887, Test Loss: 41.1137\n",
      "Epoch [48815/50000], Train Loss: 18.3274, Test Loss: 43.8504\n",
      "Epoch [48820/50000], Train Loss: 23.3634, Test Loss: 41.2303\n",
      "Epoch [48825/50000], Train Loss: 19.4022, Test Loss: 40.7190\n",
      "Epoch [48830/50000], Train Loss: 17.8845, Test Loss: 41.2566\n",
      "Epoch [48835/50000], Train Loss: 11.8992, Test Loss: 40.5522\n",
      "Epoch [48840/50000], Train Loss: 17.6092, Test Loss: 41.3483\n",
      "Epoch [48845/50000], Train Loss: 19.8786, Test Loss: 44.7816\n",
      "Epoch [48850/50000], Train Loss: 17.8520, Test Loss: 46.7970\n",
      "Epoch [48855/50000], Train Loss: 20.9396, Test Loss: 45.4365\n",
      "Epoch [48860/50000], Train Loss: 15.1388, Test Loss: 42.5275\n",
      "Epoch [48865/50000], Train Loss: 16.4440, Test Loss: 42.7410\n",
      "Epoch [48870/50000], Train Loss: 18.8608, Test Loss: 41.4637\n",
      "Epoch [48875/50000], Train Loss: 20.8658, Test Loss: 41.7407\n",
      "Epoch [48880/50000], Train Loss: 20.6880, Test Loss: 42.8195\n",
      "Epoch [48885/50000], Train Loss: 15.7212, Test Loss: 42.6116\n",
      "Epoch [48890/50000], Train Loss: 20.3326, Test Loss: 42.4609\n",
      "Epoch [48895/50000], Train Loss: 20.1025, Test Loss: 46.2706\n",
      "Epoch [48900/50000], Train Loss: 15.3259, Test Loss: 43.1909\n",
      "Epoch [48905/50000], Train Loss: 17.1411, Test Loss: 48.1316\n",
      "Epoch [48910/50000], Train Loss: 18.2385, Test Loss: 40.0880\n",
      "Epoch [48915/50000], Train Loss: 13.9501, Test Loss: 41.2355\n",
      "Epoch [48920/50000], Train Loss: 20.1936, Test Loss: 43.4052\n",
      "Epoch [48925/50000], Train Loss: 18.0336, Test Loss: 42.0942\n",
      "Epoch [48930/50000], Train Loss: 26.3376, Test Loss: 42.1694\n",
      "Epoch [48935/50000], Train Loss: 17.8053, Test Loss: 42.1198\n",
      "Epoch [48940/50000], Train Loss: 17.8428, Test Loss: 40.8116\n",
      "Epoch [48945/50000], Train Loss: 18.1511, Test Loss: 41.1475\n",
      "Epoch [48950/50000], Train Loss: 18.3030, Test Loss: 42.3177\n",
      "Epoch [48955/50000], Train Loss: 17.4547, Test Loss: 39.9314\n",
      "Epoch [48960/50000], Train Loss: 20.6869, Test Loss: 42.3281\n",
      "Epoch [48965/50000], Train Loss: 20.8279, Test Loss: 40.9639\n",
      "Epoch [48970/50000], Train Loss: 21.0250, Test Loss: 41.9106\n",
      "Epoch [48975/50000], Train Loss: 17.8514, Test Loss: 40.3705\n",
      "Epoch [48980/50000], Train Loss: 26.1613, Test Loss: 40.9100\n",
      "Epoch [48985/50000], Train Loss: 22.0014, Test Loss: 41.5324\n",
      "Epoch [48990/50000], Train Loss: 16.0913, Test Loss: 42.0987\n",
      "Epoch [48995/50000], Train Loss: 27.1076, Test Loss: 41.5963\n",
      "Epoch [49000/50000], Train Loss: 30.3373, Test Loss: 40.2254\n",
      "Epoch [49005/50000], Train Loss: 16.2184, Test Loss: 44.1035\n",
      "Epoch [49010/50000], Train Loss: 15.9467, Test Loss: 41.9564\n",
      "Epoch [49015/50000], Train Loss: 20.3392, Test Loss: 40.7478\n",
      "Epoch [49020/50000], Train Loss: 18.7521, Test Loss: 48.9601\n",
      "Epoch [49025/50000], Train Loss: 16.9677, Test Loss: 40.9586\n",
      "Epoch [49030/50000], Train Loss: 24.2088, Test Loss: 42.6327\n",
      "Epoch [49035/50000], Train Loss: 18.4769, Test Loss: 40.7205\n",
      "Epoch [49040/50000], Train Loss: 18.2751, Test Loss: 41.9054\n",
      "Epoch [49045/50000], Train Loss: 17.4388, Test Loss: 41.9539\n",
      "Epoch [49050/50000], Train Loss: 18.1085, Test Loss: 46.5649\n",
      "Epoch [49055/50000], Train Loss: 23.3994, Test Loss: 42.6634\n",
      "Epoch [49060/50000], Train Loss: 18.1611, Test Loss: 42.2410\n",
      "Epoch [49065/50000], Train Loss: 15.3350, Test Loss: 40.0386\n",
      "Epoch [49070/50000], Train Loss: 19.5164, Test Loss: 41.7745\n",
      "Epoch [49075/50000], Train Loss: 19.8925, Test Loss: 41.6255\n",
      "Epoch [49080/50000], Train Loss: 18.3240, Test Loss: 41.6490\n",
      "Epoch [49085/50000], Train Loss: 16.9044, Test Loss: 42.0939\n",
      "Epoch [49090/50000], Train Loss: 26.4656, Test Loss: 44.1645\n",
      "Epoch [49095/50000], Train Loss: 18.5662, Test Loss: 44.1326\n",
      "Epoch [49100/50000], Train Loss: 16.8266, Test Loss: 41.4133\n",
      "Epoch [49105/50000], Train Loss: 20.7494, Test Loss: 41.2061\n",
      "Epoch [49110/50000], Train Loss: 20.8138, Test Loss: 45.6413\n",
      "Epoch [49115/50000], Train Loss: 15.4934, Test Loss: 41.0607\n",
      "Epoch [49120/50000], Train Loss: 15.6531, Test Loss: 40.9144\n",
      "Epoch [49125/50000], Train Loss: 19.8840, Test Loss: 42.3303\n",
      "Epoch [49130/50000], Train Loss: 19.1697, Test Loss: 42.6491\n",
      "Epoch [49135/50000], Train Loss: 24.8394, Test Loss: 41.1956\n",
      "Epoch [49140/50000], Train Loss: 17.8207, Test Loss: 40.7811\n",
      "Epoch [49145/50000], Train Loss: 34.0882, Test Loss: 40.4359\n",
      "Epoch [49150/50000], Train Loss: 19.9017, Test Loss: 40.0191\n",
      "Epoch [49155/50000], Train Loss: 21.3983, Test Loss: 44.2853\n",
      "Epoch [49160/50000], Train Loss: 23.7869, Test Loss: 49.8064\n",
      "Epoch [49165/50000], Train Loss: 17.0788, Test Loss: 44.1965\n",
      "Epoch [49170/50000], Train Loss: 19.2964, Test Loss: 41.5875\n",
      "Epoch [49175/50000], Train Loss: 27.2051, Test Loss: 46.2539\n",
      "Epoch [49180/50000], Train Loss: 28.5081, Test Loss: 40.0600\n",
      "Epoch [49185/50000], Train Loss: 18.8276, Test Loss: 41.1609\n",
      "Epoch [49190/50000], Train Loss: 19.1334, Test Loss: 45.1636\n",
      "Epoch [49195/50000], Train Loss: 16.9328, Test Loss: 41.7094\n",
      "Epoch [49200/50000], Train Loss: 21.8594, Test Loss: 41.1905\n",
      "Epoch [49205/50000], Train Loss: 17.7342, Test Loss: 44.0248\n",
      "Epoch [49210/50000], Train Loss: 16.1406, Test Loss: 43.8197\n",
      "Epoch [49215/50000], Train Loss: 21.0778, Test Loss: 40.9076\n",
      "Epoch [49220/50000], Train Loss: 16.2925, Test Loss: 41.0449\n",
      "Epoch [49225/50000], Train Loss: 34.7139, Test Loss: 42.4193\n",
      "Epoch [49230/50000], Train Loss: 17.1924, Test Loss: 42.5834\n",
      "Epoch [49235/50000], Train Loss: 17.2080, Test Loss: 41.3892\n",
      "Epoch [49240/50000], Train Loss: 18.6496, Test Loss: 43.4367\n",
      "Epoch [49245/50000], Train Loss: 17.6748, Test Loss: 42.8516\n",
      "Epoch [49250/50000], Train Loss: 19.4518, Test Loss: 40.6043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49255/50000], Train Loss: 13.7861, Test Loss: 41.0647\n",
      "Epoch [49260/50000], Train Loss: 19.3491, Test Loss: 46.5149\n",
      "Epoch [49265/50000], Train Loss: 17.7408, Test Loss: 42.3455\n",
      "Epoch [49270/50000], Train Loss: 17.7954, Test Loss: 42.7896\n",
      "Epoch [49275/50000], Train Loss: 33.5586, Test Loss: 43.3880\n",
      "Epoch [49280/50000], Train Loss: 16.1022, Test Loss: 41.4323\n",
      "Epoch [49285/50000], Train Loss: 17.8386, Test Loss: 44.7684\n",
      "Epoch [49290/50000], Train Loss: 20.0717, Test Loss: 43.2008\n",
      "Epoch [49295/50000], Train Loss: 18.0949, Test Loss: 49.5682\n",
      "Epoch [49300/50000], Train Loss: 18.8405, Test Loss: 41.8551\n",
      "Epoch [49305/50000], Train Loss: 19.4681, Test Loss: 41.1853\n",
      "Epoch [49310/50000], Train Loss: 19.6466, Test Loss: 42.0952\n",
      "Epoch [49315/50000], Train Loss: 17.6717, Test Loss: 41.9397\n",
      "Epoch [49320/50000], Train Loss: 19.0243, Test Loss: 43.9154\n",
      "Epoch [49325/50000], Train Loss: 20.0190, Test Loss: 40.9690\n",
      "Epoch [49330/50000], Train Loss: 25.0432, Test Loss: 41.7350\n",
      "Epoch [49335/50000], Train Loss: 16.8808, Test Loss: 40.8064\n",
      "Epoch [49340/50000], Train Loss: 17.8309, Test Loss: 45.2018\n",
      "Epoch [49345/50000], Train Loss: 20.5155, Test Loss: 41.4201\n",
      "Epoch [49350/50000], Train Loss: 25.0036, Test Loss: 41.8812\n",
      "Epoch [49355/50000], Train Loss: 17.9287, Test Loss: 40.4274\n",
      "Epoch [49360/50000], Train Loss: 17.0071, Test Loss: 41.6196\n",
      "Epoch [49365/50000], Train Loss: 18.8454, Test Loss: 41.2805\n",
      "Epoch [49370/50000], Train Loss: 17.8071, Test Loss: 41.7365\n",
      "Epoch [49375/50000], Train Loss: 16.0395, Test Loss: 41.2566\n",
      "Epoch [49380/50000], Train Loss: 21.0194, Test Loss: 41.1691\n",
      "Epoch [49385/50000], Train Loss: 19.6712, Test Loss: 43.9931\n",
      "Epoch [49390/50000], Train Loss: 23.8991, Test Loss: 43.2749\n",
      "Epoch [49395/50000], Train Loss: 20.8987, Test Loss: 48.0130\n",
      "Epoch [49400/50000], Train Loss: 18.8651, Test Loss: 45.9429\n",
      "Epoch [49405/50000], Train Loss: 23.7671, Test Loss: 42.0752\n",
      "Epoch [49410/50000], Train Loss: 18.5533, Test Loss: 40.7744\n",
      "Epoch [49415/50000], Train Loss: 17.7995, Test Loss: 41.6006\n",
      "Epoch [49420/50000], Train Loss: 18.7812, Test Loss: 42.7947\n",
      "Epoch [49425/50000], Train Loss: 17.4779, Test Loss: 40.4938\n",
      "Epoch [49430/50000], Train Loss: 20.9891, Test Loss: 44.9405\n",
      "Epoch [49435/50000], Train Loss: 15.5220, Test Loss: 43.8409\n",
      "Epoch [49440/50000], Train Loss: 18.4919, Test Loss: 43.3815\n",
      "Epoch [49445/50000], Train Loss: 22.5128, Test Loss: 41.0484\n",
      "Epoch [49450/50000], Train Loss: 17.1112, Test Loss: 40.7670\n",
      "Epoch [49455/50000], Train Loss: 18.1612, Test Loss: 43.0102\n",
      "Epoch [49460/50000], Train Loss: 12.0189, Test Loss: 40.5611\n",
      "Epoch [49465/50000], Train Loss: 18.2773, Test Loss: 43.5801\n",
      "Epoch [49470/50000], Train Loss: 18.4980, Test Loss: 42.1422\n",
      "Epoch [49475/50000], Train Loss: 16.4472, Test Loss: 41.2634\n",
      "Epoch [49480/50000], Train Loss: 13.4292, Test Loss: 41.0748\n",
      "Epoch [49485/50000], Train Loss: 18.3787, Test Loss: 42.2997\n",
      "Epoch [49490/50000], Train Loss: 14.5742, Test Loss: 40.5739\n",
      "Epoch [49495/50000], Train Loss: 28.2192, Test Loss: 45.9724\n",
      "Epoch [49500/50000], Train Loss: 20.1304, Test Loss: 45.1844\n",
      "Epoch [49505/50000], Train Loss: 25.1621, Test Loss: 45.0322\n",
      "Epoch [49510/50000], Train Loss: 17.1476, Test Loss: 44.9149\n",
      "Epoch [49515/50000], Train Loss: 20.1894, Test Loss: 44.4792\n",
      "Epoch [49520/50000], Train Loss: 24.2521, Test Loss: 44.4009\n",
      "Epoch [49525/50000], Train Loss: 27.1576, Test Loss: 41.0162\n",
      "Epoch [49530/50000], Train Loss: 19.4976, Test Loss: 42.4190\n",
      "Epoch [49535/50000], Train Loss: 20.7530, Test Loss: 40.6409\n",
      "Epoch [49540/50000], Train Loss: 21.1339, Test Loss: 45.1280\n",
      "Epoch [49545/50000], Train Loss: 16.2828, Test Loss: 41.2071\n",
      "Epoch [49550/50000], Train Loss: 19.1534, Test Loss: 40.7482\n",
      "Epoch [49555/50000], Train Loss: 21.5342, Test Loss: 41.6876\n",
      "Epoch [49560/50000], Train Loss: 16.0224, Test Loss: 41.2850\n",
      "Epoch [49565/50000], Train Loss: 22.0069, Test Loss: 42.4666\n",
      "Epoch [49570/50000], Train Loss: 32.7505, Test Loss: 61.4316\n",
      "Epoch [49575/50000], Train Loss: 17.6292, Test Loss: 45.3301\n",
      "Epoch [49580/50000], Train Loss: 19.5581, Test Loss: 40.9612\n",
      "Epoch [49585/50000], Train Loss: 13.8724, Test Loss: 39.9671\n",
      "Epoch [49590/50000], Train Loss: 16.7974, Test Loss: 41.4102\n",
      "Epoch [49595/50000], Train Loss: 16.9837, Test Loss: 41.6614\n",
      "Epoch [49600/50000], Train Loss: 17.3603, Test Loss: 41.9955\n",
      "Epoch [49605/50000], Train Loss: 19.9676, Test Loss: 42.1598\n",
      "Epoch [49610/50000], Train Loss: 17.0194, Test Loss: 43.9529\n",
      "Epoch [49615/50000], Train Loss: 19.5036, Test Loss: 48.1631\n",
      "Epoch [49620/50000], Train Loss: 21.9663, Test Loss: 45.1906\n",
      "Epoch [49625/50000], Train Loss: 18.4018, Test Loss: 41.7012\n",
      "Epoch [49630/50000], Train Loss: 18.5347, Test Loss: 43.3305\n",
      "Epoch [49635/50000], Train Loss: 14.7716, Test Loss: 39.9687\n",
      "Epoch [49640/50000], Train Loss: 21.4612, Test Loss: 40.9019\n",
      "Epoch [49645/50000], Train Loss: 18.9770, Test Loss: 42.3191\n",
      "Epoch [49650/50000], Train Loss: 15.7339, Test Loss: 40.7195\n",
      "Epoch [49655/50000], Train Loss: 15.4539, Test Loss: 40.7100\n",
      "Epoch [49660/50000], Train Loss: 19.7033, Test Loss: 40.6556\n",
      "Epoch [49665/50000], Train Loss: 19.1950, Test Loss: 43.3057\n",
      "Epoch [49670/50000], Train Loss: 18.2706, Test Loss: 43.2284\n",
      "Epoch [49675/50000], Train Loss: 19.7065, Test Loss: 42.0507\n",
      "Epoch [49680/50000], Train Loss: 18.1915, Test Loss: 42.5503\n",
      "Epoch [49685/50000], Train Loss: 16.9380, Test Loss: 40.4980\n",
      "Epoch [49690/50000], Train Loss: 18.6283, Test Loss: 41.2504\n",
      "Epoch [49695/50000], Train Loss: 16.1760, Test Loss: 41.2128\n",
      "Epoch [49700/50000], Train Loss: 19.6487, Test Loss: 42.3153\n",
      "Epoch [49705/50000], Train Loss: 18.7343, Test Loss: 57.1696\n",
      "Epoch [49710/50000], Train Loss: 28.9129, Test Loss: 48.5073\n",
      "Epoch [49715/50000], Train Loss: 22.7388, Test Loss: 43.3489\n",
      "Epoch [49720/50000], Train Loss: 19.7378, Test Loss: 41.5599\n",
      "Epoch [49725/50000], Train Loss: 17.1937, Test Loss: 42.4631\n",
      "Epoch [49730/50000], Train Loss: 15.8560, Test Loss: 41.9982\n",
      "Epoch [49735/50000], Train Loss: 19.5857, Test Loss: 40.6205\n",
      "Epoch [49740/50000], Train Loss: 22.1531, Test Loss: 41.5701\n",
      "Epoch [49745/50000], Train Loss: 15.1094, Test Loss: 40.0238\n",
      "Epoch [49750/50000], Train Loss: 18.7799, Test Loss: 44.0263\n",
      "Epoch [49755/50000], Train Loss: 18.8990, Test Loss: 43.4143\n",
      "Epoch [49760/50000], Train Loss: 16.6545, Test Loss: 41.1333\n",
      "Epoch [49765/50000], Train Loss: 18.7830, Test Loss: 40.6385\n",
      "Epoch [49770/50000], Train Loss: 22.9547, Test Loss: 43.1478\n",
      "Epoch [49775/50000], Train Loss: 21.1143, Test Loss: 42.7632\n",
      "Epoch [49780/50000], Train Loss: 14.6944, Test Loss: 41.5282\n",
      "Epoch [49785/50000], Train Loss: 19.4045, Test Loss: 42.5269\n",
      "Epoch [49790/50000], Train Loss: 19.9330, Test Loss: 44.9578\n",
      "Epoch [49795/50000], Train Loss: 20.1684, Test Loss: 41.2938\n",
      "Epoch [49800/50000], Train Loss: 22.7612, Test Loss: 40.6620\n",
      "Epoch [49805/50000], Train Loss: 20.0333, Test Loss: 41.3989\n",
      "Epoch [49810/50000], Train Loss: 19.8350, Test Loss: 42.5029\n",
      "Epoch [49815/50000], Train Loss: 20.2882, Test Loss: 42.8913\n",
      "Epoch [49820/50000], Train Loss: 22.9354, Test Loss: 42.8537\n",
      "Epoch [49825/50000], Train Loss: 10.6172, Test Loss: 40.0817\n",
      "Epoch [49830/50000], Train Loss: 21.2823, Test Loss: 41.8400\n",
      "Epoch [49835/50000], Train Loss: 18.9590, Test Loss: 43.8397\n",
      "Epoch [49840/50000], Train Loss: 21.2261, Test Loss: 42.4385\n",
      "Epoch [49845/50000], Train Loss: 25.7827, Test Loss: 44.4399\n",
      "Epoch [49850/50000], Train Loss: 25.2941, Test Loss: 41.6666\n",
      "Epoch [49855/50000], Train Loss: 19.7813, Test Loss: 45.6991\n",
      "Epoch [49860/50000], Train Loss: 21.0021, Test Loss: 43.7011\n",
      "Epoch [49865/50000], Train Loss: 16.2590, Test Loss: 40.7510\n",
      "Epoch [49870/50000], Train Loss: 16.4720, Test Loss: 43.8614\n",
      "Epoch [49875/50000], Train Loss: 16.8204, Test Loss: 43.2847\n",
      "Epoch [49880/50000], Train Loss: 18.5614, Test Loss: 41.1592\n",
      "Epoch [49885/50000], Train Loss: 21.9513, Test Loss: 41.8174\n",
      "Epoch [49890/50000], Train Loss: 17.2868, Test Loss: 41.8840\n",
      "Epoch [49895/50000], Train Loss: 18.0588, Test Loss: 43.3110\n",
      "Epoch [49900/50000], Train Loss: 19.0788, Test Loss: 42.7504\n",
      "Epoch [49905/50000], Train Loss: 17.9082, Test Loss: 42.3522\n",
      "Epoch [49910/50000], Train Loss: 18.9431, Test Loss: 40.4103\n",
      "Epoch [49915/50000], Train Loss: 17.0837, Test Loss: 42.7134\n",
      "Epoch [49920/50000], Train Loss: 15.7362, Test Loss: 42.1562\n",
      "Epoch [49925/50000], Train Loss: 16.3105, Test Loss: 40.6513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49930/50000], Train Loss: 16.5487, Test Loss: 46.7205\n",
      "Epoch [49935/50000], Train Loss: 22.8817, Test Loss: 40.5797\n",
      "Epoch [49940/50000], Train Loss: 35.5898, Test Loss: 39.7940\n",
      "Epoch [49945/50000], Train Loss: 21.5896, Test Loss: 40.8538\n",
      "Epoch [49950/50000], Train Loss: 23.5770, Test Loss: 52.1595\n",
      "Epoch [49955/50000], Train Loss: 18.9475, Test Loss: 41.7259\n",
      "Epoch [49960/50000], Train Loss: 19.5254, Test Loss: 42.0588\n",
      "Epoch [49965/50000], Train Loss: 19.5335, Test Loss: 43.3334\n",
      "Epoch [49970/50000], Train Loss: 16.5344, Test Loss: 43.9597\n",
      "Epoch [49975/50000], Train Loss: 19.0289, Test Loss: 40.7939\n",
      "Epoch [49980/50000], Train Loss: 17.5062, Test Loss: 42.9437\n",
      "Epoch [49985/50000], Train Loss: 41.6499, Test Loss: 40.1868\n",
      "Epoch [49990/50000], Train Loss: 20.5192, Test Loss: 41.5092\n",
      "Epoch [49995/50000], Train Loss: 16.5494, Test Loss: 41.9794\n",
      "Epoch [50000/50000], Train Loss: 18.4204, Test Loss: 48.7312\n",
      "Execution time: 39164.60338687897 seconds\n"
     ]
    }
   ],
   "source": [
    "## Instantiate model and train\n",
    "\n",
    "# For timing cell run time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Create model\n",
    "model_aq = NMR_Model_Lipid()\n",
    "\n",
    "# Move the model to the GPU device\n",
    "model_aq.to(device)\n",
    "\n",
    "# Define the path to save and load the model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Call the function\n",
    "train_losses, test_losses, is_model_trained = train_or_load_model(model_aq, train_iter, test_iter, num_epochs, save_path)\n",
    "\n",
    "\n",
    "# Finish timing cell run time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "if is_model_trained:\n",
    "    np.save(ModelName + \"_ExecutionTime.npy\", execution_time)\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to directory for saving model parameters\n",
    "#os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/SavedParamsAndTrainingMetrics')\n",
    "#np.save(ModelName + \"_TrainLoss.npy\", train_losses)\n",
    "#np.save(ModelName + \"_TestLoss.npy\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot the data\n",
    "plt.plot(np.arange(num_epochs)+1, train_losses, label='Train Loss')\n",
    "plt.plot(np.arange(num_epochs)+1, test_losses, label='Test Loss')\n",
    "\n",
    "# Track the previous minimum test loss and its index\n",
    "prev_min_loss = test_losses[0]\n",
    "prev_min_index = 0\n",
    "\n",
    "# Annotate each local minimum test loss with arrows\n",
    "for idx, loss in enumerate(test_losses[1:], start=1):\n",
    "    if loss < prev_min_loss:\n",
    "        plt.annotate('Min', xy=(idx+1, loss), xytext=(idx+1, loss + 5000),\n",
    "                     arrowprops=dict(facecolor='red', shrink=0.05))\n",
    "        prev_min_loss = loss\n",
    "        prev_min_index = idx\n",
    "        \n",
    "# Add x and y labels\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "\n",
    "# Change axis size\n",
    "plt.rcParams['axes.labelsize'] = 45  # Change label font size\n",
    "\n",
    "# Change tick size\n",
    "plt.tick_params(axis='x', labelsize=30)  # Change tick size for x-axis\n",
    "plt.tick_params(axis='y', labelsize=30)  # Change tick size for y-axis\n",
    "\n",
    "# Plot legend, and display figure\n",
    "plt.legend(fontsize = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMR_Model_Lipid(\n",
       "  (lin1): Linear(in_features=14000, out_features=200, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (lin2): Linear(in_features=200, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Make sure best parameters are being utilized\n",
    "\n",
    "# Switch to directory for saving model parameters\n",
    "os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/SavedParamsAndTrainingMetrics')\n",
    "\n",
    "# Define the path where you saved your model parameters\n",
    "save_path = ModelName + '_Params.pt'\n",
    "\n",
    "# Load the entire dictionary from the saved file\n",
    "checkpoint = torch.load(save_path)\n",
    "\n",
    "# Instantiate the model\n",
    "model_aq = NMR_Model_Lipid()\n",
    "\n",
    "# Load the model's state dictionary from the loaded dictionary\n",
    "model_aq.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Move the model to the GPU \n",
    "model_aq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Switch to directory for saving model metrics\n",
    "\n",
    "os.chdir('/net/clusterhn.cluster.com/home/htjhnson/Desktop/XAI-Uncertainty/ModelPerformanceMetrics')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model err:  0.015454075\n"
     ]
    }
   ],
   "source": [
    "## Test model on testing dataset and deterine RMSE\n",
    "\n",
    "outputs = model_aq(X_train) # Evaluate input spectra with MLP\n",
    "\n",
    "# Move tensors to CPU and convert to numpy arrays\n",
    "outputs_cpu = outputs.detach().cpu().numpy()\n",
    "y_train_cpu = y_train.detach().cpu().numpy()\n",
    "\n",
    "err = np.sqrt(mean_squared_error(outputs_cpu, y_train_cpu))  # Determine RMSE\n",
    "print('model err: ', err)  # Print RMSE\n",
    "\n",
    "np.save(ModelName + \"TrainRMSE\",err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model err:  0.06293419\n"
     ]
    }
   ],
   "source": [
    "## Test model on testing dataset and deterine RMSE\n",
    "\n",
    "model_aq.eval() # Change to evaluation mode (maybe not needed for this model)\n",
    "outputs = model_aq(X_test) # Evaluate input spectra with MLP\n",
    "\n",
    "# Move tensors to CPU and convert to numpy arrays\n",
    "outputs_cpu = outputs.detach().cpu().numpy()\n",
    "y_test_cpu = y_test.detach().cpu().numpy()\n",
    "\n",
    "err = np.sqrt(mean_squared_error(outputs_cpu, y_test_cpu))  # Determine RMSE\n",
    "print('model err: ', err)  # Print RMSE\n",
    "\n",
    "np.save(ModelName + \"TestRMSE\",err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model err:  0.06789579\n"
     ]
    }
   ],
   "source": [
    "## Test model on validation dataset and deterine RMSE\n",
    "\n",
    "model_aq.eval()  # Change to evaluation mode (maybe not needed for this model)\n",
    "outputs = model_aq(spectraVal)  # Evaluate input spectra with MLP\n",
    "\n",
    "# Move tensors to CPU and convert to numpy arrays\n",
    "outputs_cpu = outputs.detach().cpu().numpy()\n",
    "concVal_cpu = concVal.detach().cpu().numpy()\n",
    "\n",
    "err = np.sqrt(mean_squared_error(outputs_cpu, concVal_cpu))  # Determine RMSE\n",
    "print('model err: ', err)  # Print RMSE\n",
    "\n",
    "np.save(ModelName + \"ValRMSE\", err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 18])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpValConc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 14000])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExpValSpectra.squeeze(dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134731/2375053133.py:14: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  per_err = 100*(GroundTruth[LipidGroup] - Prediction_cpu[LipidGroup]) / GroundTruth[LipidGroup]\n"
     ]
    }
   ],
   "source": [
    "APEs = []\n",
    "MAPEs = []\n",
    "\n",
    "for i in np.arange(11):\n",
    "    GroundTruth = ExpValConc[i].detach().cpu().numpy()\n",
    "    Prediction = model_aq(ExpValSpectra.squeeze(dim=2)[i])\n",
    "\n",
    "    # Move Prediction tensor to CPU and detach from computation graph\n",
    "    Prediction_cpu = Prediction.detach().cpu().numpy()\n",
    "\n",
    "    APE = []\n",
    "\n",
    "    for LipidGroup in range(18):\n",
    "        per_err = 100*(GroundTruth[LipidGroup] - Prediction_cpu[LipidGroup]) / GroundTruth[LipidGroup]\n",
    "        APE.append(abs(per_err))\n",
    "\n",
    "    MAPE = sum(APE) / len(APE)\n",
    "\n",
    "    APEs.append(APE)\n",
    "    MAPEs.append(MAPE)\n",
    "\n",
    "# Correct the MAPE for the example input with 4 of 8 metabolites present\n",
    "MAPEs[4] = (APEs[4][0] + APEs[4][1] + APEs[4][4] + APEs[4][6]) / 4\n",
    "\n",
    "# Convert lists to numpy arrays and save\n",
    "np.save(ModelName + \"_\" + \"ValExamples_APEs.npy\", np.array(APEs))\n",
    "np.save(ModelName + \"_\" + \"ValExamples_MAPEs.npy\", np.array(MAPEs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5.686122683185403,\n",
       "  4.827931092690565,\n",
       "  23.132891967873316,\n",
       "  5.625238002032372,\n",
       "  1.4721301596107763,\n",
       "  5.023281805619968,\n",
       "  10.816735535053724,\n",
       "  1.4116912696666366,\n",
       "  2.2956115867321167,\n",
       "  5.023288055014167,\n",
       "  2.3446663195710067,\n",
       "  2.9291381260671905,\n",
       "  37.26467486938877,\n",
       "  12.176492018604044,\n",
       "  6.0282216397545,\n",
       "  49.7682164724869,\n",
       "  inf,\n",
       "  inf],\n",
       " [6.0851619343815395,\n",
       "  8.320380380320598,\n",
       "  11.838078506415862,\n",
       "  5.961663824291341,\n",
       "  2.949016403330643,\n",
       "  2.770289526275983,\n",
       "  11.167332895633534,\n",
       "  0.3591670160663986,\n",
       "  4.085821647956105,\n",
       "  2.7702598167232546,\n",
       "  4.006053005638536,\n",
       "  4.940453882364847,\n",
       "  11.380626686604254,\n",
       "  6.539716163157614,\n",
       "  0.0513370786930004,\n",
       "  16.257495988700413,\n",
       "  inf,\n",
       "  inf],\n",
       " [3.596401600394766,\n",
       "  11.873536077628374,\n",
       "  12.926490458541018,\n",
       "  4.927502718089123,\n",
       "  3.111342999398882,\n",
       "  2.9866150018662223,\n",
       "  10.976331548170545,\n",
       "  0.46775241988579036,\n",
       "  4.24601889648104,\n",
       "  2.9866424597746217,\n",
       "  3.4512625448106786,\n",
       "  7.688916827969115,\n",
       "  12.143621685010299,\n",
       "  9.468837076238366,\n",
       "  24.886350214389505,\n",
       "  38.724008268894956,\n",
       "  inf,\n",
       "  inf],\n",
       " [0.9011422773690227,\n",
       "  0.4088432765775589,\n",
       "  4.237450531295667,\n",
       "  0.31589874797993517,\n",
       "  3.4900956881682705,\n",
       "  2.2155248931722937,\n",
       "  4.367251883122342,\n",
       "  11.90635527773997,\n",
       "  1.6705895661673569,\n",
       "  2.215505759566213,\n",
       "  0.9517823560439858,\n",
       "  24.020030616989203,\n",
       "  2.8628208996484443,\n",
       "  5.13307422446614,\n",
       "  6.909975729542141,\n",
       "  30.941895867244682,\n",
       "  inf,\n",
       "  inf],\n",
       " [2.3125767595275035,\n",
       "  0.45154776026662147,\n",
       "  1.6877742422066266,\n",
       "  1.0856351143298772,\n",
       "  1.3153009726604419,\n",
       "  2.4967824450700498,\n",
       "  6.338396443484259,\n",
       "  7.3663020755682656,\n",
       "  0.007787357197711149,\n",
       "  2.496763311464579,\n",
       "  0.033024423779888,\n",
       "  13.577062936693933,\n",
       "  2.6905980218164416,\n",
       "  3.8066918083445795,\n",
       "  8.139712349749056,\n",
       "  22.662858097336834,\n",
       "  inf,\n",
       "  inf],\n",
       " [7.456419336424793,\n",
       "  6.4308994024296515,\n",
       "  4.9686475131667684,\n",
       "  6.6798828917036355,\n",
       "  4.130845504007523,\n",
       "  2.9608967461677542,\n",
       "  12.189113084394819,\n",
       "  0.17878883125393563,\n",
       "  4.983917899324154,\n",
       "  2.9608967461677542,\n",
       "  4.219195220501308,\n",
       "  1.8890946096256638,\n",
       "  5.565827380814228,\n",
       "  1.1053926895363895,\n",
       "  13.346065470720887,\n",
       "  2.7250792844404828,\n",
       "  inf,\n",
       "  inf],\n",
       " [10.627983427852314,\n",
       "  9.731420207135288,\n",
       "  10.421333107946785,\n",
       "  11.061308426463345,\n",
       "  8.623830769911612,\n",
       "  8.200707694060759,\n",
       "  16.747605258954472,\n",
       "  8.149828291316474,\n",
       "  8.826592605450443,\n",
       "  8.194051340395728,\n",
       "  8.995340449255467,\n",
       "  9.720957533879638,\n",
       "  18.63256379482577,\n",
       "  3.980585950330218,\n",
       "  5.375739237114666,\n",
       "  12.983267437938204,\n",
       "  inf,\n",
       "  inf],\n",
       " [3.5155073717894796,\n",
       "  1.3450825823471004,\n",
       "  2.789040836347014,\n",
       "  4.2914239716618985,\n",
       "  4.755915260882421,\n",
       "  7.5506517924805765,\n",
       "  2.006507879764224,\n",
       "  11.755265614613084,\n",
       "  6.083064322621648,\n",
       "  7.562290100781449,\n",
       "  35.910481033039964,\n",
       "  29.1710225609447,\n",
       "  2.897009318255096,\n",
       "  0.07376074497855722,\n",
       "  4.54510414971872,\n",
       "  0.7488553644700564,\n",
       "  inf,\n",
       "  inf],\n",
       " [113.83841815478932,\n",
       "  95.2965572772037,\n",
       "  98.34890908214835,\n",
       "  102.21911962991443,\n",
       "  92.78260801775193,\n",
       "  130.2875401490381,\n",
       "  101.54511646648103,\n",
       "  92.21306579926826,\n",
       "  93.2322636519893,\n",
       "  132.5745833507275,\n",
       "  89.51027017618328,\n",
       "  97.25353727670387,\n",
       "  96.71694749250376,\n",
       "  95.46724221640196,\n",
       "  95.92589862265676,\n",
       "  94.56342660019705,\n",
       "  102.26226420150988,\n",
       "  103.2208653280241],\n",
       " [110.3787683679264,\n",
       "  95.17284131850518,\n",
       "  98.29831671944203,\n",
       "  102.2061598765801,\n",
       "  92.70825507535788,\n",
       "  123.53978297448755,\n",
       "  101.58387746332579,\n",
       "  92.24538211412151,\n",
       "  93.07043507464073,\n",
       "  124.43094199223023,\n",
       "  90.16079045761379,\n",
       "  97.94025912385138,\n",
       "  97.53768507559347,\n",
       "  95.76807628568386,\n",
       "  96.94447750906936,\n",
       "  91.84514533908312,\n",
       "  103.39358579034705,\n",
       "  104.83117836817858],\n",
       " [108.3029350325728,\n",
       "  95.04240457610265,\n",
       "  98.24448084098962,\n",
       "  102.19328864582386,\n",
       "  92.63218570036913,\n",
       "  119.25087393381204,\n",
       "  101.62458206027502,\n",
       "  92.27714032754868,\n",
       "  92.90065059938125,\n",
       "  119.54476862753192,\n",
       "  90.73534813647568,\n",
       "  98.35237730374935,\n",
       "  98.03027003077578,\n",
       "  96.03135883554707,\n",
       "  97.55561951678165,\n",
       "  83.69015264287545,\n",
       "  106.78640543047277,\n",
       "  109.66302522261999]]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
